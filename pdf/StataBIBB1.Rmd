--- 
title: "Stata Fortbildungen am BIBB"
author: "Andreas Filser"
date: "`r Sys.Date()`"
output: 
  pdf_document
header-includes:
   - \usepackage{xcolor,soul,mdframed,color}
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

```{r setup, echo = F, message=F, warning = F}
# output: bookdown::pdf_document2
if(Sys.getenv("USERNAME") == "Filser" ) .libPaths("D:/R-library4")  # set library
list.of.packages <- c("tidyr","dplyr","readr","haven","rmarkdown","bookdown","devtools","tidyverse","knitr") # Pakete eintragen
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(echo = T)
# knitr::opts_chunk$set(dpi=400)
# devtools::install_github("hemken/Statamarkdown")
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
library(Statamarkdown)
library(tidyverse)
library(ggthemes)
```

# Herzlich Willkommen {-}



**Hier entsteht das Begleitskript für die Stata-Weiterbildungen am BIBB von Andreas Filser vom 02.12. bis zum 03.12.2021.** 


```{r index_illustration, echo = F,out.width = "80%",fig.height= 3.5, fig.align="center"}
knitr::include_graphics("pics/09_mplot4.png")
```

<!-- ```{r kapitel, ft.align = "center",message=F,warning=F, echo = F} -->
<!-- library(kableExtra) -->
<!-- opts <- options(knitr.kable.NA = "") -->

<!-- readxl::read_xlsx("00_kapitel.xlsx",sheet = 1) %>%  -->
<!--   kable() %>%  -->
<!--   kable_styling(bootstrap_options = "condensed", full_width = F) %>%  -->
<!--   column_spec(1,bold = T) -->
<!-- ``` -->




```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Überblick zu Stata  {#stata}


```{r setup1, echo = F, message=F, warning = F}
if(Sys.getenv("USERNAME") == "Filser" ) .libPaths("D:/R-library4")  # set library
stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
# stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
library(Statamarkdown)
library(tidyverse)
library(ggthemes)
```


Hier sehen wir die Startansicht von Stata:
```{r screenshot, fig.cap= "Startansicht Stata16", echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata_intro1.png")
```
Die Standardansicht von Stata besteht aus mehreren Fenstern:

  (1) Kommandos/Befehle
  (2) Ergebnisse
  (3) Variablenübersicht
  (4) Übersicht zu den geladenen Daten
  (5) Befehlshistorie


## Taschenrechner

In das Befehlsfeld (1) können wir Kommandos eingeben, z.B. eine Berechnung durchführen. Dazu geben wir zunächst den Befehl `display` und dann die gewünschte Rechnung ein und drücken dann Enter. Der Befehl wird dann im Ergebnisfenster gespiegelt und darunter das Ergebnis ausgegeben:

```{r screenshot_tr, echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata3_calc.png")
```

Wir können `display` auch mit `dis` abkürzen. 

## Darstellung in diesem Skript

Eingaben und Ergebnisse werden im weiteren Skript so dargestellt:
```{stata s_input, eval=FALSE}
display 3 + 12
```
```{stata s_input2, echo=F}
dis ""
display 3 + 12
```


```{stata s_input3, eval=FALSE}
display sqrt(9)
```

```{stata s_input4, echo=F}
dis ""
dis sqrt(9)
```

## DoFile Editor

Zwar funktioniert die direkte Eingabe in das Befehlsfeld, allerdings werden für Auswertungen sehr viel längere und komplexere Befehle und längere Befehlsfolgen verwendet. Für solche Anwendungen werden in Stata sog. DoFiles angelegt. Darin können Befehle entworfen und für eine spätere Verwendung abgespeichert werden. Um ein DoFile zu öffnen geben wir entweder `doedit` in das Befehlsfeld ein oder klicken auf das Feld "New Do-file-Editor" rechts oben:

```{r screenshot_dof_open, echo = F,out.width = "60%",fig.height= 4, fig.align="center"}
knitr::include_graphics("./pics/01_Stata_open_DOedit.png")
```
Es öffnet sich ein neues Fenster:
```{r screenshot_dof_opened, echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata5_dofile_editor.png")
```
In diesem sog. Do-File-Editor können wir Befehle entwerfen. Wenn wir diese dann durchführen möchten, markieren wir die entsprechenden Zeilen und drücken **STRG + D**. Die ausgeführten Befehle werden wieder im Ergebnisfenster gespiegelt und jeweils darunter die Ergebnisse angezeigt:
```{r screenshot_dof_run, echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata7_dofile_routine.png")
```

Das DoFile können wir speichern, um es später wieder aufzurufen. Wichtig ist dabei, der gespeicherten Datei die Endung "`.do`" zu geben, also zum Beispiel "01_Einstieg.do". Diese DoFiles können dann einfach ausgetauscht werden, um später daran weiter zu arbeiten oder Analysen für Dritte nachvollziehbar zu machen.
```{r save1,echo = F, out.width = "60%",fig.height= 3, fig.align="center"}
knitr::include_graphics("./pics/01_Stata7_dofile_save.png")
```

## Datensätze laden

Das ist aber alles soweit sehr unspektulär - es gibt schönere und günstigere Taschenrechner als Stata. Die eigentliche Stärke von Stata ist die Analyse von Datensätzen. Diese müssen zunächst eingelesen werden. Im einfachsten Fall liegen die Daten als Stata-Datensatz (mit der Endung `.dta`) vor und wir können die Daten wie folgt einlesen:

```{stata readin, eval = F}
cd "C:/Kurse/Stata_BIBB/data/"
use "BIBBBAuA_2018_suf1.0.dta"
```

Der Einlesevorgang besteht also aus zwei Befehlen: zuerst geben wir `cd` den Pfad an, unter welchem der einzulesende Datensatz zu finden ist. Natürlich hängt der Dateipfad aber ganz davon ab, wo Sie den Datensatz gespeichert haben, hier ist es "C:/Kurse/Stata_BIBB/data/":

```{r,echo = F, out.height="80%",out.width="80%", fig.align="center"}
knitr::include_graphics("./pics/01_directory.png")
```
Um den Pfad des Ordners herauszufinden, klicken Sie bei Windows in die obere Adresszeile im Explorerfenster: 
```{r,echo = F, out.height="55%",out.width="65%", fig.align="center"}
knitr::include_graphics("./pics/01_Dateipfad_WIN.png")
```

:::note
In iOS (Mac) finden Sie den Pfad, indem Sie einmal mit der rechten Maustaste auf die Datei klicken und dann die ALT-Taste gedrückt halten. Dann sollte die Option "...als Pfadname kopieren" erscheinen. [**Youtube Anleitung**](https://www.youtube.com/watch?v=zcb3D6Xdv4s)
:::


Hat das funktioniert?   
Wir haben zwei Möglichkeiten zu das aktuelle Arbeitsverzeichnis überprüfen: wir sehen das aktuelle Arbeitsverzeichnis im Stata-Fenster links unten oder wir geben `pwd` ein:
```{r,echo = F, out.height="60%",out.width="60%", fig.align="center"}
knitr::include_graphics("./pics/01_Stata_directory.png")
```

Wir können diese beiden Schritte natürlich auch einfach kombinieren und den gesamten Pfad nach `use` angeben:
```{stata readin2, eval = F}
use "C:/Kurse/Stata_BIBB/data/BIBBBAuA_2018_suf1.0.dta"
```

Wenn das funktioniert hat, dann sehen wir im Fenster rechts eine Variablenübersicht:
```{r datensatz_eingelesen, echo = F,out.width = "90%",fig.height= 4, fig.align="center"}
knitr::include_graphics("./pics/01_Stata8_Datensatz.PNG")
```

## Daten, Codebücher, Fragebogen


Die Daten des hier geladenen Datensatzes enthalten die Angaben von 20012 Befragten der BIBB/BAuA-Erwerbstätigenbefragung 2018. Die BIBB/BAuA ist eine repräsentativbefragung von in Deutschland zu Arbeit und Beruf im Wandel und Erwerb und Verwertung beruflicher Qualifikation.

Das heißt, die hier geladenen Daten beruhen auf standardisierten Interviews. Derartige Datensätze werden in der Regel zusammen mit einem Codebuch, Methodenreport und dem Fragenkatalog veröffentlicht. Im Fragebogen der BIBB/BAuA 2018 finden wir den exakten Wortlaut der Fragen:

```{r Fragenb, echo = F,out.width = "90%",fig.height= 3, fig.align="center",dpi = 600}
knitr::include_graphics("./pics/01_Fragebogen.PNG")
```

Hier sind auch die Anweisungen an die Interviewenden vermerkt, z.B. dass die Vorgaben zum Schichtmodell zunächst nicht vorgelesen werden sollen. Außerdem werden im Methodenbericht weitere Variablen vorgestellt, die bereits im Datensatz enthalten sind, beispielsweise das Alter der Befragten, der Wohnort und die berufliche Stellung:

```{r 01methoden, echo = F,out.width = "90%",fig.height= 3, fig.align="center",dpi = 600}
knitr::include_graphics("./pics/01_methodenbericht.PNG")
```

Die Antworten der Befragten werden dann im Datensatz gesammelt. Dazu werden den Antworten in der Regel Zahlencodes zugewiesen (siehe die ganz linke Spalte im Fragebogen) und so abgelegt. Stata erlaubt dann in einem zweiten Schritt, dass diese Zahlen wieder mit Wertelabeln versehen werden - dazu kommen wir später. In diesem Datensatz entspricht dann jede Zeile einer befragten Person:
```{r codierung, echo = F,out.width = "100%",fig.height= 4, fig.align="center",dpi = 600}
knitr::include_graphics("./pics/01_Codierung_label.PNG")
```
In Fragebogendokumentation finden wir Kürzel links oben, welche die Variablennamen für den Datensatz angeben.

## Überblicksbefehle

Für eine erste Annäherung an die Daten helfen uns drei Überblicksbefehle:  `browse`, `describe` und `list`. Zur Erinnerung: wir haben den Datensatz geladen:

```{stata readin2x, eval = F}
cd "C:/Users/Andreas/Dokumente/Statistik/"
use "BIBBBAuA_2018_suf1.0.dta"
```

### `describe`
Mit `describe, short` bekommen wir einen Überblick zu den eingelesenen Daten:
```{stata desc1,eval = F}
describe, short
```
```{stata desc_actual, echo = F, collectcode=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
describe, short
```
Hier wurde also ein Datensatz geladen, welcher 20012 Fälle/Zeilen (`obs`) und 683 Variablen/Spalten (`vars`) enthält. 
(Wir können den Befehl auch als `d,s` abkürzen)

Wir können den `describe` Befehl auch nutzen, um Informationen zu einer Variable zu erhalten. Dazu hängen wir einfach eine oder mehrere Variablen an `describe` an:
```{stata des_var, eval=FALSE}
describe F209
```

```{stata des_var1, echo = F, collectcode=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
describe F209
```

Hier sehen wir also in der ersten Spalte nochmal den vollständigen Variablennamen, den Variablentypen (`storage type` - mehr dazu [hier](#var_types)) sowie - falls vorhanden - eine Beschreibung der Variable (`variable label`) sowie (ggf.) die Labels für Ausprägungen der Variable (`value label`).
Letztere können wir mit Hilfe von `labelbook` aufrufen (mehr zu Labels später):
```{stata des_var5_lab, eval=FALSE}
labelbook F209
```

```{stata des_var5_lab2, echo = F, collectcode=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
labelbook F209
```

Wir können auch alle Variablen aufrufen, die mit `F209` beginnen, indem wir einen `*` einsetzen:
```{stata des_var5, eval=FALSE}
describe F209*
```

```{stata des_var5b, echo = F, collectcode=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
describe F209*
```


### `browse`

Mit `browse` bekommen wir eine Ansicht des Datensatzes:
```{stata browse1, eval = F}
browse
```
```{r browse_window, echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata9_bro_Datensatz.png")
```

Wenn wir nur einige Variablen betrachten möchten, hängen wir diese einfach `browse` an:
```{stata browse2, eval = F}
browse intnr Bula gkpol Stib zpalter 
```
```{r browse_window2, echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata9_bro_Datensatz2.png")
```

Hier bekommen aber immer die gelabelte Ansicht - allerdings stehen dahinter Zahlenwerte. Die Labels helfen uns, deren Bedeutung zu verstehen. Wenn wir aber wie in den nächsten Tagen auch mit den Daten arbeiten wollen, dann beziehen sich diese Operationen immer auf die dahinterstehenden Zahlenwerte. Diese bekommen wir mit der Option `nolabel` (oft auch einfach als `nol` abgekürzt):

```{stata browse_nol, eval = F}
browse intnr Bula gkpol Stib zpalter , nolabel
```
```{r browse_window2_nol, echo = F,out.width = "90%",fig.height= 4.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata9_bro_Datensatz2_nol.png")
```

### `list`

`browse` eignet sich vor allem für größere Übersichten. Wenn wir nur einige wenige Fälle betrachten möchten, dann ist `list` eine gute Alternative, da der Output hier gleich im Ergebnisfenster (2) angezeigt wird. Die Funktionsweise von `list` ähnelt sich im Prinzip der von `browse`. Mit dem Zusatz `in 1/5` können wir die ersten 5 Zeilen anzeigen lassen (ansonsten würden alle 20012 Fälle angezeigt!):

```{stata list, eval=FALSE}
list intnr Bula gkpol Stib zpalter in 1/5
```
```{stata list1, eval = F, collectcode=F}
     | intnr     Bula     gkpol       Stib   zpalter |
     |-----------------------------------------------|
  1. |   260   Berlin   500.000   Selbstst        41 |
  2. |   361   Berlin   500.000   Angestel        51 |
  3. |   491   Berlin   500.000   Arbeiter        49 |
  4. |   690   Berlin   500.000    Beamter        63 |
  5. |   919   Berlin   500.000   Angestel        41 |
     |-----------------------------------------------|
```
 
Auch hier können wir wie bei `browse` die Zahlenwerte ohne die Labels anzeigen lassen, indem wir `, nolabel` anhängen:
 
```{stata list2, eval=FALSE}
list intnr Bula gkpol Stib zpalter in 1/5, nolabel
```
```{stata list4, eval = F, collectcode=F}
     | intnr   Bula   gkpol   Stib   zpalter |
     |---------------------------------------|
  1. |   260     11       7      4        41 |
  2. |   361     11       7      2        51 |
  3. |   491     11       7      1        49 |
  4. |   690     11       7      3        63 |
  5. |   919     11       7      2        41 |
     |---------------------------------------|
``` 
 
 
### `lookfor`

All diese Befehle setzen aber voraus, dass wir wissen, unter welchem Variablennamen die interessierende Information abgelegt wurde. Mit `lookfor "stichwort"` können wir den Datensatz nach einem Stichwort durchsuchen:
```{stata lookf, eval = F}
lookfor "Englisch"
```
```{stata lookfb, echo = F}
set linesize 200
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
lookfor "Englisch"
```

Alternativ können wir auch in der Variablenübersicht mit Hilfe des Suchfelds nach Variablen suchen:
```{r variablen_suche, echo = F,out.width = "100%",fig.height= 2.5, fig.align="center"}
knitr::include_graphics("./pics/01_Stata8_Datensatz_suche2.PNG")
```


## Übungen 1 {#U1}

(@) Öffnen Sie Stata und öffnen Sie den DoFile Editor

(@) Führen Sie folgende (oder beliebige andere) Taschenrechneraufgaben mit Stata durch:
  + `2 * 4`
  + `2 / 8`
  + `2 ^ 2`   // (Potenz)
  + `sqrt(9)` // (Wurzel)

(@) Lesen Sie die Erwerbstätigenbefragung 2018 in Stata ein. Folgende Schritte helfen Ihnen dabei:
  + In welchem Arbeitsverzeichnis befindet sich Stata aktuell? Wo können Sie das erkennen? 
  + In welchem Ordner haben Sie den Erwerbstätigenbefragung Datensatz abgelegt? 
  + Navigieren Sie Stata mit `cd` in diesen Ordner, in dem der Datensatz abgelegt ist! 
  + Lesen Sie den Datensatz mit  `use` ein
  
(@) Nutzen Sie `describe` und `browse`, um sich einen Überblick über den Datensatz zu verschaffen.
(@) Wie viele Variablen und wie viele Fälle enthält der Datensatz?
(@) Unter welchem Variablennamen ist die Information abgelegt, ob der\*die Befrage  Kinder hat?
  + Nutzen Sie `lookfor` oder die Suchfunktion im Variablenfenster.
  + Lassen Sie sich die Informationen zur Variable mit `describe` anzeigen.
  + Wie heißt das angehängte Label? Inspizieren Sie es mit `labelbook`.
  + Lassen Sie sich diese Variable im Datenexplorer mit `browse` anzeigen.
  + Lassen Sie sich diese Variable mit den Zahlencodes (`nolabel`) anstatt der labels anzeigen.


## Profi-Übungen 1

Für alle, die noch etwas mehr machen möchten: 

+ Im zip-Ordner finden Sie auch eine SPSS-Version (.sav-Datei) der Erwerbstätigenbefragung 2018. Lesen Sie diese Dateiversion in Stata ein! (Siehe [weitere Anmerkungen](#otherfmts))



## Anhang 1


### Variablentypen {#var_types}

In Stata gibt es zwei Variablentypen:

+ Zahlenvariablentypen: `byte`,`int`,`long`,`float`,`double` - der Unterschiede zwischen diesen Variablentypen besteht vor allem in der Spannbreite der möglichen Werte: während `byte` Werte zwischen -127 und 100 annehmen kann, können in `long`  zwischen -2,147,483,647 2,147,483,620 (Stata verwaltet diese Typen in der Regel automatisch - für weitere Hinweise `help data_types`)
 
+ Textvariablen, sog. strings `strXX` - wobei `XX` die Zahl der (maximal) verwendeten Zeichen angibt
  
In der BIBB/BAuA Erwerbstätigenbefragung 2018 sind nur Zahlenvariablen enthalten.



### import anderer Dateiformate {#otherfmts}

Häufig liegen Datensätze nicht im `.dta`-Format vor, sondern beispielsweise als Excel-Tabelle (.xslx oder .xls), SPSS-Datei (.sav) oder als .csv-Datei.

Hierfür stehen alternative Importbefehle zur Verfügung:

+ `import excel` für Excel Tabellen
+ `import spss` für SPSS-Dateien
+ `import delimited` für csv-Dateien

Hier empfiehlt sich ein Blick in das Auswahlmenü unter File > Import:

```{r import_other, echo = F,out.width = "60%",fig.height= 3.5, fig.align="center"}
knitr::include_graphics("./pics/01_import_other.png")
```
Nach Klick auf das entsprechende Dateiformat öffnet sich ein Dialogfenster, in das die gewünschten Optionen eingetragen werden können. Sind alle Optionen wie gewünscht gesetzt, gibt es unten gibt es die Möglichkeit, den so zusammengestellten Befehl nicht nur durchzuführen, sondern auch zu kopieren (um ihn beispielsweise in ein DoFile einzufügen):
```{r import_other2, echo = F,out.width = "40%",fig.height= 3.5, fig.align="center"}
knitr::include_graphics("./pics/01_import_other_copy.png")
```

<!--chapter:end:01-IntroI.Rmd-->

# Arbeiten mit Datensätzen in Stata {#stata2} 

```{r setup2, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = F)
library(Statamarkdown)
# # options(width = 200) # 157
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
```

Nachdem wir uns in Stata etwas umgesehen haben, können wir uns jetzt dem eigentlichen Arbeiten mit Datensätzen zuwenden.
```{stata tab_su1, eval = F}
cd ....
use "BIBBBAuA_2018_suf1.0.dta"
```

## Befehlsstruktur

Ganz nebenbei haben wir in Kapitel 1 bereits die ersten Stata-Befehle verwendet. Bevor wir jetzt aber tiefer einsteigen nochmal einmal allgemein:

Die grundsätzliche Struktur von Stata-Kommandos ist immer `befehl variable, optionen`. Zunächst geben wir also immer an, was passieren soll - bisher war das eben zum Beispiel eine Auflistung (`list`) einiger Variable für einige Zeilen: 
```{stata scommand1, eval = F}
list intnr Bula gkpol Stib zpalter in 11/12
```
```{stata scommand2, eval = F}
     | intnr       Bula      gkpol       Stib   zpalter |
     |--------------------------------------------------|
 11. |  1562     Berlin    500.000   Angestel        31 |
 12. |  1955     Berlin    500.000   Angestel        58 |
```

Mit Hilfe von Optionen können wir die Ausgabe verändern: bspw. hatten wir schong gesehen, dass durch das Anhängen der Option `nolabel` die 'rohen' Zahlenwerte statt der beschrifteten Ausprägungen angezeigt werden:
```{stata scommand3, eval = F}
list intnr Bula gkpol Stib zpalter in 11/12, nolabel
```
```{stata scommands4, eval = F}
     | intnr   Bula   gkpol   Stib   zpalter |
     |---------------------------------------|
 11. |  1562     11       7      2        31 |
 12. |  1955     11       7      2        58 |
```

### Zeilenumbrüche
Kommandos in Stata sind zeilenbasiert. D.h. alles was zu einem Kommando gehört, muss in der gleichen Zeile stehen. Umgekehrt wird auch alles, was in einer Zeile steht als Teil des Kommandos verstanden. Daher werden bei `list intnr Bula gkpol Stib zpalter in 11/15` auch nicht nur `intnr` sondern auch die anderen Variablen angezeigt.
Das funktioniert nicht:
```{stata rowbased2, eval = F}
list intnr Bula 
gkpol Stib zpalter in 11/12
```
> (Hier werden erstmal 20012 Zeilen von `intnr` und `Bula` ausgegeben)  

<span style="color:red">`command gkpol is unrecognized`</span>  
<span style="color:blue">`r(199);`</span>

Mit `///` können wir Zeilen verknüpfen:
```{stata rowbased3, eval = F}
list intnr Bula ///
gkpol Stib zpalter in 11/12 
```
```{stata rowbased3b, eval = F}
     | intnr   Bula   gkpol   Stib   zpalter |
     |---------------------------------------|
 11. |  1562     11       7      2        31 |
 12. |  1955     11       7      2        58 |
```

### Kommentare

Außerdem können wir mit `//` Kommentare in unsere DoFiles einfügen. `//` blendet  alles folgende am Ende der Zeile aus:
```{stata comment, eval = F}
list intnr Bula gkpol Stib zpalter in 11/15 // dies ist ein kommentar
```

`///` kann auch zum Kommentieren verwendet werden:
```{stata comment3, eval = F}
list intnr Bula /// hier kann ein kommentar stehen
gkpol Stib zpalter in 11/15 // hier geht es weiter, aber der Befehl endet mit dieser Zeile
```

Alternativ können wir mit `*` eine komplette Zeile 'deaktivieren':
```{stata comment4, eval = F}
*list intnr Bula gkpol Stib zpalter in 11/15 
```

So wird der Befehl ignoriert.

## Auszählen & Überblick erhalten

Mit `tabulate` bekommen wir eine Auszählung der Ausprägungen einer Variable, indem wir einfach den Variablennamen anhängen
(kann auch mit `tab m1202` abgekürzt werden):
```{stata tab, eval = F}
tabulate m1202
```

```{stata tabb, echo = F, collectcode = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tab m1202
```

In der ersten Spalte werden die verschiedenen Ausprägungen aufgelistet, in der zweiten Spalte finden wir dann die absoluten Häufigkeiten (`Freq.`) der jeweiligen Ausprägung, in der dritten Spalte finden wir die relativen Häufigkeiten (`Percent`) und in der vierten Spalte (`Cum.`) finden wir die kumulierten relativen Häufigkeiten:

+ `Freq.`: der Datensatz enthält 1,091 Befragte ohne Berufsabschluss  (absolute Häufigkeit)
+ `Percent`: 5,45% der im Datensatz enthaltenen Befragte haben keinen Berufsabschluss (relative Häufigkeit)
+ `Cum`: 5,68% der im Datensatz enthaltenen Befragte haben keinen Berufsabschluss oder machten keine Angabe (kumulierte relative Häufigkeit)

Mit `nolabel` die dahinterliegenden Zahlenwerte anzeigen lassen:
```{stata tab2, eval = F}
tabulate m1202, nolabel
```

```{stata tabb2, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tab m1202, nola
```

Mit Hilfe von `d` und `labelbook` können wir uns die Labels ausgeben lassen:
```{stata tab_d, eval = F}
d m1202
labelbook M1202
```

```{stata tab_d2, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
d m1202
labelbook M1202
```

Für Fälle ohne gültige Angaben zum höchsten Ausbildungsabschluss wurde also -1 als Wert in `m1202` abgelegt. 
Stata berücksichtigt diese Tatsache aber noch nicht und summiert bspw. die kumulierten relativen Häufigkeiten auch über die fehlenden Angaben auf.  
Um das zu ändern, müssen wir -1 als *missing value* überschreiben. 

## Fehlende Werte {#missv}

Fehlende Werte, sog. *missing value*s werden in Stata mit `.` abgelegt und werden dann für die weiteren Berechnungen ausgeschlossen. 
Um also -1 als `.` zu überschreiben, können wir zB auf den `replace` Befehl zurückgreifen:

```{stata miss2, eval = F}
tabulate m1202
replace m1202 = . if m1202 == -1
tabulate m1202
```
```{stata miss2b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tab m1202
replace m1202 = . if m1202 == -1
tab m1202
```

Wir bekommen von Stata mitgeteilt, dass wir 45 Beobachtungen verändert und auf missing gesetzt haben. 
Im folgenden `tabulate` werden diese fehlenden Werte dann ignoriert.

Ein Spezialbefehl für die Überschreibung von bestimmten Werten ist `mvdecode`. Mit `mvdecode` können wir `-1` in mehreren Variablen gleichzeitig als missing überschreiben. Zum Beispiel in `F100_kldb2010_BOF`, `F1609_kldb2010_BOF` und `F1610_kldb2010_BOF`:
```{stata miss3, eval = F}
mvdecode F100_kldb2010_BOF F1609_kldb2010_BOF F1610_kldb2010_BOF, mv(-1)
```
```{stata miss3b, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
mvdecode F100_kldb2010_BOF F1609_kldb2010_BOF F1610_kldb2010_BOF, mv(-1)
```
> (`F1609_kldb2010_BOF` enthält das BIBB-Berufsoberfeld f. Erwerbsberuf, KldB2010, `F1609_kldb2010_BOF` die selbe Information für den Vater und `F1610_kldb2010_BOF` für die Mutter der\*des Befragten.)

Wir können auch mit `/` einen Wertebereich angeben, der als Missing definiert werden soll, zB. für das Bundesland der Betriebsstätte:
```{stata miss8, eval = F}
mvdecode F100_wib1, mv(-4/-1)
```

```{stata miss9, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
mvdecode F100_wib1, mv(-4/-1)
```

Mit `mdesc` bekommen wir eine Auszählung zu fehlenden Werten. `mdesc` ist allerdings kein Standard-Stata-Befehl, sondern muss extra installiert werden. Das ist allerdings kein größerer Aufwand:
```{stata miss4, eval = F}
ssc install mdesc
```
Anschließend können wir mit `mdesc` die Zahl der missings in `zpalter`, `S2_j`, `F510` und `F511_j` ausgeben lassen:
```{stata miss5, eval = F}
mdesc m1202 F100_kldb2010_BOF F1609_kldb2010_BOF F1610_kldb2010_BOF F100_wib1
```
```{stata miss5b, echo = F, collectcode=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode m1202 F100_kldb2010_BOF F1609_kldb2010_BOF F1610_kldb2010_BOF, mv(-1)
qui mvdecode F100_wib1, mv(-4/-1)
mdesc m1202 F100_kldb2010_BOF F1609_kldb2010_BOF F1610_kldb2010_BOF F100_wib1
```

Hinweis: welche Angaben als Missing definiert werden sollen, ist teilweise von der Frage ab, die beantwortet werden soll. Bspw. wurden oben alle uneindeutigen Angaben in `F233` zum Bundesland der Betriebsstätte als missing codiert. Je nach Fragestellung ist das eine mehr oder weniger gute Idee.

In `tabulate` können wir mit der Option `, missing` die fehlenden Werte anzeigen lassen:
```{stata miss6, eval = F}
tabulate m1202, missing
```
```{stata miss7, echo = F, collectcode=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode m1202, mv(-1)
tabulate m1202, missing
```

## Zwei Variablen, eine Tabelle: Kontingenztabellen

Neben der einfachen Verteilung der Variable interessiert uns aber meistens, ob sich die Verteilung zwischen Gruppen unterscheidet. Hierfür sind Kontingenztabellen ein wichtiges und sehr häuifg verwendetes Werkzeug.

Aus Kontingenztabellen erfahren wir, wie häufig Merkmalskombinationen auftreten. Auch für Kontingenztabellen können wir ebenfalls `tabulate` verwenden. Zum Beispiel können wir uns eine Tabelle anzeigen lassen, die uns die Ausbildungsabschlüsse getrennt nach Geschlechtern zeigt. Da die Variable `S1` keine Missings hat, können wir direkt loslegen.

Für die Kontingenztabelle geben wir dann nach `tabulate` die beiden Variablen an, welche die Zeilen und Spalten definieren:
```{stata cross_tabx, echo = T, eval = F}
tabulate m1202 S1
```
```{stata cross_tabb, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode m1202, mv(-1)
tabulate m1202 S1
```

Wir erkennen aus dieser Tabelle beispielsweise, dass 1,073 Männer und 652 Frauen eine Aufstiegsfortbildung besitzen.

### In Prozent: Relative Häufigkeiten

Auch hier können wir uns die relativen Häufigkeiten anzeigen lassen, indem wir die Option `,cell` anwenden.
Um die Tabelle übersichtlich zu halten, können wir mit `nofreq` die absoluten Häufigkeiten ausblenden (ansonsten werden die absoluten  *und* die relativen Häufigkeiten ausgegeben).
```{stata cell_pct2, echo = T, eval = F}
tabulate m1202 S1, cell nofreq
```
```{stata cell_pct2b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode m1202, mv(-1)
tabulate m1202 S1, cell nofreq
```

Die hier dargestellten relativen Häufigkeiten beziehen sich jeweils auf die Gesamtzahl der Befragten. Formal dargestellt wird also für die Kombination Aufstiegsfortbildung und Geschlecht  = weiblich die Anzahl der Frauen mit Aufstiegsfortbildung durch die Anzahl **aller Befragten** geteilt: $\frac{\text{Anzahl der Frauen mit Aufstiegsfortbildung}}{\text{Gesamtzahl der Befragten}}$ - Wir können also aus dieser Tabelle ablesen, dass 1,27% *aller* Befragten weiblich sind und eine Aufstiegsfortbildung haben. 

Für den Vergleich zwischen Gruppen sind aber in der Regel die *bedingten relativen Häufigkeiten* informativer. Hier kommt es jetzt darauf an, welche der beiden Variablen die Gruppen definieren sollen und welche Variable wir untersuchen möchten:

### pro Gruppe 1: Zeilenprozente

Wir können diese Tabelle auch mit Zeilenprozenten anzeigen lassen, indem wir die Option `row` verwenden:

```{stata row_pct, echo = T, eval = F}
tabulate m1202 S1, row nofreq
```
```{stata row_pctb, echo = F}
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode m1202, mv(-1)
tabulate m1202 S1, row nofreq
```
*Achtung!* Damit ändert sich jeweils die Interpretation der Tabelle! Wir ändern durch `row` die Bezugsgröße oder formal ausgedrückt den Nenner:  

Für die **Zeilenprozente** werden die Werte in Bezug zu den Zeilensummen gesetzt. Also wird die Anzahl der ledigen 35-59 Jährigen ins Verhältnis zur Gesamtzahl der 1x/Woche Internet nutzenden Befragten gesetzt: $\frac{\text{Anzahl der Frauen mit Aufstiegsfortbildung}}{\text{Anzahl der Befragten mit Aufstiegsfortbildung}}$

Interpretation: 37.80% *der Befragten mit Aufstiegsfortbildung* sind Frauen.
  
### pro Gruppe 2: Spaltenprozente
Wir können diese Tabelle auch mit Spaltenprozenten anzeigen lassen, indem wir die Option `col` verwenden:

```{stata col_pct, echo = T, eval = F}
tabulate m1202 S1, col nofreq
```
```{stata col_pctb, echo = F}
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode m1202, mv(-1)
tabulate m1202 S1, col nofreq
```

Für die **Spaltenprozente** werden die Werte in Bezug zu den Spaltensummen gesetzt. Also wird die Anzahl der 1x/Woche Internet nutzenden Frauen ins Verhältnis zur Zahl der befragten Frauen gesetzt: $\frac{\text{Anzahl der Frauen mit Aufstiegsfortbildung}}{\text{Gesamtzahl der befragten Frauen}}$ - Interpretation: "6.58% der *befragten Frauen* haben eine Aufstiegsfortbildung". 

*** 

**[Übung1](#tab2)**

*** 

## metrische Variablen


Für metrisch skalierte Variablen mit deutlich mehr Ausprägungen können wir `tabulate` zwar theoretisch auch verwenden, allerdings wird eine Tabelle hier sehr schnell sehr unübersichtlich:
(Zum Unterschied zwischen metrischen und kategorial skalierten Variablen kommen wir [gleich](#skalenn))
```{stata tab1, eval = F}
mvdecode zpalter, mv(9999)
tabulate zpalter
```

```{stata tab1b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tab zpalter
```

Für metrische Variablen empfiehlt sich daher `summarize` als Überblicksbefehl:
```{stata su1, eval = F}
summarize zpalter
```
```{stata su1b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
su zpalter
```
`summarize zpalter` kann auch mit `su zpalter` abgekürzt werden.
Hier bekommen wir die Anzahl der Beobachtungen (`Obs.`), das arithmetische Mittel (`Mean`), sowie die Minimal- und Maximalwerte ausgegeben (`Min`&`Max`). (zur Standardabweichung `Std. Dev.` kommen wir noch).

`summarize` klappt auch mit mehreren Variablen auf einmal (bei F518_SUF handelt es sich um den monatlichen Bruttoverdienst der Befragten):
```{stata su2, eval = F}
summarize zpalter F518_SUF // klappt auch mit mehreren Variablen
```
```{stata su2b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter F518_SUF, mv(9999 99998/99999)
summarize zpalter F518_SUF // klappt auch mit mehreren Variablen
```

## Exkurs: Skalenniveau {#skalenn}

Im Zusammenhang mit Alter war gerade von metrischen Variablen die Rede. Dabei geht es um das sog. Skalenniveau von Variablen und Merkmalen.
Dabei geht es um den Informationsgehalt von Variablen: welche Informationen können wir aus einer Variable ablesen? 
Prinzipiell wird zwischen <span style="color:#7D9BB5">kategorialen</span> und <span style="color:#E38717">metrischen</span> Skalenniveaus unterschieden.
Der zentrale Unterschied besteht darin, dass bei kategorialen Variablen Zahlen_codes_ verwendet werden, wohingegen bei metrischen Variablen die Werte als tatsächliche Zahlenwerte interpretierbar sind. 
Innerhalb der kategorialen Merkmale wird wiederum zwischen nominaler und ordinaler Skalierung unterschieden, die metrischen Merkmale lassen sich in intervall- und ratio-skalierte Variablen unterscheiden.


```{r skal1, echo = F,out.width = "100%",fig.height= 2.5, fig.align="center"}
knitr::include_graphics("./pics/02_Skalenniveau.png")
```

Sehen wir uns das einmal für die Variablen Geschlecht (`S1`), Schulbildung (`S3`), Geburtsjahr (`S2_j`) und Alter (`zpalter`) an:

```{stata skal_list, eval = F}
list S1 S3 S2_j zpalter in 1/5, nol
```

```{stata skal_list2, eval = F}
     +--------------------------+
     | S1   S3   S2_j   zpalter |
     |--------------------------|
  1. |  1    8   1976        41 |
  2. |  2    5   1966        51 |
  3. |  1    7   1968        49 |
  4. |  2    8   1954        63 |
  5. |  2    7   1976        41 |
     +--------------------------+
```


```{stata skal_list3, eval = F}
list S1 S3 S2_j zpalter in 1/5
```

```{stata skal_list4, eval = F}
     +--------------------------------------+
     |       S1         S3   S2_j   zpalter |
     |--------------------------------------|
  1. |  männlic   Abitur /   1976        41 |
  2. | weiblich   Realschu   1966        51 |
  3. |  männlic   Fachhoch   1968        49 |
  4. | weiblich   Abitur /   1954        63 |
  5. | weiblich   Fachhoch   1976        41 |
     +--------------------------------------+
```

**$\Rightarrow$ Skalenniveau bestimmt, ob `tabulate` oder `summarize` die passenden Befehle sind**


Literaturtipps:

+ S.18 *ff.* in [Diaz-Bone, R. (2019). Statistik für Soziologen (4. Auflage)](https://www.utb-studi-e-book.de/9783838550718)
+ S.12 *ff.* in [Bortz, J., & Schuster, C. (2010). Statistik für Human- und Sozialwissenschaftler (7. Auflage)](https://link.springer.com/book/10.1007/978-3-642-12770-0) (ausführliche formale Beschreibung)

*** 

**[Übung2](#su2)**

*** 

## Nochmal von vorne: Daten neu laden

Fälle gelöscht, die doch nötig ist? Missings falsch codiert? Das ist zwar ärgerlich, aber ein riesen Vorteil der DoFile-basierten Datenarbeit mit Stata ist die Reproduzierbarkeit. Wir können einfach nochmal von vorne anfangen. Dazu lesen wir die Original-Daten einfach erneut mit `use` ein. Allerdings ist dann eine Besonderheit zu beachten:

```{stata re_use, eval = F}
use "BIBBBAuA_2018_suf1.0.dta"
```

`no; dataset in memory has changed since last saved`

Wir müssen erst den existierenden Datensatz mit `clear` löschen 
```{stata, eval = F, echo =T }
clear
use "BIBBBAuA_2018_suf1.0.dta"
```
oder die `clear` Option für `use` verwenden:
```{stata, eval = F, echo =T }
use "BIBBBAuA_2018_suf1.0.dta", clear
```

## Speichern

Natürlich können wir unsere Daten auch abspeichern, wenn alles wie gewünscht geklappt hat. Dafür gibt es den Befehl `save`, der analog zu `use` funktioniert. Wenn wir allerdings einfach wieder den Original-Datensatznamen angeben und in der Zwischenzeit Variablen erstellt oder gelöscht haben, dann bekommen wir folgende Fehlermeldung:

```{stata, eval = F}
cd ... 
save "BIBBBAuA_2018_suf1.0.dta"
```
<span style="color:red">`file BIBBBAuA_2018_suf1.0.dta already exists`</span>  
<span style="color:blue">`r(602);`</span>

Wir geben also entweder einen anderen Dateinamen an:
```{stata, eval = F}
save "BIBBBAuA_2018_suf1.0_NEU.dta"
```
Wir können aber auch mit der Option `replace` explizit mitteilen, dass die Datei überschrieben werden soll:
```{stata, eval = F}
save "BIBBBAuA_2018_suf1.0.dta", replace
```
**Achtung:** Damit sind die Originaldaten aber natürlich weg.

## Übungen 2
### Übung 2-1 {#tab2}

(@) Laden Sie die Erwerbstätigenbefragung in Stata. 

(@) Die Variable `F100_wib1` erfasst, ob die Befragten in einem wissensintensiven Beruf tätig sind (ja/nein).
      + Lassen Sie sich eine Tabelle für `F100_wib2` anzeigen. Welche ist die häufigste Ausprägung?
      + Lassen Sie sich die Zahlencodes anstelle der Labels anzeigen (`nolabel`)
      + Überschreiben Sie alle Codes für fehlende Angaben mit `.` Denken Sie an [`labelbook`](#missv)).
      + Lassen Sie sich mit `tabulate` die Häufigkeitsauszählung erneut anzeigen. Hat sich die Fallzahl zu vorhin verändert?
      + Wie können Sie die fehlenden Werte auch in `tabulate` auszählen lassen? Lassen Sie sich diese Auszählung anzeigen!
      + Für wie viele Befragte liegt keine Angabe zum wissensintensiven Beruf vor? Nutzen Sie `mdesc`.

(@) Lassen Sie sich jetzt `F100_wib2` getrennt nach der Wohnortgröße ausgeben. Die Variable `gkpol` beinhaltet die Größenklasse der Wohngemeinde der Befragten in 7 Kategorien (bis 2000 Einwohner, 2000-5000 Einwohner, usw.)
      + Welche Merkmalskombination ist die häufigste? 
      + Welcher Anteil der Befragten aus Städten mit über 500.000 Einwohnern ist in einem wissenintensiven Beruf tätig?
      + Wie hoch ist der Anteil der in einem wissenintensiven Beruf tätig Befragten, die in Orten mit unter 2.000 Einwohnern leben?

(@) Kommentieren Sie einen Ihrer bisherigen Befehle mit `//`! Probieren Sie auch aus, mit `///` einen Befehl über mehrere Zeilen zu verteilen!

### Übung 2-2 {#su2}

(@) Die Variable `F1104` beinhaltet das Jahr des Schulabschlusses.
      + Verwenden Sie `summarize` um einen Überblick zu dieser Variable zu bekommen
      + Welches Skalenniveau hat die Variable? 
      + Hat die Variable `F1104` Werte, die Sie mit `.` ersetzen sollten? (Denken Sie an [`labelbook`](#missv))
      + Führen Sie ggf. die Umcodierung der problematischen Werte auf `.` durch, um sie so Stata kenntlich zu machen.
      + Lassen Sie sich `F1104` erneut mit `summarize` anzeigen: hat alles geklappt wie es soll?
      + Für wie viele Befragte liegt kein Schulabschlussjahr vor? Nutzen Sie `mdesc`.
      
      



<!--chapter:end:02-Datensaetze.Rmd-->

# Deskriptive Statistik {#desc1} 

```{r setup3, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
# knitr::opts_chunk$set(collectcode = T)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F)
```

## Metrische Variablen beschreiben 

Im vorherigen Kapitel bezog sich vor allem kategoriale Variablen (mit einer begrenzten Anzahl an Ausprägungen). Für ein metrisches Merkmal, wie zum Beispiel das Alter (`zpalter`) macht eine Häufigkeitstabelle wenig Sinn, da das Alter sehr viele Ausprägungen hat und eine Tabelle unübersichtlich wäre (es gibt insgesamt 71!).

### Exkurs: Klassen bilden {#cut}
Um dieses Problem zu umgehen, könnten wir die Informationen in Klassen/Gruppen zusammenzufassen.
So transformieren eine metrische Variable in eine kategoriale, ordinale Variable:

```{r skal2, echo = F,out.width = "100%",fig.height= 2.5, fig.align="center"}
knitr::include_graphics("./pics/03_skalen_transformation.png")
```

*Hier nur ein ganz kurzes Beispiel, in Kapitel 4 werden wir uns ausführlicher mit der Erstellung von Variablen in Stata beschäftigen.*
Für diese Transformation können wir in Stata den `egen`-Befehl nutzen. 
Um die Klassen zu bilden, nutzen wir `egen` zusammen mit der Funktion `cut()`. In `cut` geben wir die zu unterteilende Variable an, außerdem legen wir in `at()` die Grenzen fest. Die so generierten Werte legen wir in einer neuen Variable `age_cat` ab: 
```{stata age_cut, eval = F}
egen age_cat = cut(zpalter), at(15 18 35 60 100 ) label
```

```{stata age_cut2, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
egen age_cat = cut(zpalter), at(15 18 35 60 100 )  label
```

Für die neue, klassierte Variable können wir dann wieder mit `tabulate` eine Häufigkeitstabelle anfordern:
```{stata, eval = F}
tabulate age_cat
```
```{stata age_cut_tab, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
qui egen age_cat = cut(zpalter), at(15 18 35 60 100 ) label
tabulate age_cat
```

Bspw. erkennen wir in dieser Tabelle, dass 69.85% der Befragten zwischen 35 und 59 Jahre alt sind (`Cum.` $\Rightarrow$ 86.79% sind bis zu 59 Jahre alt).
Allerdings geht uns dadurch sehr viel Information verloren: wir 'erkaufen' uns die Übersichtlichkeit durch einen Verlust an Präzision bzgl. der Altersangaben.

### Lage- und Konzentrationsmaße

Allerdings können metrische Variablen auch direkt Hilfe von Lage- und Konzentrationsmaßen beschrieben werden.
Klassische Lagemaße zur Beschreibung von metrischen Variablen sind bspw. Minimum und Maximum, das arithm. Mittel sowie der Median und Quantile. Auch hier haben wir bereits den wichtigsten Befehl kennen gelernt: `summarize` gibt uns einen ersten Überblick zur Verteilung einer metrischen Variable:

```{stata su1F, eval = F}
summarize zpalter
```

```{stata su1T, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
summarize zpalter
```
Wir erfahren also, dass die Befragten im Mittel 47.19228 Jahre alt sind, die jüngsten Befragten 15 Jahre alt und ältesten Befragten sind 87 Jahre alt.

Mit der Option `,detail` bekommen wir eine deutlich umfangreichere Auskunft:
```{stata su2F, eval = F}
summarize zpalter, detail
```

```{stata su2T, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
summarize zpalter, detail
```

Hier wird eine ganze Menge auf einmal angezeigt - die Kennzahlen sind dabei in drei Spalten organisiert: 

+ Aus der ersten Spalte erfahren wir die Quantile (`Percentiles`). Ein Quantil einer Verteilung trennt die Daten so in zwei Teile, dass `x`\% der Daten darunter und 100-`x`\% darüber liegen. Hier können wir zB ablesen, dass 25% der Befragten 38 Jahre oder jünger sind. 95% der Befragten sind <= 63 Jahre alt. Dementsprechend sind 5% der Befragten 63 Jahre oder älter. Die 50%-Grenze für das Alter (*der Median*) liegt bei 49 Jahren.[Mehr zu Perzentilen](#perc)

+ die zweite Spalte gibt uns jeweils die 4 kleinsten und größten Werte für das Alter aus: die 4 kleinsten Alterswerte sind jeweils 15,16,16,16 die größten Werte sind 81,82,83 und 87.

+ in der dritten Spalte bekommen wir eine Reihe weiterer Informationen:
  + die Zahl der Beobachtungen (`Obs`) und das arithmetische Mittel (`Mean`) [mehr zum arithm. Mittel](#mean)
  + die Streuung anhand der Standardabweichung (`Std. Dev.`) und Varianz (`Variance`) [mehr dazu](#variance)
  + sowie die Schiefe (`Skewness`) und Wölbung (`Kurtosis`) [mehr dazu](#sk_kur)


### `tabstat`

Häufig möchten wir aber vielleicht nur eine bestimmte Information. Dafür ist `tabstat` eine gute Lösung. Mit `tabstat` können wir eine ganze Reihe statistischer Kennzahlen für metrische/kontinuierliche Merkmale berechnen. Die Syntax hat dabei das folgende Format: `tabstat zpalter, s(*option*)`

Hier eine Übersicht der Kennzahlen:
```{r, ft.align = "center",message=F,warning=F, echo = F}
library(kableExtra)
opts <- options(knitr.kable.NA = "")

readxl::read_xlsx("03_tabstat.xlsx",sheet = 1) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 11) %>% 
  column_spec(1,monospace = TRUE)
```

Hier ein Bespielbefehl für die Berechnung des arith. Mittels, des Medians, der Varianz und des Varianzkoeffizienten mit `tabstat`:
```{stata tabstatF, eval = F}
tabstat zpalter, s(mean median var cv)
```

```{stata tabstatT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tabstat zpalter, s(mean median var cv)
```

Allerdings lassen sich mit `tabstat` nicht beliebige Quantil-Grenzen (nur für 5,10,25,50,75,90,95,99) berechnen, dafür können wir `centile` nutzen:
```{stata centileF, eval = F}
centile age, centile(35)
```

```{stata centileT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
centile zpalter, centile(35)
```
Aus den Angaben unter `Centile` erfahren wir, dass 35% der Befragten im Datensatz sind 44 Jahre alt oder jünger sind. Dementsprechend sind 65% der Befragten 44 Jahre oder älter.

### Kennzahlen vergleichen mit `tabstat` {#by_vgl}

Interessant sind diese Kennzahlen auch wieder erst im Vergleich zwischen Gruppen. Hierfür steht uns die `by()`-Option von `tabstat` zur Verfügung. Bspw. können wir die Altersangaben von Männern und Frauen vergleichen, indem wir in `tabstat` die Option `by(S1)` verwenden:

```{stata bytabstF, eval = F}
tabstat zpalter, s(mean median var cv) by(S1)
```
```{stata bytabstT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tabstat zpalter, s(mean median var cv) by(S1)
```

Wir sehen hier also, dass sowohl das arith. Mittel als auch der Median des Befragtenalters bei Frauen höher ist als bei Männern. Außerdem ist die Streuung bei Männern höher als bei Frauen.

Eine andere Option ist es, auf den `if`-Befehl zurückzugreifen - siehe [hier](#if_vgl)


### Streuungsmaße: Varianz, Standardabweichung, Variationskoeffizient {#streu}

Streuungsmaße helfen uns zu beurteilen, wie groß die Unterschiede in unseren Daten sind. Je größer das Streuungsmaß, desto mehr Unterschiede gibt es zwischen den beobachteten Werten.

Die häufigste Kennzahl zur Beschreibung von Streuung ist aber die **Varianz**. Die Varianz ist definiert als die durchschnittliche quadrierte Abweichung vom arith. Mittel: $$var(x) = \frac{\sum_{i=1}^{n} (x_{i}-\bar{x})^2}{n}$$    
[Was bedeutet diese Formel?](#variance)    

Wir können die Varianz mit `tabstat , s(var)` berechnen:
```{stata varF, eval = F}
tabstat zpalter, s(var)
```
```{stata varT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tabstat zpalter, s(var)
```
```{r, echo = F}
var <- var(baua$zpalter[baua$zpalter<100])
```

In unseren Datensatz beträgt der Varianz des Alters also `r format(var,big.mark = ".",decimal.mark = ",")` Jahre². Auf der Varianz beruhen noch zwei weitere Streuungsmaße. Die **Standardabweichung** entspricht der Wurzel der Varianz und hat somit die gleiche Maßeinheit wie die Variable (hier also Jahre): 
```{stata std2, eval=F}
dis sqrt(128.5417)
```

```{stata std2T, echo=F}
dis sqrt(128.5417)
```

Wir können die Standardabweichung auch mit `tabstat , s(sd)` berechnen:
```{stata stdF, echo = F}
tabstat zpalter, s(sd)
```
```{stata stdT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tabstat zpalter, s(sd)
```

Der **Variationskoeffizient**  entspricht `sd`/`mean` und dient dem Vergleich verschiedener Streuungen relativ zum jeweiligen Mittelwert, `tabstat , s(cv)` übernimmt die Berechnung für uns:
```{stata cvF, eval = F}
tabstat zpalter, s(cv)
```
```{stata cvT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tabstat zpalter, s(cv)
```

Somit können wir bspw. mit Hilfe von `,by(S1)` die Streuung des Alters bei männlichen und weiblichen Befragten vergleichen:
```{stata cvF2, eval = F}
tabstat zpalter, s(cv) by(S1)
```

```{stata cvT2, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tabstat zpalter, s(cv) by(S1)
```

***

> $\Rightarrow$ Für welches Geschlecht ist die Streuung also größer?[^034]

***


[^034]: Da der Variationskoeffizient für Männer mit `0.2554795` etwas größer ist als für Frauen (`0.2236982`), ist die Streuung bei den Männern etwas größer.




### Gini-Koeffizient {#gini}

Zur Beschreibung der (Ungleich-)Verteilung von metrischen Variablen, insbesondere bei Einkommens- und Vermögensdaten wird häufig der Gini-Koeffizient verwendet. Der Gini-Koeffizient beruht auf der Fläche zwischen der Lorenzkurve und der Gleichverteilung. Auf der x-Achse werden die kumulierten Perzentile der Befragten abgetragen, auf der y-Achse die Perzentile des HH-Einkommens:



```{r gini_plot1, echo=F, warning=F, message=F, out.height="75%", out.width="75%", fig.align="center"}
gdf2 <- 
  data.frame(id = 1:5) %>% 
  mutate(eq = 1, ineq = id/5*5/3, w = 6-id ) %>% 
  mutate(across( matches("eq"), ~ cumsum(.x),.names = "{col}_{fn}")) %>% 
  uncount(w,.id = "x") %>% 
  mutate(eq2 = ifelse(lead(id)==id | id == 5,NA,eq_1))

  gdf2 %>% 
  ggplot(.,aes(x = 6-x,y = ineq)) +
  geom_col(color = "grey25", aes(fill = factor(id,levels=5:1))) +
  geom_line(data = data.frame(x=1:5),  aes(y = 1:5,x=1:5), color = "#EEDA9DFF", size = .5) +
  geom_point(data = data.frame(x=1:5),  aes(y = 1:5,x=1:5), fill = "#EEDA9DFF", size = 3, shape= 21, color = "grey25", stroke = .25)+
  geom_text(aes(label = id,y = ineq_1-ineq/2, color = factor(id,levels=5:1) )) + 
    scale_x_continuous(breaks = 1:5, labels = c("kleinster","2.kleinster","3","zweitgrößter","größter")) +
    scale_y_continuous(breaks = 0:5, labels = paste0(seq(0,100,20),"%") ) +
    scale_fill_manual(values = paletteer::palettes_d$dutchmasters$pearl_earring[2:6]) +
    scale_color_manual(values = c("grey25","grey25","grey98","grey25","grey25")) +
    guides(fill = "none", color = "none") +
    labs(y = "Anteil am Gesamtwert", x = "Sortierte Beobachtungen") +
    theme_minimal(base_size = 11) +
    theme(panel.grid.major.x = element_blank(),panel.grid.minor.x = element_blank())
```

```{r gini_plot, echo=F, warning=F, message=F, out.height="75%", out.width="75%", fig.align="center"}
gdf <- baua %>% 
  mutate(across(F518_SUF,~ifelse(.x %in% 99998:99999,NA,.x)) ) %>%  
  select(intnr, F518_SUF) %>% 
  na.omit() %>% 
  arrange(F518_SUF) %>% 
  mutate(pct_id = 1:n()/n(),
         pct_di05 = cumsum(F518_SUF/sum(F518_SUF)),
         sampl = runif(n = nrow(.)))

gini_ann_df <- gdf %>% 
  filter(sampl<.0025, between(pct_id,.5,.6 ) ) %>% slice(1) %>%
  mutate(lab_lo = "Lorenzkurve", 
         labl = paste0("Lorenzkurve (aus Beobachtungen):\n\"",
                       round(pct_id*100,0),"% der Befragten verfügen über\n",
                       round(pct_di05*100,0),"% des gesamten HH-Einkommens\""),
         labl1 = paste0("(Theoretische) Diagonale:\n\"",
                       round(pct_id*100,0),"% der Befragten verfügen über\n",
                       round(pct_id*100,0),"% des gesamten HH-Einkommens\""),
         labl2 = "Gini Koeffizient:\nFläche zwischen Diagonale und Lorenzkurve\nim Verhältnis zu Fläche unter Lorenzkurve (weiß)\n(0=Gleichverteilung,1=Maximale Konzentration)")

ggplot(gdf, aes(x = pct_id, y = pct_di05)) + 
  geom_ribbon(aes(ymin = pct_di05, ymax = pct_id, xmin = pct_id, xmax = pct_id), fill = "#1BB6AF" ) +
  geom_line(aes(y = pct_id), linetype = 2, size = .95, color = "#172869") + # Diagonale
  geom_line(size = .5, color = "#9ED80B") + # Lorenzkurve
  geom_segment(data = gini_ann_df, aes(xend = pct_id, x = pct_id , y = 0, yend = pct_id), size = .35, linetype = 2) + # gestrichelte Linie
  geom_segment(data = gini_ann_df, aes(xend = pct_id, x = 0 , y = pct_id, yend = pct_id), size = .35, linetype = 2) + # gestrichelte Linie
  geom_segment(data = gini_ann_df, aes(xend = pct_id+.0035, x = pct_id+.05 , y = pct_di05- .15, yend = pct_di05-.0075), arrow = arrow(length = unit(.02,"npc"),type = "closed")) + # lorenzkurve pfeil
  geom_label(data = gini_ann_df, aes(label=labl,x = pct_id+.05, y = pct_di05- .15), hjust = 0, fontface = "italic", size = 3.2, 
             color = "grey25", fill = alpha("#9ED80B",.75)) + # lorenzkurve label
  geom_segment(data = gini_ann_df, aes(xend = pct_id, x = pct_id-.05 , y = pct_id + .3, yend = pct_id), arrow = arrow(length = unit(.02,"npc"),type = "closed")) + # diagonale pfeil
  geom_label(data = gini_ann_df, aes(label=labl1,x = pct_id-.05, y = pct_id + .3), hjust = 1, fontface = "italic", size = 3.2, 
             color = "grey95", fill = "#172869") + # diagonale label
  geom_segment(data = gini_ann_df, aes(y = .62, x = .03, xend = .4, yend = .3)) + # Fläche / Gini Linie
  geom_point(data = gini_ann_df, aes(y = .3, x = .4), shape = 15, size = 2.75) + # Fläche / Gini -> Rechteck statt Pfeil
  geom_label(data = gini_ann_df, aes(label=labl2,  y = .62, x = .025), hjust = 0, fontface = "italic", size = 3.2, 
             color = "grey15", fill = "#1BB6AF") + # Gini Label
  geom_point(data= filter(gdf,sampl<= 0.0151), size = 1.5, color = "#FF5300") +
  scale_x_continuous("Kumulierter Befragtenanteil", labels = scales::percent_format(accuracy = 1L)) +
  scale_y_continuous("Kumulierter Einkommensanteil", labels = scales::percent_format(accuracy = 1L)) +
  theme_minimal(base_size = 11) +
  coord_cartesian(x =c(0,1),y =c(0,1),expand = F)
```
Den Gini-Koeffizienten können wir mit `fastgini` berechnen, allerdings müssen wir diesen Befehl erst (einmalig) installieren:
```{stata giniF, eval = F}
ssc install fastgini
fastgini F518_SUF
```
```{stata giniT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode F518_SUF, mv(99998/99999)
fastgini F518_SUF
```

Leider funktioniert `by(S1)` bei `fastgini` nicht, wir müssen hier auf die Variante mit [`if`](#if_vgl) zurückgreifen:

```{stata gini2F, eval = F}
fastgini F518_SUF if S1 == 1
```
```{stata gini2T, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode F518_SUF, mv(99998/99999)
fastgini F518_SUF if S1 == 1
```

```{stata gini3F, eval = F}
fastgini F518_SUF if S1 == 2
```
```{stata gini3T, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode F518_SUF, mv(99998/99999)
fastgini F518_SUF if S1 == 2
```
$\Rightarrow$ *Wo sind die Einkommen also ungleicher verteilt?*[^033]

[^033]: Der etwas höhere Wert des Gini-Koeffizienten legt nahe, dass die Einkommen bei Frauen (`S1`=2) etwas stärker ungleich verteilt als bei Männern (`S1`=1).


Anmerkung: hier wurden die Missings in `F518_SUF` mit `mvdecode F518_SUF, mv(99998/ 99999)` ausgeschlossen.


## Übungen 3

### Übung 3-1 {#met}  


(@) Laden Sie den BIBB/BAuA Erwerbstätigenbefragung 2018 (`BIBBBAuA_2018_suf1.dta`). Analysieren Sie die Einkommensangaben (`F518_SUF`):
    + Denken Sie daran, die fehlenden Werte für `F518_SUF` mit als Missing zu überschreiben! (bspw. `mvdecode`, sehen Sie mit `labelbook F518` Codebuch nach, welche Werte als fehlende Angaben zu betrachten sind)
(@) Betrachten Sie die Einkommensangaben mit `summarize`
    + Für wie viele Beobachtungen haben Sie eine gültige Angabe (nicht-Missing)?
    + Wo liegt das arith. Mittel für die Einkommensangaben?
    + Wie können Sie sich den Median mit Hilfe von `summarize` ausgeben lassen?
    + Bei welchem Wert liegt die 75%-Perzentilsgrenze?
    + Sind die Einkommensangaben eher rechts- oder linksschief verteilt?

(@) Verwenden Sie jetzt `tabstat`, um folgende Kennzahlen für `F518_SUF` zu berechnen:
    + Das arithm. Mittel, den Median, das 25%- und 75%-Quartil sowie die Varianz und den Variationskoeffizienten - was sagen Ihnen die Werte jeweils?
    + Berechnen Sie nun alle Werte getrennt für Männer und Frauen (Variable `S1`) - welche Werte erhalten Sie jeweils für die Kennzahlen?
    + Vergleichen Sie die Werte! Wie unterscheiden sich die Einkommensangaben zwischen Männern und Frauen?
  
(@) Berechnen Sie den Gini-Koeffizenten für `F518_SUF`! (Denken Sie daran, vor der ersten Verwendung `fastgini` zu installieren - siehe [hier](#gini))


## Anhang Kap 3

### Arithmetisches Mittel {#mean}

Das arithmetische Mittel wird häufig auch einfach als Durchschnitt bezeichnet:

$$\bar{x} = \frac{\sum_{i=1}^{n}{x_i}}{n}$$

Setzt metrische Daten (also mind. Intervallskalenniveau) voraus: wir interpretieren die Abstände zwischen den Werten.


### Median {#median}

+ Anordnung aller Werte einer Variable nach Größe
+ Da Rangfolge nötig: mindestens Ordinalskalenniveau
+ Median  ist mittlerer Wert dieser geordneten Variable (zentraler Wert)
+ 50% der Werte sind niedriger, 50% der Werte sind höher



### Perzentile erklärt {#perc}

+ Verallgemeinerung des Konzepts des Medians (auch: Quantile)
+ Ein p-Quantil teilt die Daten in zwei Teile, so dass mindestens ein Anteil p der Daten kleiner/ gleich und ein Anteil 1–p grösser/ gleich dem p-Quantils-Wert xp ist.
+ Die Ermittlung von p-Quantilen erfolgt analog zur Bestimmung des Medians (der Median entspricht dem 50%-Quantil).
+ Häufig verwendete Quantilsarten: Quartile (25%-, 50%-, 75%-Grenzen), Quintile (20%-, 40%-, 60%-, 80%-Grenzen), Dezile (10%,20%,30%,...)

+ Datensatz 1:  1, 2, 3, 4, 5
  + Mean: `r mean(1:5)`
  + Median: `r median(1:5)`
  + 20%-Perzentil: 2
  
+ Datensatz 2:  1, 2, 3, 4, *10*
  + Mean: `r mean( c(1:4,10))`
  + Median: `r median(c(1:4,10))`
  + 20%-Perzentil: 2


```{r percplot1, echo=F, warning=F, message=F, out.height="38%", out.width="100%", fig.align="center"}
m_df <-  data.frame(y= 1:5, y2 = c(1:4,10),id=1:5)
library(patchwork)
pp1 <- 
  m_df %>% 
  ggplot(aes(y=id,x=y)) +
  geom_col(color = "grey25", aes(fill = factor(id,levels=5:1))) +
  geom_segment(data=data.frame(x=3,y1=1.1,y2=4.9),size = 1,
               aes(xend=y2,x=y1,yend=3,y=3), color = "#48211A") + # median linie
  geom_segment(data=data.frame(y1=1.05,y2=4.9), size = 1, linetype = 2,
                aes(xend=y2,x=y1,yend=3,y=3), color = "orange") + # mean linie
  geom_label(data= data.frame(lab1="Median"), size = 3,
             aes(y=3.15,x=0,label=lab1),fill= "#48211A",color="grey96",label.size = 0,hjust=0,label.padding = unit(.3,"lines")) +  # median label
  geom_label(data= data.frame(lab1="arithm. Mittel"), size = 3,
               aes(y=3.15,x=4.1,label=lab1),fill= "orange",color="grey20",label.size = 0,hjust=0,label.padding = unit(.3,"lines")) + # mean label
  geom_segment(data=data.frame(x=2,y1=1.1,y2=4.9), size = 1,
               aes(xend=y2,x=y1,yend=2,y=2), color = "#376597") + #20% linie
  geom_label(data= data.frame(lab1="20% Perzentil"),size = 3,
             aes(y=1.65,x=3.5,label=lab1),fill= "#376597",color="grey96",label.size = 0,hjust=0,label.padding = unit(.3,"lines")) + # 20% label
  scale_fill_manual(values = paletteer::palettes_d$dutchmasters$milkmaid[c(12,6:13)]) +
  scale_x_continuous(breaks = 1:5, labels = c("kleinster","2.kleinster","3","zweitgrößter","größter")) +
  labs(y = "Ausprägungwert", x = "Sortierte Beobachtungen", title = "Datensatz 1") +
  guides(fill = "none", color = "none") +
  theme_minimal(base_size = 11) +
  coord_cartesian(ylim = c(0,10))+
  theme(panel.grid.major.x = element_blank(),panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(size=rel(.5)))
  
  
pp2 <-  
  m_df %>% 
    ggplot(aes(y=y2,x=y)) +
    geom_col(color = "grey25", aes(fill = factor(id,levels=5:1))) +
    geom_segment(data=data.frame(y1=.95,y2=5.1), size = 1, 
                 aes(xend=y2,x=y1,yend=4,y=4), color = "orange") + # mean linie
    geom_segment(data=data.frame(x=3,y1=.95,y2=5.1),size = 1,
                 aes(xend=y2,x=y1,yend=3,y=3), color = "#48211A") + # median linie
    geom_segment(data=data.frame(x=2,y1=.95,y2=5.1), size = 1,
                 aes(xend=y2,x=y1,yend=2,y=2), color = "#376597") + #20% linie
    geom_label(data= data.frame(lab1="arithm. Mittel"),size = 3,
               aes(y=4.15,x=4.1,label=lab1),fill= "orange",color="grey20",
               label.size = 0,hjust=0,label.padding = unit(.3,"lines")) + # mean label
    geom_label(data= data.frame(lab1="Median"),size = 3,
               aes(y=3.15,x=0,label=lab1),fill= "#48211A",color="grey96",
               label.size = 0,hjust=0,label.padding = unit(.3,"lines")) + # median label
    geom_label(data= data.frame(lab1="20% Perzentil"),size = 3,
               aes(y=1.65,x=3.5,label=lab1),fill= "#376597",color="grey96",
               label.size = 0,hjust=0,label.padding = unit(.3,"lines")) + # 20% label
    scale_fill_manual(values = paletteer::palettes_d$dutchmasters$milkmaid[c(11,6:13)]) +
    scale_x_continuous(breaks = 1:5, labels = c("kleinster","2.kleinster","3","zweitgrößter","größter")) +
    labs(y = "Ausprägungwert", x = "Sortierte Beobachtungen", title = "Datensatz 2") +
    guides(fill = "none", color = "none") +
    theme_minimal(base_size = 11) +
    theme(panel.grid.major.x = element_blank(),panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(size=rel(.5)))

pp1 + pp2 + plot_layout(ncol= 2)
```




### Varianz erklärt {#variance}
Die häufigste Kennzahl zur Beschreibung von Streuung ist aber die **Varianz**. Die Varianz ist definiert als die durchschnittliche quadrierte Abweichung vom arith. Mittel: $$var(x) = \frac{\sum_{i=1}^{n} (x_{i}-\bar{x})^2}{n}$$    


Schauen wir erst in den Zähler des Bruchs: $(x_{i}-\bar{x})^2$
Es geht also um die Abstände der einzelnen Datenpunkte ($x_{i}$) zum arithmetischen Mittel ($\bar{x}$) - mathematisch die Differenz zwischen den einzelnen Datenpunkten $x_{i}-\bar{x}$ - hier eingezeichnet als gestrichelte Linien:
```{r var_graph,dpi = 800, echo = F, out.height="70%", out.width="70%", fig.align="center"}
pfad <- "D:/Studium/01_Oldenburg/Lehre/Datensaetze/"
  a18 <- read.csv(paste0(pfad,"allbus_2018.csv"),sep = ";",stringsAsFactors = F,header=T)

eq <-  substitute(italic(bar(x))) # formel erstellen

a18 %>% 
  select(respid,age) %>% 
  filter(age > 0) %>% 
  na.omit() %>%  
  sample_n(30) %>% 
  select(respid, age) %>% 
  mutate(mean = mean(a18$age,na.rm = T), id = 1:n()) %>% 
  ggplot(.,aes(x = id, y = age)) +
  geom_segment(aes(yend= mean, xend = id ),
               size = .45, linetype = 2, color = "#1BB6AF") +
  geom_point(size = 1.95, color = "#172869") +
  geom_hline(aes(yintercept = mean), color = "#AF6125", size = .75) +
  geom_label(aes(x = min(id)-.75, y = mean, label = as.character(as.expression(eq))), label.size = .01, hjust = 0.5, color = "#AF6125", fill = "white", parse = T, size = 4.5) +
  guides(color = "none") +
  labs(y = "Alter", x = "") +
  theme_minimal(base_size = 13) +
  expand_limits(y = 15) +
  theme(axis.text.x = element_blank(), plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot")
```
Für Punkte, die über dem arith. Mittel liegen, erhalten wir für die Differenz einen positiven Wert (wir ziehen ja jeweils das arith. Mittel vom Wert für den Datenpunkt ab). Für Punkte mit Werten, die kleiner als das arith. Mittel sind, erhalten wir einen negativen Wert.     
Würden wir die Abstände (gestrichelten Linien) einfach aufsummieren, erhielten wir als Ergebnis immer Null! Das arithmetische Mittel liegt ja per Definition immer genau "in der Mitte": die Abstände nach oben sind in der Summe genauso groß wie die Abstände nach unten. Daher wird zunächst jeder Abstand quadriert: $(\bar{x} - x_{i})^2$   
Der Rest der Formel gibt dann an, dass alle quadrierten Differenzen aufsummiert ($\sum_{i=1}^{n}$) und dann durch die Anzahl der Beobachtungen geteilt werden ($\frac{}{n}$)- zusammengefasst hier nochmal die Formel:  $$var(x) = \frac{\sum_{i=1}^{n} (x_{i}-\bar{x})^2}{n}$$

[Zurück zu Streuungsmaße](#streu)    

### Schiefe & Wölbung {#sk_kur}

Die Schiefe (skewness) ist ein Maß für die Asymmetrie einer Verteilung. Bei einer symmetrischen Verteilung beträgt die Schiefe 0. Ein negativer Wert für die Schiefe deutet darauf hin, dass Median > Mean und Verteilung wird als links-schief/rechts-steil bezeichnet. Bei einem positiven Wert der Schiefe ist Median < Mean und die Verteilung ist rechts-schief/links-steil. 


```{r skewplot1, echo=F, warning=F, message=F, out.height="25%", out.width="100%", fig.align="center"}
sk_df <-  
  data.frame(x= 1:10) %>%
  mutate(y1 = 13-(round(4*x/4)+ 2),
         y2 = round(4*x/4)+ 2,
         y3 = as.numeric(round(20 - abs(scale(x))*9.5)),
         y4 = ifelse(x %in% 5:6, 10,1) ) %>% 
  bind_rows(data.frame(x=0,y1=8,y2=2,y3=1,y4=1),
            data.frame(x=11,y1=1,y2=9,y3=1,y4=1)) 

skew1 <- 
  sk_df %>% 
  ggplot(aes(x=x,y=y2)) +
  geom_col(fill = paletteer::paletteer_d("dutchmasters::pearl_earring")[1]) +
  labs(title = "negative Schiefe:\n(rechtssteil/linksschief)" ) + 
  theme_minimal(base_size = 11) +
  theme(panel.grid= element_blank(),
        axis.title = element_blank(), axis.text = element_blank(),
        plot.title = element_text(hjust = .5, size = rel(.8)))
skew2 <- 
  sk_df %>% 
  ggplot(aes(x=x,y=y3)) +
  geom_col(fill = paletteer::paletteer_d("dutchmasters::pearl_earring")[3]) +
  coord_cartesian(ylim=c(0,19)) +
  labs(title = paste0("Schiefe = 0:\nsymmetrisch")) +
  theme_minimal(base_size = 11) +
  theme(panel.grid= element_blank(),
        axis.title = element_blank(), axis.text = element_blank(),
        plot.title = element_text(hjust = .5, size = rel(.8)))

skew3 <- 
  sk_df %>% 
  ggplot(aes(x=x,y=y1)) +
  geom_col(fill = paletteer::paletteer_d("dutchmasters::pearl_earring")[4]) +
  labs(title = "positive Schiefe:\n(linkssteil/rechtsschief)") +
  theme_minimal(base_size = 11) +
  theme(panel.grid= element_blank(),
        axis.title = element_blank(), axis.text = element_blank(),
        plot.title = element_text(hjust = .5, size = rel(.8)))

  
  
skew1 + skew2 + skew3 + plot_layout(ncol = 3)
```

Die Wölbung (Kurtosis) ist ein Maß für die Steilheit bzw. "Spitzigkeit" einer Verteilung. Je kleiner der Wert der Kurtosis, desto desto flacher die Verteilung. Bspw. hat die Normalverteilung hat eine Kurtosis von 3.
```{r kurtplot, echo=F, warning=F, message=F, out.height="25%", out.width="100%", fig.align="center"}
kurt_df <-  
  data.frame(x= 1:10) %>%
  mutate(y3 = 30,
         y4 = round(dt(1:10 - 5.5,df=2) * 100)) %>% 
  bind_rows(data.frame(x=0 ,y3=1,y4=0),
            data.frame(x=11,y3=1,y4=0)) 

kurt1 <- round(kurt_df %>% uncount(weights = y3) %>% pull(x) %>% moments::kurtosis(.),2)
pkurt1 <- 
  kurt_df %>% 
  ggplot(aes(x=x,y=y3)) +
  geom_col(fill = paletteer::paletteer_d("dutchmasters::pearl_earring")[7]) +
  labs(title = paste0("Wölbung = ",kurt1)) +
  coord_cartesian(ylim = c(0,30.5)) + 
  theme_minimal(base_size = 11) +
  theme(panel.grid.major.x = element_blank(),panel.grid.minor.x = element_blank(),
        axis.title = element_blank(), 
        plot.title = element_text(hjust = .5))

kurt2 <- round(kurt_df %>% uncount(weights = y4) %>% pull(x) %>% moments::kurtosis(.),2)
pkurt2 <- 
  kurt_df %>% 
  ggplot(aes(x=x,y=y4)) +
  geom_col(fill = paletteer::paletteer_d("dutchmasters::pearl_earring")[8]) +
  labs(title = paste0("Wölbung = ",kurt2)) +
  coord_cartesian(ylim = c(0,30.5)) + 
  theme_minimal(base_size = 11) +
  theme(panel.grid.major.x = element_blank(),panel.grid.minor.x = element_blank(),
        axis.title = element_blank(), 
        plot.title = element_text(hjust = .5))
pkurt1 + pkurt2 + plot_layout(ncol = 2)
```


In Stata bekommen wir die Schiefe und Wölbung einer Verteilung mit `summary varname,detail` oder mit `tabstat varname, s(skewness kurtosis)` angezeigt.

Mit `hist variablenname` bekommen wir ein Histogramm ausgegeben, welches einen Überblick wie die oben gezeigten Darstellungen bietet.


### Kennzahlvergleich mit `tabulate` und der Option `summarize()` 

Wir können aber auch zwei Variablen für den Kennzahlenvergleich verwenden. Bspw. sind neben Ost-West-Vergleichen auch häufig Geschlechterunterschiede von Interesse. Stata stellt uns damit mit der `summarise`-Option eine einfache Möglichkeit zur Verfügung:

```{stata tabsuF, eval = F}
tab gkpol S1, summarize(zpalter)
```
```{stata tabsuT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
tab gkpol S1, summarize(zpalter)
```
> <span style="color:#FFA500FF"><i>Leider funktioniert die Darstellung hier nicht richtig - in Stata werden die Label richtig angezeigt</i></span>

Wir sehen hier, dass  524 Männer aus Wohnorten unter 2000 Einwohnern befragt wurden, die im Durchschnitt 47.30 Jahre alt sind. Die Standardabweichung des Alters in dieser Gruppe beträgt 11.895558 Jahre. Außerdem wurden 785 Frauen aus Wohnorten mit  50.000 bis 99.000 Einwohnern befragt, die im arith. Mittel 48.32 Jahre alt sind und deren Altersangaben eine Standardabweichung 10.9279 aufweisen.

Je nach Präferenz können wir eine der drei Kennzahlen (Anzahl der Beobachtungen, arith. Mittel und Standardabw.) ausblenden in dem wir die entsprechende Option verwenden:

+ `nofreq`               Häufigkeiten ausblenden
+ `nomeans`              arith. Mittel ausblenden
+ `nostandard`           Standardabw. ausblenden
  
`tab gkpol S1, summarize(age) nofreq` würde uns also nur die arith. Mittel und Standardabweichungen ausgeben.  







<!--chapter:end:03_Deskription.Rmd-->

# if-Bedingungen {#iflab} 

```{r setup4, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
# baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F)
```

## if-Bedingungen - nur manche Zeilen ansehen

Bisher haben wir uns immer auf den gesamten Datensatz bezogen.
Häufig möchten wir aber nur bestimmte Beobachtungen berücksichtigen. 
Ein Beispiel war gerade eben schon der `fastgini`-Befehl, der keine `by`-Option hat. Wir können mit einer `if`-Bedingung mitteilen, dass wir lediglich die Beobachtungen mit `S1==2` berücksichtigen möchten (also Frauen):

```{stata gini5F, eval = F}
fastgini F518_SUF if S1 == 2
```

Stata geht dann also alle Zeilen durch und verwendet nur diejenigen mit der entsprechenden Ausprägung.
Einige Beispiele `list` zeigen die Möglichkeiten vielleicht etwas besser:
Mit einer `if`-Bedingung können wir uns die Variablen `S1` `S3` `S2_j` und `zpalter` für 81-jährigen Befragten anzeigen lassen:
```{stata if1, eval = F}
list S1 S3 S2_j zpalter if zpalter == 81
```
```{stata if2, eval = F}
       +--------------------------------------+
       |       S1         S3   S2_j   zpalter |
       |--------------------------------------|
 5114. |  männlic   Abitur /   1936        81 |
 5179. |  männlic   Abitur /   1936        81 |
 6118. |  männlic   Realschu   1936        81 |
 9045. |  männlic   Abitur /   1936        81 |
10517. | weiblich   Hauptsch   1936        81 |
       +--------------------------------------+
```
Wichtig ist hier `==` - ein einfaches `=` wird in Stata für die Zuweisung von Werten verwendet wie wir noch sehen werden.
Für `if`-Bedingungen stehen uns die üblichen Operatoren zur Verfügung:
`>`, `<`, `==`, `>=`, `<=`, `!=` (ungleich)

Dementsprechend können wir so Befragte auswählen, die vor 1936 geboren wurden:
```{stata if3, eval = F}
list S1 S3 S2_j zpalter if S2_j < 1936
```

Das funktioniert auch mit anderen Befehlen, wie z.B. `tabulate` - so können wir beispielsweise auszählen, wie viele 81-jährige Frauen und Männer im Datensatz vorhanden sind:
```{stata if4, eval = F}
tabulate S1 if zpalter == 81
```

```{stata if5, echo = F, collectcode = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tab S1 if zpalter == 81
```
Wir sehen unter anderem an der Fallzahl (Total) in dieser Tabelle, dass hier nicht alle Fälle aus dem Datensatz berücksichtigt werden.

Wir können auch mehrere Bedingungen setzen. Sollen beide zutreffen, verbinden wir die Bedingungen mit `&` - zB. wenn wir 81-jährige Befragte mit einem Bruttoverdienst von unter 1000 Euro auswählen möchten:
```{stata if6, eval = F}
list S1 if zpalter > 81 & F518_SUF < 1000
```

Mit `|` können wir hingegen Fälle auswählen, für welche die ein *oder* die andere Bedingung zutrifft:
```{stata if7, eval = F}
tabulate S1 if zpalter == 81 | F518_SUF < 1000 // 81 Jahre alt oder unter 1000EUR Einkommen
```

Mit `inrange` können wir Auswahlen auf einen Wertebereich eingrenzen - d.h. diese beiden Auswahlen führen zum gleichen Ergebnis (nur Befragte, die zwischen 20 und 30 Jahre alt sind):
```{stata tabif1, eval = F}
tabulate S1 if zpalter >= 20 & zpalter <= 30
tabulate S1 if inrange(zpalter,20,30)
```

Mit `inlist` können wir spezifische Werte angeben und so lange Folgen von `|` vermeiden:
```{stata browseif, eval = F}
tabulate S1 if zpalter == 15 | zpalter == 79 | zpalter == 80 | zpalter == 82 
tabulate S1 if inlist(zpalter,15,79,80,82)
```


## Labels 

Genauso könnten wir auch vorgehen wenn wir nur männliche oder weibliche Befragte betrachten möchten. Hier nochmal die Auszählung der Variable `S1`:
```{stata tab3, eval = F}
tabulate S1
```

```{stata tab3b, echo = F, collectcode=F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tab S1
```

Allerdings sind `männlich` und `weiblich` lediglich *Variablenlabels*. D.h. hier wurden Zahlencodes wieder mit Beschreibungen verbunden. Wir können diese Labels nicht für `if`-Bedingungen verwenden:
```{stata if_lab, eval = F}
tab S1 if S1 == "weiblich"
```
<span style="color:red">`type mismatch`</span>  
<span style="color:blue">`r(109);`</span>

Im Hintergrund ist `S1` nämlich eine numerische Variable
```{stata if_lab2, eval = F}
describe S1
```
```{stata if_lab2b, echo = F, collectcode=F}
set linesize 90
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
describe S1
```
Unter `value label` sehen wir, dass hier das Label `S1` angefügt wurde.

Wir können die eigentlichen Werte in `tabulate` mit der Option `,nol` ausblenden: 
```{stata if_lab3, eval = F}
tabulate S1, nol
```
```{stata if_lab3b, echo = F, collectcode=F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tabulate S1,nol
```
Wenn wir also nach Frauen filtern möchten, müssen wir den entsprechenden Zahlencode angeben:

```{stata keep_l3, eval = F}
tab S1 if S1 == 2
```

```{stata keep_l4, echo = F, collectcode=F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tabulate S1 if S1 == 2
```

Mehr zu labels [hier](#adva)

## Fehlende Werte {#miss_if}

Eine häufige Fehlerquelle bei fehlende Werten in Stata ist deren Verhältnis zu `>`. 
Fehlende Werte werden nämlich von Stata als "unendlich groß" gewertet! 
Wenn wir uns Befragte mit einem Alter über 92 ausgeben lassen, dann bekommen wir auch diejenigen ohne Altersangabe ausgegegeben:

```{stata list_if, eval = F}
list S1 S3 zpalter if zpalter > 81
```
```{stata list_if2, eval = F}
       +-------------------------------+
       |       S1         S3   zpalter |
       |-------------------------------|
   39. | weiblich   Abitur /         . |
  514. | weiblich   Abitur /        83 |
  657. |  männlic   Hauptsch         . |
  823. | weiblich   Realschu         . |
  982. | weiblich   keine An         . |
       |-------------------------------|
       |      150 weitere Zeilen       |
```

Es gibt zwei Möglichkeiten, dies zu umgehen: 

+ entweder wir verwenden `inrange` - wobei wir hier eine Obergrenze angeben müssen und außerdem die Untergrenze immer mit eingeschlossen wird. Wenn wir also nur Fälle sehen wollen, die *älter* als 81 sind, dann müssen wir einen Wert über 81 angeben:
```{stata list_if3, eval = F}
list S1 Bula zpalter if inrange(zpalter,81.5,100)
```

+ oder wir hängen den `missing()`-Operator an die Bedingung an. Mit `missing(zpalter)` können wir alle Zeilen auswählen, für die `zpalter` *missing* ist. Wenn wir diesen mit Hilfe eines `!` verneinen, können wir die fehlende Fällen ausschließen:
```{stata list_if5, eval = F}
list S1 Bula zpalter if zpalter > 81 & !missing(zpalter)
```

Beide Befehle führen zum gleichen Ergebnis:
```{stata list_if6, eval = F}
       +-------------------------------+
       |       S1       Bula   zpalter |
       |-------------------------------|
  514. | weiblich    Hamburg        83 |
 6438. |  männlic   Nordrhei        82 |
11786. |  männlic     Berlin        87 |
       +-------------------------------+
```

## keep & drop: Fälle dauerhaft löschen

Manchmal möchten wir nur bestimmte Beobachtungen im Datensatz behalten. Beispielsweise möchten wir für eine Analyse nur Befragte, die jünger als 30 Jahre sind, im Datensatz behalten. Dazu können wir `keep` oder `drop` die gewünschten Fälle behalten bzw. die ungewünschten ausschließen:
```{stata keep, eval = F}
keep if zpalter < 30
```
Alternativ können wir auch mit `drop` alle Befragten aus dem Datensatz löschen, die 60 Jahre oder älter sind:
```{stata drop, eval = F}
drop if zpalter >= 30
```

Mit `describe, short` sehen wir, dass wir jetzt deutlich weniger Fälle im Speicher haben:
```{stata dropdesc1, eval = F}
describe, short
```
```{stata dropdesc, echo = F,collectcode=F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui drop if zpalter >= 30
describe, short
```


## Übungen 4 {#ifue}

(@) Laden Sie den BIBB/BAuA Erwerbstätigenbefragung 2018 (`BIBBBAuA_2018_suf1.dta`). 

(@) In der Variable `intnr` ist die Interviewnummer abgelegt. Lassen Sie sich die Interviewnummer (`intnr`), Alter (`zpalter`), Geschlecht (`S1`) und die Wohnortgröße (`gkpol`) für den\*die Befragte mit der Interviewnummer 2388097 ausgeben. 

(@) Ersetzen Sie die Werte 9999 in `zpalter` mit `.`: `mvdecode zpalter, mv(9999)`

(@) Lassen Sie sich die Wohngröße (`gkpol`) für alle Befragte mit fehlender Altersangabe ausgeben.

(@) Lassen Sie sich eine Häufigkeitsauszählung der Wohnortgröße (`tab gkpol`) für folgende Gruppen ausgeben:
  + Befragte, die jünger als 30 Jahre alt sind (das Alter ist in `zpalter` abgelegt)
  + Befragte, die älter als 60 Jahre alt sind (Denken Sie an den Umgang mit Missings!)
  + Befragte, die jünger als 30 Jahre alt und weiblich sind (das Geschlecht ist in `S1` erfasst)
  + Befragte, die zwischen 30 und 60 Jahre alt sind
  + Befragte, die jünger als 30 Jahre alt sind oder älter als 60 Jahre alt sind  
  
  
(@) Löschen Sie alle Beobachtungen von Befragten aus Städten mit 500.000 Einwohnern und mehr aus dem Speicher. Denken Sie an die Unterscheidung zwischen Labels und Zahlenwerten.



<!--chapter:end:04_if_label.Rmd-->

# Variablen erstellen und verändern {#gen} 

```{r setup5, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
# baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F)
```


Natürlich sind wir nicht nur darauf beschränkt, bestehende Variablen anzusehen, sondern wir können auch neue Variablen erstellen. Das geht mit `gen`. Dazu geben wir erst den neuen Variablennamen an und nach `=`, wie die neue Variable bestimmt werden soll:
```{stata gen, echo = T, eval = F }
gen alter_mon = zpalter * 12
su zpalter
```

```{stata gen_b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
gen alter_mon = zpalter * 12
su alter_mon
```

Wenn wir eine Variable überschreiben möchten, dann müssen wir diese erst mit `drop` löschen, bevor wir sie überschreiben. Würden wir den `gen` Befehl von gerade nochmal verwenden, dann bekommen wir eine Fehlermeldung:
```{stata gen2, eval = F}
gen alter_mon = zpalter * 12
```
```{stata gen2_b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
qui gen alter_mon = zpalter * 12
gen alter_mon = zpalter * 12
```
Wir müssen den Namen `age_mon` also erst wieder frei machen, dann funktioniert der Befehl auch:
```{stata, eval = F}
drop alter_mon
gen alter_mon = zpalter * 12
```

## gen ist gut, Kontrolle ist besser

Wie gerade gesehen gibt uns Stata aber keinerlei Erfolgsmeldungen. Nach der Bearbeitung oder Neuurstellung von Variablen sollte ein Blick in die Daten folgen. Dazu empfiehlt es sich, wieder auf den `browse` Befehl zurückzugreifen:

```{stata bro_ansicht_cmd, eval = F}
browse intnr zpalter alter_mon 
```

```{r bro_ansicht, echo = F,out.width = "90%",fig.height= 3.5, fig.align="center"}
knitr::include_graphics("./pics/05_stata_browse_kontrolle.png")
```  

In Kombination mit `if` können wir auch einige Spezialfälle betrachten, z.B. ob die Missings richtig verarbeitet wurden:
```{stata bro_ansicht_cmd2, eval = F}
browse intnr zpalter alter_mon  if missing(zpalter)
```
```{r bro_ansicht2, echo = F,out.width = "90%",fig.height= 3.5, fig.align="center"}
knitr::include_graphics("./pics/05_stata_browse_kontrolle2.png")
```  

Häufig empfiehlt sich auch ein `summarize` und ein Vergleich der Missingszahl der alten und neuen Variable mit `mdesc`:
```{stata check, eval = F}
summarize zpalter alter_mon
mdesc zpalter alter_mon
```


```{stata check2, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
qui gen alter_mon = zpalter * 12
summarize zpalter alter_mon
mdesc zpalter alter_mon
```

## Dummy-Variablen erstellen {#dummyvar}

Wir können auch die Operatoren aus den if-Bedingungen verwenden, um eine Variable zu generieren. Beispielsweise könnten wir eine Dummy-Variable bilden, ob die Befraten im gleichen Bundesland wohnen (`Bula`) und arbeiten (`F233`), indem wir den `==` Operator verwenden. Die entstehende Variable enthält dann immer eine 1 wenn beide Werte gleich sind, unterschiedliche Werte ergeben eine 0:
```{stata penl1, eval = F}
mvdecode Bula F233, mv(97/99)
gen ao_wo = Bula ==  F233
```
**Die Ergebnisse solcher Veränderungen sollten immer überprüft werden!**
```{stata penl2, eval = F}
tab ao_wo
browse Bula F233 ao_wo
```

```{stata penl2c, eval = F}
list Bula F233 ao_wo in 157/160
```

```{stata penl2d, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode Bula F233, mv(97/99)
qui gen ao_wo = Bula ==  F233
list Bula F233 ao_wo in 157/160, noobs
```

Allerdings haben wir für `F233` doch missings definiert - was passiert mit denen?
```{stata penlMiss, eval = F}
mdesc Bula F233 ao_wo 
```
```{stata penl2Miss, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode Bula F233, mv(97/99)
qui gen ao_wo = Bula ==  F233
mdesc Bula F233 ao_wo
```



Leider gleicht Stata auch Missings mit gültigen Werten ab und vergibt dann dementsprechend 0 oder 1. 
Um die Missings als Missing zu behalten, müssen wir mit entsprechenden `if`-Bedingungen die Zeilen mit Missings in `Bula` und `F233` ausschließen:

```{stata penl3Miss, eval= F}
drop ao_wo // löschen
gen ao_wo =  Bula ==  F233 if !missing(F233) & !missing(Bula)
mdesc Bula F233 ao_wo
```

```{stata penl4Missb, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode Bula F233, mv(97/99)
qui gen ao_wo =  Bula ==  F233 if !missing(F233) & !missing(Bula)
mdesc Bula F233 ao_wo
```


```{stata penl40, eval= F}
tab ao_wo
```


```{stata penl40b, echo= F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode Bula F233, mv(97/99)
qui gen ao_wo =  Bula ==  F233 if !missing(F233) & !missing(Bula)
tab ao_wo
```

## Neue Variablen labeln {#label}

Die so erstellte Dummy-Variable können wir auch labeln. 
Dazu definieren wir zunächst ein Wertelabel. 
Dazu verwenden wir `label define`, gefolgt von einem Objektnamen für dieses Label (hier `aowo_lab`) und dann jeweils die Ausprägungen zusammen mit dem entsprechenden Label in "". Dieses Label-Objekt wenden wir dann mit `label values` auf die Variable `ao_wo` an:

```{stata penl4, eval= F}
label define aowo_lab 0 "ungleich" 1 "gleich"
label values ao_wo aowo_lab
tab ao_wo
```


```{stata penl4b, echo= F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode Bula F233, mv(97/99)
qui gen ao_wo =  Bula ==  F233 if !missing(F233) & !missing(Bula)
label define aowo_lab 0 "ungleich" 1 "gleich"
label values ao_wo aowo_lab
tab ao_wo
```

Wenn wir das Label verändern, sehen wir das anschließend auch in `tabulate`:
```{stata penl5, eval= F}
label define aowo_lab 0 "Ungleich" 1 "Gleich", replace
tab ao_wo
```

```{stata penl5b, echo= F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode Bula F233, mv(97/99)
qui gen ao_wo =  Bula ==  F233 if !missing(F233) & !missing(Bula)
label define aowo_lab 0 "Ungleich" 1 "Gleich", replace
label values ao_wo aowo_lab
tab ao_wo
```

*** 

[**Übung 5-1**](#ue51)

*** 

## Bestehende Variablen verändern

Natürlich können wir auch bestehende Variable verändern, ein Beispiel hatten wir mit [`mvdecode`]() bereits kennen gelernt. Während es bei `mvdecode` ja aber nur um Missings geht, gibt es auch Möglichkeiten die gültigen Werte zu verändern.

### `recode` {#recode}

Mit `recode` können wir Werte in einer bestehenden Variable verändern. Die veränderten Werte können wir in der bestehenden Variable überschreiben. Die häufig bessere Variante ist aber, die Originalwerte zu behalten und die veränderten Werte in einer neuer Variable abzulegen. Das geht mit der Option `,into(neuer_variablenname)`

Beispielsweise könnten wir `gkpol` zu weniger Kategorien zusammenfassen - zur Erinnerung, das waren die Originalausprägungen:
```{r gkpoltab, echo=F}

 tibble::tribble(~"Wert",~"Label", 
 1 , "unter 2.000 Einwohner",
 2 , "2.000 bis unter 5.000 Einwohner",
 3 , "5.000 bis unter 20.000 Einwohner",
 4 , "20.000 bis unter 50.000 Einwohner",
 5 , "50.000 bis unter 100.000 Einwohner",
 6 , "100.000 bis unter 500.000 Einwohner",
 7 , "500.000 und mehr Einwohner") %>% 
  bind_cols(data.frame("Kategorien"=c(rep("1 - Klein",3),rep("2 - Mittel",2),rep("3 - Groß",2)))) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 10) %>% 
  column_spec(1,monospace = TRUE) %>% 
  collapse_rows(columns = 3, valign = "top") %>% 
  add_header_above(c("gkpol" = 2, "neue Variable" = 1))
```

Mit `recode` können wir diese Ausprägungen zusammenfassen, indem wir immer (`alt`=`neu`) angeben. Umcodierungen sollten immer mit `tab alt neu` überprüft werden:
```{stata recode1, eval = F}
recode gkpol (2=1) (3=1) (4=2) (5=2) (6=3) (7=3), into(gkpol2)
tab gkpol gkpol2
```

```{stata recode1a, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
recode gkpol (2=1) (3=1) (4=2) (5=2) (6=3) (7=3), into(gkpol2)
tab gkpol gkpol2
```
Nicht erwähnte Ausprägungen werden einfach übernommen - daher ist `1` hier nicht aufgeführt.

Ein Vorteil von recode ist, dass wir direkt Labels vergeben können, die wir einfach in `""` anhängen - nochmal der gleich Befehl mit direkten Labels:
```{stata recode2, eval = F}
drop gkpol2 // nochmal neu
recode gkpol (2=1 "Klein") (3=1 "Klein") (4=2 "Mittel") (5=2 "Mittel") (6=3 "Groß") (7=3 "Groß"), into(gkpol2)
```
```{stata recode2a, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
recode gkpol (2=1 "Klein") (3=1 "Klein") (4=2 "Mittel") (5=2 "Mittel") (6=3 "Groß") (7=3 "Groß"), into(gkpol2)
tab gkpol gkpol2
```

**Tricks**, die alle aber zum gleichen Ergebnis führen:

Wir können auch mehrere Werte vor dem `=` angeben:
```{stata recode3, eval = F}
drop gkpol2
recode gkpol (1 2 3=1 "Klein") (4 5=2 "Mittel") (6 7=3 "Groß"), into(gkpol2)
```

Außerdem können wir mit Hilfe von `/` auch Wertebereiche ansprechen:
```{stata recode4, eval = F}
drop gkpol2
recode gkpol (1/3=1 "Klein") (4/5=2 "Mittel") (6/7=3 "Groß"), into(gkpol2)
```


Wir können in `recode` auch folgende Hilfsausdrücke verwenden, bspw. `recode gkpol .... (6/max =3 "Groß"),....`:
```{r egnspec, echo=F}
tribble(~"expr",~"expl",
        "min" 	  	,"Minimalwert",
        "max" 	  	,"Maximalwert (missings werden hier ausnahmsweise nicht mit eingeschlossen)",
        "else or *" ,"alle anderen Werte",
        "miss" 	  	,"alle missing Werte, die nicht von einer anderen Regeln angesprochen werden",
        "nonmiss" 	,"alle nicht-missing Werte, die nicht von einer anderen Regeln angesprochen werden") %>% 
    kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 10) %>% 
  column_spec(1,monospace = TRUE)  %>% 
    row_spec(0, color = "white")

```

*** 

[**Übung 5-2**](#ue52)

*** 

### `replace`: Informationen aus mehreren Variablen in einer Variable zusammenführen

Ein weiterer nützlicher Befehl ist `replace`. Hier können wir bestehende Variable verändern. Das ist inbesondere in Zusammenspiel mit `if`-Bedingungen hilfreich.
Manchmal liegt eine interessierende Information aufgeteilt auf mehrere Variablen vor. Ein Beispiel hierfür ist die Erwerbskonstellation von Paaren in der Erwerbstätigenbefragung. Wir können aus den Informationen `F1600` (Familienstand), `F1601` (leben die Befragten mit der\*der Partner\*in zusammen?) und `F1603` (ist Partner\*in berufstätig?) eine Variable mit 3 Ausprägungen bauen:

+ 1 "nicht verh./alleine lebend" 
+ 2 "verh. & 2 Erwerbspersonen im HH" 
+ 3 "verh. & 1 Erwerbsperson"

```{stata replace1, eval = F}
gen erw_hh = . 			// leere Variable erstellen
replace erw_hh = 1 if F1601 == 2 // alleine -> leben nicht zusammen
replace erw_hh = 2 if F1601 == 1 & F1603 == 1 // zusammenlebend, Partner*in erwerbstätig
replace erw_hh = 3 if F1601 == 1 & F1603 == 2 // zusammenlebend, Partner*in nicht erwerbstätig

replace erw_hh = 1 if inlist(F1600,2,3,4) // keine Partnerschaft -> auch auf 1
*! unverheiratete werden hier nicht als Partnerschaften behandelt

lab define erw_lab 1 "nicht verh./alleine lebend" 2 "verh. & 2 Erwerbspersonen im HH" 3 "verh. & 1 Erwerbsperson"
lab values erw_hh erw_lab
tab erw_hh
```

*** 

[**Übung 5-3**](#ue53)

*** 

## Übungen 5 {#genue}

(@) Laden Sie den BIBB/BAuA Erwerbstätigenbefragung 2018 (`BIBBBAuA_2018_suf1.dta`). 

### Übung 5-1 {#ue51}

(@) Erstellen Sie eine Dummy-Variable, die erfasst, ob die Befraten in der gleichen NUTS-2-Region leben und arbeiten
  + `nuts2` enthält den Wohnort, `F233_nuts2` enthält den Ort der Betriebsstätte
  + Schließen Sie die Missings aus (`99996` bis `99999`)
  + Erstellen Sie die Variable mit `gen`. Denken Sie an den `==` Operator.
  + Kontrollieren Sie das Ergebnis mit `browse`
  + Definieren Sie Labels für diese Dummyvariable: "Wohnort = Arbeitsort" "Wohnort != Arbeitsort" 

(@) Verändern Sie das Label `S1` von 1 = "männlich" und 2=  "weiblich" auf 1 = "Männer" und 2 = "Frauen".
  + Lassen Sie sich mit `tab S1` die Variable ausgeben.
  + Verändern Sie die Labels mit  `label define .... , replace`
  + Lassen Sie sich mit `tab S1` die Variable erneut ausgeben - hat das geklappt wie gedacht?

### Übung 5-2 {#ue52}

(@) Fassen Sie die Variable "Gesamtnote Schulabschluss" (`F1108`)  in drei Kategorien zusammen:
```{r F1108tab, echo=F}

 tibble::tribble(~"Wert",~"Label", 
           1,   "sehr gut",
           2,   "gut",
           3,   "befriedigend",
           4,   "ausreichend",
           7,   "keine Note vorgesehen",
           8,   "weiß nicht",
           9,   "keine Angabe"
) %>% 
  data.frame("neu"=c( rep("1 - (sehr) gut",2),rep("2 - bestanden",2), rep("3 - fehlend",3) ) ) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 10) %>% 
  column_spec(1,monospace = TRUE) %>% 
  collapse_rows(columns = 3, valign = "top") %>% 
  add_header_above(c("F1108" = 2, "neue Variable" = 1))
```

  + Nutzen Sie die Label-Funktion von [`recode`](#recode)

### Übung 5-3 {#ue53}

(@) Fassen Sie die Variablen `F209` ( Arbeitszeit normalerweise zwischen 7 und 19 Uhr?) und `F223` (mindestens einmal im Monat Sonntagsarbeit) zu einer Variable `a_zeit` mit folgenden Ausprägungen zusammen:

    > 1 zwischen 7-19 Uhr & keine Sonntagsarbeit              
    > 2 nicht zwischen 7-19 Uhr, aber keine Sonntagsarbeit    
    > 3 zwischen 7-19 Uhr aber Sonntagsarbeit                 
    > 4 nicht zwischen 7-19 Uhr und Sonntagsarbeit            
  
  (Für Tipp weiter nach unten Scrollen)







****

    F209 == 1 & F223 == 2  // 1 zwischen 7-19 Uhr & keine Sonntagsarbeit              
    F209 == 2 & F223 == 2  // 2 nicht zwischen 7-19 Uhr, aber keine Sonntagsarbeit    
    F209 == 1 & F223 == 1  // 3 zwischen 7-19 Uhr aber Sonntagsarbeit                 
    F209 == 2 & F223 == 1  // 4 nicht zwischen 7-19 Uhr und Sonntagsarbeit            
  
  

<!--chapter:end:05_gen.Rmd-->

# Variablen erstellen (2), Hilfe verwenden {#egen} 

```{r setup6, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
# baua <- readstata13::readhttp://127.0.0.1:8264/rmd_output/0/appendix.html#.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F)
```


## `egen`

Mit `egen`, den "Extensions to generate", stehen eine ganze Menge an Funktionen zur Verfügung. In [Kapitel 4](#cut) hatten wir schon die Option `cut` zur Einteilung in Gruppen (`cut`) kennengelernt:
```{stata age_cut4, eval = F}
egen age_cat = cut(zpalter), at(15 18 35 60 100 ) label
tab age_cat
```

```{stata age_cut5, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
egen age_cat = cut(zpalter), at(15 18 35 60 100 )  label
tab age_cat
```

Außerdem könnten wir bspw. eine Variable `durchschnittsalter` mit dem Mittelwert für `zpalter` erstellen:
```{stata egenmean1, eval = F}
egen durchschnittsalter = mean(zpalter)
```

Oder wir könnten den Vergleich des Wohn- und Arbeitsbundeslandes aus [Kapitel 5](#dummyvar) mit `diff()` durchführen:
```{stata egendiff, eval = F}
gen ao_wo =  Bula ==  F233 if !missing(F233)
egen ao_wo2 = diff( Bula F233_Bula) if !missing(F233) 
```
Das führt zum gleichen Ergebnis, allerdings könnten wir in `diff()` noch weitere Variablen für einen Abgleich einfügen (bspw. weitere Arbeitsorte).

Eine Liste aller Optionen für `egen` findet sich unter `help egen`:


```{r egenhelp1, echo = F,out.width = "80%", fig.align="center"}
knitr::include_graphics("./pics/06_egen_help.png")
```

## Erklärungen für `help` und Stata-Begriffe

### numlist, varlist, varname

Eine `numlist` ist eine Liste von Zahlen, die je nach Anwendungsfall durch Leerzeichen oder Kommas getrennt sind. Es gibt eine ganze Reihe von Möglichkeiten, eine `numlist` zu erzeugen. Im `cut`-Beispiel oben haben wir einfach in `at()` direkt die Zahlen angegeben und so eine `numlist` mit den Werten `15 18 35 60 100` erstellt. Wir können aber mit `(start(intervall)stop)` auch eine Zahlenreihe anlegen, bspw. mit `(0(2)8)` die Zahlenreihe `r seq(0,8,2)`. Eine `numlist` kann auch einer Reihe und einem Wert bestehen: `0(1)5 8` führt zu `r c(seq(0,5,1),8)`. Mehr Tricks unter  `help numlist`.

Eine `varlist` ist hingegen eine Liste an Variablennamen. Wenn wir eine solche aufrufen, werden die Namen mit den im Datensatz vorhandenen Variablen abgeglichen (und ggf. eine Fehlermeldung ausgegeben). Wir können `varlist`en einfach direkt angeben, bspw. wie in `diff(bula F233)`, wir können aber auch Verallgemeinerungen, sog. 'wild cards' für die Auswahl von Variablen verwenden:


```{r egen_helpers, echo =F}
tribble(~"v",~"l",
  "myvar"         , "nur Variable `myvar`",
  "myvar*"        , "Variablen, deren Name mit `myvar` beginnt, alsp bspw. auch `myvar2` oder `myvariable`",
  "*var"          , "Variablen, deren Name mit `var` endet, alsp bspw. auch `yourvar`",
  "my*var"        , "Variablen deren Name mit `my` beginnt und mit `var` endet - mit einer beliebigen Zahl an Zeichen dazwischen",
  "my?var"        , "Variablen deren Name mit `my` beginnt und mit `var` endet - mit nur einem Zeichen dazwischen",
  "myvar1-myvar6" , "myvar1, myvar2, ..., myvar6 (je nachdem, welche Variablen vorhanden sind - in der Reihenfolge aus dem Variablenexplorer)"
        ) %>% 
    kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 12) %>% 
  column_spec(1,monospace = TRUE) %>% 
  row_spec(0, color = "white")
```


Weiteres unter `help varlist`

`#` steht in den Hilfe-Dateien immer für eine Zahl. 


### Beispielsyntax in den `help` Dateien

Ganz unten finden sich in den Hilfen immer Beispiele:

```{r egenhelp2, echo = F,out.width = "100%",fig.height= 3.5, fig.align="center"}
knitr::include_graphics("./pics/06_egen_help2.png")
```

Diese beruhen auf Datensätzen, die lokal vorhanden sind (`sysuse ...`) oder direkt aus dem Internet geladen werden können (`webuse`). Oft ist es hilfreich, die Beispiele Schritt für Schritt durchlaufen zu lassen und im `browse`-Modus zu beobachten, was passiert.

***

**[Übung 1](#egenue)**

***

## `by`

Mit dem Präfix `by` können Berechnungen getrennt für verschiedene Gruppen durchgeführt werden. 
Das ist insbesondere in Kombination mit `egen` ein sehr vielseitiges Werkzeug.
So können wir beispielsweise das Durchschnittsalter in unserem Datensatz getrennt für Frauen und Männer berechnen:
```{stata by1, eval = F}
by S1: egen mean_byS1 = mean(zpalter)
```
<span style="color:red">`not sorted`</span>  
<span style="color:blue">`r(5);`</span>

Allerdings setzt das immer voraus, dass der Datensatz nach den in `by` angegebenen Variablen sortiert ist - sonst bekommen wir eine Fehlermeldung. 
Sortieren können wir entweder mit `sort varname`:
```{stata sort1, eval= F}
sort S1
by S1: egen mean_byS1 = mean(zpalter)
```
...oder alles in einem Befehl:
```{stata sort2, eval = F}
bysort S1: egen mean_byS1 = mean(zpalter)
```


```{stata by2, echo = F}
quietly{
  set linesize 120
  qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode zpalter, mv(9999 )
}
bysort S1: egen mean_byS1 = mean(zpalter)
sort intnr
list S1 zpalter mean_byS1 in 1/5, noobs abbrev(10)
```

> Anmerkung für die Ausgabe der Fälle wurde nach `by S1,sort: egen mean_byS1 = mean(zpalter)` mit `sort intnr` wieder die ursprüngliche Reihenfolge hergestellt.


Wir könnten aber auch die geschlechts- und schulbildungsspezifischen Durchschnittseinkommen berechnen, indem wir beide Variablen im `by`-Befehl angeben:
```{stata bys3, eval = F}
bysort S1 S3: egen mean_byS1S3 = mean(zpalter)
```

```{stata bys4, echo = F}
quietly{
  set linesize 120
  qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode zpalter, mv(9999 )
}
qui bysort S1: egen mean_byS1 = mean(zpalter)
qui bysort S1 S3: egen mean_byS1S3 = mean(zpalter)
qui sort intnr
list S1 S3 zpalter mean_byS1 mean_byS1S3 in 1/5, noobs abbrev(14)
```

***

**[Übung 2](#byue)**

***


## Übungen 6 {#egenue}

(@) Laden Sie den BIBB/BAuA Erwerbstätigenbefragung 2018 (`BIBBBAuA_2018_suf1.dta`). 

### Übungen 6-1 {#egenue1}

(@) Bilden Sie mit Hilfe von `egen` verschiedene Einkommensklassen (bspw. ab 0EUR in 500EUR-Schritten bis 72000EUR). Nutzen Sie die `numlist`-Funktion mit `(start(intervall)stop)`. Verändern Sie Labels.


(@) Welche Variablen bekommen Sie mit `describe` angezeigt, wenn Sie entweder `describe F21`, `describe F21*`, `describe F21?`, `describe F2*4` oder `describe F2?4` verwenden?


### Übungen 6-2 {#byue}

(@) Berechnen Sie das Durchschnittseinkommen (basierend auf `F518_SUF`) getrennt für Männer und Frauen und legen Sie es in einer neuen Variable `m_inc_byS1` ab.
  + mit `mvdecode F518_SUF, mv(99998/99999)` können Sie die Missings ausschließen
  + Denken Sie daran, dass Sie die Daten sortieren müssen.

(@) Wie würden Sie jetzt für jede\*n Befragten berechnen, um wieviel ihr\*sein Einkommen vom geschlechtsspezifischen Durchschnittswert unterscheidet?

### Profiaufgabe 

(@) Öffnen Sie die Hilfe für `egen` und lassen Sie ein Beispiel Schritt für Schritt durchlaufen - beobachten Sie was passiert, indem Sie mit `browse` den Datensatz betrachten.



## Anhang

### Kombination von `summarize` mit dem `by`-Präfix {#bys_su}

Mit `bysort` können wir auch Gruppenvergleiche erstellen. Bspw. können wir die Altersangaben für Männer (`S1`=1) und Frauen (`S1`=2) vergleichen, indem wir das Befehls-Präfix `bys varX:` verwenden. Stata sortiert also den Datensatz entsprechend der angegebenen Variable und die folgende Berechnung wird getrennt nach den Werten für `varX` ausgeführt. Wenn wir also `summarize` für beide Ausprägungen von `S1` berechnen möchten, gehen wir wir folgt vor:
```{stata bysuF, eval = F}
bysort S1: summarize zpalter 
```
```{stata bysuT, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
bys S1: summarize zpalter 
```


<!--chapter:end:06_egen.Rmd-->

# Inferenzstatistik {#infer} 

```{r setup7, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
# baua <- readstata13::readhttp://127.0.0.1:8264/rmd_output/0/appendix.html#.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F)
z_wert <-2.5804
```


## Inferenz: von der Stichprobe zur allgemeinen Aussage

Bisher haben wir die Angaben aus unserem Datensatz immer als fix angesehen. 
Ziel einer statistischen Auswertung ist aber meistens, Aussagen über die *Grundgesamtheit* oder *Population* zu treffen. 
Im Fall der ETB 2018 wären das also alle Erwerbstätigen in Deutschland.
In der ETB 2018 wurde eine Zufallsstichprobe erhoben, um eine repräsentative Datengrundlage zu schaffen. 
Das heißt, es wurde ein Verfahren gewählt, das potenziell allen Erwerbstätigen die gleiche Chance gibt, in der Befragung zu landen.
Damit ist unser Datensatz eine "verkleinerter Ausschnitt" aus der großen Gesamtpopulation, die wir dann für Aussagen über eben diese Gesamtpopulation verwenden können.

Wenn wir uns jetzt beispielsweise das Durchschnittsalter in der Erwerbstätigenbefragung ansehen:
```{stata inf1, eval = F}
summarize zpalter
```

```{stata inf1t, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
summarize zpalter
```

Natürlich ist relativ intuitiv, dass das Durchschnittsalter *aller Erwerbstätigen in Deutschland* nicht exakt `47.19228` betragen wird. Wir wären uns aber relativ sicher, dass es nicht 30 oder 60 sein wird. 
Bei 47.4 ist das aber nicht ganz so klar. 
Vielleicht haben wir ja nur etwas "Pech gehabt" und liegen mit unserer Stichprobe eben etwas neben dem wahren Wert. 
Das Problem ist aber natürlich, dass wir den wahren Wert nicht kennen - wir kennen das Alter für alle Erwerbstätigen in Deutschland schlicht nicht.

Da wir nun aber davon ausgehen müssen, dass wir mit unserer Stichprobe den exakten wahren Wert wohl nicht getroffen haben, geben wir ein Intervall innerhalb dessen wir den wahren Wert mit einigem Selbstbewusstsein verorten können.
Dieses Intervall wird in der Regel als Konfidenzintervall bezeichnet.
Aber wie bestimmen wir die Grenzen für dieses Intervall?

Um die Grenzen sinnvoll zu bestimmen, benötigen wir eine Annahme wie sich der Stichprobenwert zum wahren Wert in der Grundgesamtheit verhält.

> Übrigens: Stichprobenmittelwerte werden häufig mit $\bar{x}$ bezeichnet, wohingegen der wahre Mittelwert in der Grundgesamtheit mit $\mu$ bezeichnet wird. 
> Also: lateinischer Buchstabe für die Stichprobe, griechischer für die Grundgesamtheit.
> Für eine Schätzung von $\mu$ auf Basis eines Stichprobenwerts wird $\hat{x}$ verwendet ("Dach").

Wie kommen wir also von $\bar{x}$ zu $\mu$ oder zumindest zu $\hat{x}$? 
Wie viel Schwankungsbreite sollten wir ansetzen? 
Dabei hilft uns die Streuung in der Stichprobe: wenn die Werte in der Stichprobe alle sehr nahe beieinander liegen, dann liegt es nahe, dass auch die Streuung in der Grundgesamtheit eher gering ist.
Dementsprechend kann das Intervall um unsere Schätzung eher eng sein.
Sollten wir beispielsweise im Extremfall 22000 identische Angaben im Datensatz haben, dann ist unser Selbstbewusstsein natürlich sehr viel kleiner als für Werte, die sehr weit auseinander liegen (= mit einer großen Varianz).
Außerdem sind wir bei einer größeren Stichprobe vielleicht etwas sicherer als bei einer kleineren Datenbasis. 
Die Fallzahl sollte also auch berücksichtigt werden.

Diese Überlegungen sind in folgender Formel eingebaut:

$$\bar{x}\,\pm\,t\times\frac{s}{\sqrt{n}}$$
Wir bestimmen also  auf Basis einer Stichprobe das Konfidenzintervall einer Schätzung, indem wir um den Punktschätzer (den Stichproben-Mittelwert, also die `47.19228 Jahre`) mit Hilfe des eines geeigneten $t$-Werts und des Standardfehlers ($\frac{s}{\sqrt{n}}$[^11]) einen Wertebereich um den Mittelwert $\bar{x}$ konstruieren.

[^11]: Also die Standardabweichung ($s$) dividiert durch die Wurzel der Stichprobengröße ($n$)

Das $t$ kommt dabei aus der Student-t-Verteilung bzw. als $z$-Wert aus der Standard-Normalverteilung ([Illustration](#distr)). $t$ wählen wir dabei so, dass bei wiederholter Stichprobenziehung 95% der resultierenden KI den wahren Wert aus der Grundgesamtheit beinhalten würden:
```{r, out.height="25%", out.width="100%", fig.align='center', echo=F}
data1 <- data.frame(z = seq(-4,4,.01)) # dataframe erstellen mit Zahlenfolge zwischen -4 & 4
data1$t.var <- dt(x=data1$z,df =  9999) 


ki95 <- 
  ggplot(data = data1, aes(x=z, y =t.var)) + 
  theme_minimal(base_size = 10) +
  labs(y = "Häufigkeitsdichte", x = "t",title ="95% Konfidenzintervall") +
  geom_ribbon(data=filter(data1,z <= - 1.960201), aes(ymin=0, ymax = t.var), fill = "#F5CC71") + # fläche links
  geom_segment(data = data.frame(z = - 1.960201,y1 = dt(x=- 1.960201,df =  1757)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + # grenze links
  geom_ribbon(data=filter(data1,z >=  1.960201), aes(ymin=0, ymax = t.var), fill = "#F5CC71") + # fläche rechts
  geom_segment(data = data.frame(z =  1.960201,y1 = dt(x= 1.960201,df =  1757)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + # grenze rechts
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "grey50", size = .5, linetype = 3) + # mittellinie
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=-0.0125,xend = -1.85, yend = -0.0125), color = "grey25", size = .5,
            arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + # pfeil nach links
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=-0.0125,xend = 1.85, yend = -0.0125), color = "grey25", size = .5,
            arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + #pfeil nach rechts
  geom_label(data=data.frame(z = 0 , y1 = -0.0125, lab1 = "+/- 1.96", t.var = 0),aes(label = lab1), size = 3.5 ) +
  geom_line(color = "navy")  +   
  scale_x_continuous(breaks = seq(-3,3,1),minor_breaks = seq(-3,3,1))+
  theme(aspect.ratio = 1, panel.grid = element_line(size = rel(.25))) +
  geom_text(data=data.frame(z = c(-3,3), t.var = 0.015, label = rep(paste0(round(pt(q = -1.959964,df = 9999)*100,2),"%"),2) ), 
                            aes(x = z, y = t.var, label = label),
                            size = 3.25,vjust= 0, hjust = c(1,0))  +
  theme(plot.margin = margin(0, 0.1, 0, 0.1, "cm"))



ki90 <- ggplot(data = data1, aes(x=z, y =t.var)) + 
  theme_minimal(base_size = 10) +
  labs(y = "Häufigkeitsdichte", x = "t",title ="90% Konfidenzintervall") +
  geom_ribbon(data=filter(data1,z <= qt(p = .05, df = 9999)), aes(ymin=0, ymax = t.var), fill = "#FCE8CE") + # fläche links
  geom_segment(data = data.frame(z = qt(p = .05, df = 9999),y1 = dt(x=- qt(p = .05, df = 9999),df =  1757)) , 
               aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + # grenze links
  geom_ribbon(data=filter(data1,z >=  qt(p = .95, df = 9999)), aes(ymin=0, ymax = t.var), fill = "#FCE8CE") + # fläche rechts
  geom_segment(data = data.frame(z =  qt(p = .95, df = 9999),y1 = dt(x= qt(p = .05, df = 9999),df =  1757)) , 
               aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + # grenze rechts
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
               aes(x=z,y=y1,xend = z, yend = 0), color = "grey50", size = .5, linetype = 3) + # mittellinie
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
               aes(x=z,y=-0.0125,xend = -1.64, yend = -0.0125), color = "grey25", size = .5,
               arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + # pfeil nach links
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
               aes(x=z,y=-0.0125,xend = 1.64, yend = -0.0125), color = "grey25", size = .5,
               arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + #pfeil nach rechts
  geom_label(data=data.frame(z = 0 , y1 = -0.0125, lab1 = paste0("+/- ",round(qt(p = .95, df = 9999),3)), t.var = 0),aes(label = lab1), size = 2.75 ) +
  geom_line(color = "#263056")  +   
  scale_x_continuous(breaks = seq(-3,3,1),minor_breaks = seq(-3,3,1))+
  theme(aspect.ratio = 1, panel.grid = element_line(size = rel(.25))) +
  geom_text(data=data.frame(z = c(-3,3), t.var = 0.015, label = rep(paste0(round(pt(q = -1.645,df = 9999)*100,3),"%"),2) ), 
            aes(x = z, y = t.var, label = label),
            size = 3.25,vjust= 0, hjust = c(1,0))+
  theme(plot.margin = margin(0, 0.1, 0, 0.1, "cm"))

ki99 <- ggplot(data = data1, aes(x=z, y =t.var)) + 
  theme_minimal(base_size = 10) +
  labs(y = "Häufigkeitsdichte", x = "t",title ="99% Konfidenzintervall") +
  geom_ribbon(data=filter(data1,z <= qt(p = .005, df = 9999)), aes(ymin=0, ymax = t.var), fill = "#FFABC2") + # fläche links
  geom_segment(data = data.frame(z = qt(p = .005, df = 9999),y1 = dt(x=- qt(p = .005, df = 9999),df =  1757)) , 
               aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + # grenze links
  geom_ribbon(data=filter(data1,z >=  qt(p = .995, df = 9999)), aes(ymin=0, ymax = t.var), fill = "#FFABC2") + # fläche rechts
  geom_segment(data = data.frame(z =  qt(p = .995, df = 9999),y1 = dt(x= qt(p = .005, df = 9999),df =  1757)) , 
               aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + # grenze rechts
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
               aes(x=z,y=y1,xend = z, yend = 0), color = "grey50", size = .5, linetype = 3) + # mittellinie
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
               aes(x=z,y=-0.0125,xend = -2.55, yend = -0.0125), color = "grey25", size = .5,
               arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + # pfeil nach links
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
               aes(x=z,y=-0.0125,xend = 2.55, yend = -0.0125), color = "grey25", size = .5,
               arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + #pfeil nach rechts
  geom_label(data=data.frame(z = 0 , y1 = -0.0125, lab1 = paste0("+/- ",round(qt(p = .995, df = 9999),3)), t.var = 0),aes(label = lab1), size = 2.75 ) +
  geom_line(color = "#263056")  +   
  scale_x_continuous(breaks = seq(-3,3,1),minor_breaks = seq(-3,3,1))+
  theme(aspect.ratio = 1, panel.grid = element_line(size = rel(.25))) +
  geom_text(data=data.frame(z = c(-3,3), t.var = 0.015, label = rep(paste0(round(pt(q = -2.576321,df = 9999)*100,3),"%"),2) ), 
            aes(x = z, y = t.var, label = label),
            size = 3.25,vjust= 0, hjust = c(1,0)) +
  theme(plot.margin = margin(0, 0.1, 0, 0.1, "cm"))

library(patchwork)
ki95 + ki90 + ki99
```

Die t-Werte sind allerdings nicht fix, sondern hängen von der Stichprobengröße ab, mit `invttail( n-1,  .025)` bekommen wir bspw. den t-Wert für das 95%-Konfidenzintervall:

+ $n = 3$       
  `display invttail( 3-1,  .025)`     = `r abs(qt(p = .025, df = 3-1))`
+ $n = 30$        
  `display invttail( 30-1,  .025)`    = `r abs(qt(p = .025, df = 30-1))`
+ $n = 300$     
  `display invttail( 300-1,  .025)`   = `r abs(qt(p = .025, df = 300-1))`
+ $n = 3000$    
  `display invttail( 3000-1,  .025)`  = `r abs(qt(p = .025, df = 3000-1))`
+ $n = 19836$   
  `display invttail( 19836-1,  .025)` = `r abs(qt(p = .025, df = 19836-1))`


Also würden für das 95%-Konfidenzintervall folgendes einsetzen - zur Erinnerung die Formel:
$$\bar{x}\,\pm\,t\times\frac{s}{\sqrt{n}}$$

```{stata ki_manual, eval = F}
dis 47.19228 + 1.960084* 11.33762 / sqrt(19836) // obere Grenze
```
```{stata ki_manualt, echo = F}
dis 2 // verschluckt
dis 47.19228 + 1.960084* 11.33762 / sqrt(19836) // obere Grenze
```


```{stata ki_manual2, eval =F}
dis 47.19228 - 1.960084* 11.33762 / sqrt(19836) // untere Grenze
```
```{stata ki_manual2t, echo =F}
dis 2 // verschluckt
dis 47.19228 - 1.960084* 11.33762 / sqrt(19836) // untere Grenze
```


Aber nicht erschrecken - das macht alles Stata für uns:
```{stata kistata, eval =F}
mean zpalter
```

```{stata kistata2, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
mean zpalter
```

## Hypothesentests {#ttest}

Neben Aussagen über Parameter in einer Grundgesamtheit auf Basis einer Stichprobe ist das eigentliche Ziel statistischer Auswertungen  aber in der Regel, allgemeine Zusammenhänge/Hypothesen zu testen.

Ausgangsszenario: wir betrachten ein Merkmal und möchten durch eine Stichprobe überprüfen, ob der Mittelwert mit unserer Vermutung übereinstimmt bzw. größer/kleiner ist. Testen wir auf Übereinstimmung, verwenden wir einen sog. beidseitigen Test, bei einem einseitigen Test testen wir ob der Stichprobenwert signifikant größer bzw. kleiner als der vermutete Populationswert ist.
Auch hier bleiben wir beim Durchschnittsalter der Erwerbstätigen - mit welcher Sicherheit können wir ausschließen, dass das Durchschnittsalter 47.4 Jahre ist?

Wie wir eben gesehen haben, sollten wir uns nicht allein auf den Punktschätzer verlassen, sondern auch die Streuung in der Stichprobe mitberücksichtigen. 
Dies leisten die Hypothesentests. 
Dabei werden immer zwei sich widersprechende Hypothesen formuliert, die sog. $H_0$ - die Nullhypothese und die $H_A$ - die Alternativhypothese. 
Die $H_0$ beschreibt dabei immer den bisherigen Kenntnisstand und die $H_A$ formuliert die zu testende Aussage. Die Hypothesen unterscheiden sich dann je nachdem ob wir einen gerichteten oder einen ungerichteten Test durchführen: 

Zunächst müssen wir uns entscheiden ob wir eine gerichtete oder ungerichtete Hypothese testen möchten:    

+ ungerichtete Hypothese: "das wahre Durchschnittsalter der Erwerbstätigen in D ist ungleich 47.4 Jahre"
+ gerichtete Hypothese: "das wahre Durchschnittsalter der Erwerbstätigen in D ist **kleiner**/**größer** als 47.4 Jahre"   

Formal werden die Hypothesen dann wie folgt festgehalten:

+ ungerichtete Hypothesen: $H_0: \mu = 47.4\;Jahre$ und $H_A: \mu \neq 47.4\;Jahre$  
+ gerichtete Hypothesen:
  + rechtsseitig  $H_0: \mu \leqslant 47.4\;Jahre$ und $H_A: \mu > 47.4\;Jahre$  
  $\Rightarrow$ die $H_A$ postuliert, dass der wahre Wert **größer** als der Wert aus der $H_0$ ist
  + linksseitig   $H_0: \mu \geqslant 47.4\;Jahre$ und $H_A: \mu < 47.4\;Jahre$  
  $\Rightarrow$ die $H_A$ postuliert, dass der wahre Wert **kleiner** als der Wert aus der $H_0$ ist

Die grundlegende Idee des Hypothesentests ist, dass wir uns nur dann für die Alternativhypothese entscheiden, wenn wir eine ausreichend große Abweichung von dem in der $H_0$ postulierten Wert feststellen. Dazu berechnen wir den $t$-Wert für den SP-Mittelwert entsprechend dieser Formel:  

$$t = \frac{\bar{x}-\mu_{0}}{\frac{s_{x}}{\sqrt{n}}}$$

Wir berechnen also, mit welcher Irrtumswahrscheinlichkeit wir die $H_0$ verwerfen können. Anders formuliert: wie wahrscheinlich ist es, das $\bar{x}$ in einer Stichprobe zu erhalten obwohl $\mu_0$ in der Grundgesamtheit richtig ist?

In der Wissenschaft hat sich als Konvention etabliert, von einem signifikanten Unterschied zu sprechen wenn die Irrtumswahrscheinlichkeit unter 5% liegt. Das bedeutet:

> *Assuming that the null hypothesis is true and the study is repeated an infinite number times by drawing random samples from the same populations(s), less than 5% of these results will be more extreme than the current result.*[^1]

[^1]: [Failing Grade: 89% of Introduction-to-Psychology Textbooks That Define or Explain Statistical Significance Do So Incorrectly. Advances in Methods and Practices in Psychological Science, 2515245919858072.](https://doi.org/10.1177/2515245919858072)

### beiseitiger t-Test {#ttest1}

Ein **beid**seitiger Test testet die $H_0$ im Vergleich zur folgenden Aussage der $H_A$: "die wahren Mietausgaben der Studierenden in Oldenburg sind *ungleich* 47.4 Jahre". Dazu formalisieren wir zunächst die $H_0$ und $H_A$:

$H_0: \mu = 47.4\;Jahre \qquad H_A: \mu \neq 47.4\;Jahre$  

Für die Berechnung können wir in Stata die Funktion `ttest` nutzen. 
Neben den zu testenden Werten geben wir mit `mu` den in der Nullhypothese festgehaltenen Mittelwert an:
```{stata ttest1a, eval = F}
ttest zpalter==47.4
```
```{stata ttest1b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
ttest zpalter==47.4
```

```{r ttest_bsp_beid_illu, out.height="60%", out.width="60%", fig.align='center', echo=F}
data1 <- data.frame(z = seq(-4,4,.01)) ## dataframe erstellen mit Zahlenfolge zwischen -4 & 4
data1$t.var <- dt(x=data1$z,df =  19836) 

z_wert <-2.5804

ggplot(data = data1, aes(x=z, y =t.var)) + 
  theme_minimal(base_size = 15) +
  labs(y = "Häufigkeitsdichte", x = "t") +
  geom_ribbon(data=filter(data1,z <= - z_wert), aes(ymin=0, ymax = t.var), fill = "#F5CC71") + ## fläche links
  geom_segment(data = data.frame(z = - z_wert,y1 = dt(x=- z_wert,df =  1757)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + ## grenze links
  geom_ribbon(data=filter(data1,z >=  z_wert), aes(ymin=0, ymax = t.var), fill = "#F5CC71") + ## fläche rechts
  geom_segment(data = data.frame(z =  z_wert,y1 = dt(x= z_wert,df =  1757)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + ## grenze rechts
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "grey50", size = .5, linetype = 3) + ## mittellinie
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=-0.0125,xend = -z_wert, yend = -0.0125), color = "grey25", size = .5,
            arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + ## pfeil nach links
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=-0.0125,xend = z_wert, yend = -0.0125), color = "grey25", size = .5,
            arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + #pfeil nach rechts
  geom_label(data=data.frame(z = 0 , y1 = -0.0125, lab1 = paste0("+/- ", z_wert), t.var = 0),aes(label = lab1), size = 3.5 ) +
  geom_line(color = "navy")  +   
  scale_x_continuous(breaks = seq(-3,3,1),minor_breaks = seq(-3,3,1))+
  theme(aspect.ratio = 1, panel.grid = element_line(size = rel(.25))) +
  geom_text(data=data.frame(z = c(-3,3), t.var = 0.015, label = rep(paste0(round(pt(q = -z_wert,df = 19836)*100,2),"%"),2) ), 
                            aes(x = z, y = t.var, label = label),
                            size = 3.25,vjust= 0, hjust = c(1,0))
```

Das Ergebnis liegt also deutlich unter 0,05. Wir würden also die $H_0$ verwerfen $\Rightarrow$ die Aussage "das wahre Durchschnittsalter der Erwerbstätigen in D ist gleich 47.4 Jahre" kann als (vorläufig) widerlegt gelten.

Außerdem werden auch gleich die Ergebnisse für den linksseitigen und rechtsseitigen Test angezeigt. Diese sehen uns noch genauer an:

### linksseitiger t-Test

Ein **links**seitiger Test testet die $H_0$ im Vergleich zu folgender Aussage: "das wahre Durchschnittsalter der Erwerbstätigen in D ist *kleiner* als 47.4 Jahre". Formal sehen die $H_0$ und $H_A$ so aus:

$H_0:\, \mu \geqslant 47.4\;Jahre \qquad H_A:\, \mu < 47.4\;Jahre$

```{stata ttest2a, eval = F}
ttest zpalter==47.4
```
```{stata ttest2b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
ttest zpalter==47.4
```

Der Wert unter `mean < 47.4` ist mit `r round(pt(q = -z_wert, df = 19836),4)` kleiner als 0,05, dementsprechend würden wir auf Basis eines linksseitigen Hypothesentests die $H_0$ verwerfen.


```{r ttest_bsp_links_illu, out.height="60%", out.width="60%", fig.align='center', echo=F}
data1 <- data.frame(z = seq(-4,4,.01)) ## dataframe erstellen mit Zahlenfolge zwischen -4 & 4
data1$t.var <- dt(x=data1$z,df =  19836) 

z_wert <-2.5804


ggplot(data = data1, aes(x=z, y =t.var)) + 
  theme_minimal(base_size = 15) +
  labs(y = "Häufigkeitsdichte", x = "t") +
  geom_ribbon(data=filter(data1,z <= - z_wert), aes(ymin=0, ymax = t.var), fill = "#F5CC71") + ## fläche links
  geom_segment(data = data.frame(z = - z_wert,y1 = dt(x=- z_wert,df =  1757)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + ## grenze links
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "grey50", size = .5, linetype = 3) + ## mittellinie
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=-0.0125,xend = -z_wert, yend = -0.0125), color = "grey25", size = .5,
            arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + ## pfeil nach links
  geom_label(data=data.frame(z = 0 , y1 = -0.0125, lab1 = paste0("- ",z_wert), t.var = 0),aes(label = lab1), size = 3.5 ) +
  geom_line(color = "navy")  +   
  scale_x_continuous(breaks = seq(-3,3,1),minor_breaks = seq(-3,3,1))+
  theme(aspect.ratio = 1, panel.grid = element_line(size = rel(.25))) +
  geom_text(data=data.frame(z = -3, t.var = 0.015, label = paste0(round(pt(q = -z_wert,df = 19836)*100,2),"%") ), 
                            aes(x = z, y = t.var, label = label),
                            size = 3.25,vjust= 0, hjust = 1)
```

### rechtsseitiger t-Test 

Ein **rechts**seitiger Test testet die $H_0$ im Vergleich zu folgender Aussage: "das wahre Durchschnittsalter der Erwerbstätigen in ist *größer* als 47.4 Jahre". Formal sehen die $H_0$ und $H_A$ so aus:

$H_0: \mu \leqslant 47.4\;Jahre \qquad H_A: \mu > 47.4\;Jahre$

```{stata ttest3a, eval = F}
ttest zpalter==47.4
```
```{stata ttest3b, echo = F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
ttest zpalter==47.4
```

Wir sehen unter `mean > 47.4` dass der Wert `r round(pt(q = z_wert, df = 19836),4)` deutlich größer als 0,05 ist - dementsprechend würden wir auf Basis eines rechtsseitigen Hypothesentests die $H_0$ **verwerfen**.

```{r ttest_bsp_rechts_illu, out.height="60%", out.width="60%", fig.align='center', echo=F}
data1 <- data.frame(z = seq(-4,4,.01)) ## dataframe erstellen mit Zahlenfolge zwischen -4 & 4
data1$t.var <- dt(x=data1$z,df =  19836) 
z_wert <-2.5804

ggplot(data = data1, aes(x=z, y =t.var)) + 
  theme_minimal(base_size = 15) +
  labs(y = "Häufigkeitsdichte", x = "t") +
  geom_ribbon(data=filter(data1,z >=  z_wert), aes(ymin=0, ymax = t.var), fill = "#F5CC71") + ## fläche rechts
  geom_segment(data = data.frame(z =  z_wert,y1 = dt(x= z_wert,df =  1757)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "#404040", size = .5, linetype = 2) + ## grenze rechts
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=y1,xend = z, yend = 0), color = "grey50", size = .5, linetype = 3) + ## mittellinie
  geom_segment(data = data.frame(z = 0,y1 = dt(x=0,df = 9999)) , 
            aes(x=z,y=-0.0125,xend = z_wert, yend = -0.0125), color = "grey25", size = .5,
            arrow = arrow(length = unit(0.5, "lines"), type = "closed")) + #pfeil nach rechts
  geom_label(data=data.frame(z = 0 , y1 = -0.0125, lab1 = paste0("+ ",z_wert), t.var = 0),aes(label = lab1), size = 3.5 ) +
  geom_line(color = "navy")  +   
  scale_x_continuous(breaks = seq(-3,3,1),minor_breaks = seq(-3,3,1))+
  theme(aspect.ratio = 1, panel.grid = element_line(size = rel(.25))) +
  geom_text(data=data.frame(z = 3, t.var = 0.015, label = paste0(round(pt(q = -z_wert,df = 19836)*100,2),"%") ), 
                            aes(x = z, y = t.var, label = label),
                            size = 3.25,vjust= 0, hjust = 0 )
```


***

[**Übung 1**](#tvsmu)

***

## Mittelwertvergleiche mit dem t-Test {#mittelwertttest}

Diese Testlogik können wir auch dazu verwenden, Kennzahlen für verschiedene Gruppen zu vergleichen. 

### Unverbundener t-Test
Eine häufige Frage zielt darauf ab zu analysieren, ob sich die Durchschnittswerte eines Merkmals zwischen zwei Gruppen unterscheiden.
Beispielsweise könnten wir das Durchschnittsalter zwischen erwerbstätigen Männern und Frauen vergleichen.

Auch für Gruppenvergleich müssen zunächst Hypothesen aufgestellt werden. 

+ Für einen beidseitigen Test ist die Alternativhypothese, dass es einen Gruppenunterschied gibt:  

    $H_0: \mu_1 - \mu_2 = 0 \qquad H_A: \mu_1 - \mu_2 \neq 0$
 
+ Ein linksseitiger Test hätte entsprechend die Alternativhypothese, dass der Gruppenunterschied *kleiner* als 0 ist:  

    $H_0: \mu_1 - \mu_2 \geqslant 0 \qquad H_A: \mu_1 - \mu_2 < 0$

+ Ein rechtssseitiger Test hätte entsprechend die Alternativhypothese, dass der Gruppenunterschied *größer* als 0 ist:  

    $H_0: \mu_1 - \mu_2 \leqslant 0 \qquad H_A: \mu_1 - \mu_2 > 0$

Für den Altersvergleich zwischen Frauen und Männern ergeben sich folgende Hypothesen:

+ ungerichtete Hypothese: $H_0: Alter_{m} - Alter_{f} = 0 \qquad H_A: Alter_{m} - Alter_{f} \neq 0$
+ linksseitige Hypothese: $H_0: Alter_{m} - Alter_{f} \geqslant 0 \qquad H_A: Alter_{m} - Alter_{f} < 0$
+ rechtsseitige Hypothese: $H_0: Alter_{m} - Alter_{f} \leqslant 0 \qquad H_A: Alter_{m} - Alter_{f} > 0$


<!-- bezieht sich das natürlich wieder nur auf die *Punkt*schätzer für die Stichprobe: -->
<!-- ```{stata ttest5a, eval = F} -->
<!-- tabstat zpalter,s(mean) by(S1)  -->
<!-- ``` -->

<!-- ```{stata ttest5b, echo = F} -->
<!-- set linesize 80 -->
<!-- qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear -->
<!-- qui mvdecode zpalter, mv(9999) -->
<!-- tabstat zpalter,s(mean) by(S1)  -->
<!-- ``` -->

<!-- Aber ist diese, in der Stichprobe festgestellte Differenz auch bezogen auf die **Grundgesamtheit** von Bedeutung? Dazu  -->
Wenn wir nun die beiden Mittelwerte für Männer und Frauen vergleichen, greifen wieder auf den `ttest` zurück, aber hier geben wir anstelle von `== XYZ` mit `by(sex)` die Gruppenvariable an. Mit `unequal` lassen wir zudem zu, dass die Varianz des Alters in beiden Gruppen unterschiedlich ist (was sehr häufig der Fall ist):
```{stata ttest6, eval = F}
ttest zpalter, by(S1) unequal
```
```{stata ttest6b, echo=F}
set linesize 80
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
ttest zpalter, by(S1) unequal
```

Für die jeweiligen Tests ergeben sich folgende Entscheidungen:

  + beidseitiger Test: Da der p-Wert für den beiseitigen Test (unter `Ha: diff != 0`) deutlich unter 0,05 liegt, können wir hier die $H_0$ verwerfen und gehen von signifikanten Altersunterschieden aus.  
  + rechtsseitiger Test: für einen rechtsseitigen Test achten wir auf `Ha: diff > 0`- die erwerbstätigen Männer sind also nicht signifikant *älter* als die erwerbstätigen Frauen.
 + linksseitiger Test: für einen linksseitigen Test ist hingegen `Ha: diff < 0` ausschlaggebend: erwerbstätigen Männer sind also signifikant *jünger* als erwerbstätige Frauen.

***

[**Übung 7.2**](#tgroup)

***

### Verbundener t-Test

Möchten wir Werte vergleichen, welche in einer Verbindung zueinander stehen, ist der verbundene t-Test die richtige Wahl. Beispiele für verbundene Stichproben sind beispielsweise experimentelle Untersuchungen, welche Daten vor und nach einer Maßnahme/Treatment/Intervention messen. Anschließend soll anhand des Vergleichs der Ergebnisse und die Wirkung der Maßnahme evaluiert werden. Hier sind die Messwerte aus den beiden Gruppen ("vorher" und "nachher") miteinander *verbunden* - bspw. wird eine Person mit Bluthochdruck auch nach der Maßnahme in der Tendenz einen höheren Blutdruck haben als eine Person, welche bereits zuvor einen niedrigeren Blutdruck hatte.

Ein fiktionales Beispiel:

```{stata , eval = F}
webuse bpwide
browse bp_before bp_after 
```


```{r bpwide_shot, echo = F,out.width = "60%",out.height="60%", fig.align="center"}
knitr::include_graphics("./pics/07_bpwide.png")
```

Diese vorher/nachher Werte können wir jetzt mit einem verbundenen t-Test vergleichen:
```{stata, eval = F}
ttest bp_before==bp_after
```

```{stata, echo = F}
set linesize 200
qui clear
qui webuse bpwide,clear
ttest bp_before==bp_after
```

Auch hier sehen wir wieder die Ergebnisse für einen links- (` Ha: mean(diff) < 0 `) beid- (`Ha: mean(diff) != 0`) und rechtsseitigen (`Ha: mean(diff) > 0`) Test. Wir erkennen aus den Ergebnissen, dass der Blutdruck der Patient\*innen:

  + nach dem Treatment nicht signifikant höher ist - linksseitiger Test (*bp_before < bp_after*), linke Spalte
  + sich vor und nach dem Treatment signifikant unterscheidet - beiseitiger Test (*bp_before != bp_after*), mittlere Spalte
  + nach dem Treatment signifikant niedriger ist - rechtsseitiger Test (*bp_before > bp_after*), rechte Spalte

    


## Übersicht zu Varianten für `ttest`

Für alle `ttest`-Varianten können wir mit `, level(..)` auch ein anderes Signifikanzniveau wählen. Standardmäßig wird $\alpha=0,05\%$ verwendet.

+ Vergleich zu einem Referenzwert: `ttest testvariable == referenzwert`

Für Mittelwertvergleich gibt es insgesamt zwei Aspekte, anhand derer sich t-Tests unterscheiden:

+ Die Varianz der Messwerte in den verglichenen Gruppen ist ...
  + gleich: $\Rightarrow$ `ttest testvariable, by(gruppenvariable)` 
  + verschieden: $\Rightarrow$ `ttest testvariable, by(gruppenvariable) unequal` (wie oben)
  
+ Verbundene oder unverbundene Stichprobe?
  + Sind die einzelnen Messwerte voneinander unabhängig? D.h. ein Messwert steht in keinem direkten Zusammenhang mit einem anderen $\Rightarrow$ `ttest testvariable, by(gruppenvariable)` für eine unverbundene Stichprobe (mit ggf. `unequal`)
  + Stehen die einzelnen Messwerte in einem Zusammenhang? D.h. ein Messwert steht in einem direkten Zusammenhang mit einem anderen
    $\Rightarrow$ Werte für beide Variablen sollten "nebeneinander" abgelegt sein (*wide*-Format), dann kann mit `ttest vorher==nachher` ein verbundener `ttest` durchgeführt werden.



## Gewichtung in Stata

> Bei der Datenanalyse ist man oft mit einer Stichprobe aus einer größeren Population konfrontiert und man möchte aus dieser Stichprobe Rückschlüsse auf die größere Population ziehen. Die meisten statistischen Verfahren für diese sog. „Inferenzstatistik“ beruhen dabei auf der Annahme, dass die Stichprobe eine einfache Zufallsstichprobe ist. Bei einer solchen Stichprobe gelangen alle Elemente der Grundgesamtheit mit der gleichen Wahrscheinlichkeit in die Stichprobe. In der Praxis sind solche Stichproben aber die große Ausnahme. Häufig haben bestimmte Gruppen von Personen höhere Auswahlwahrscheinlichkeiten als andere. [Kohler/Kreuter, S.1](https://doi.org/10.1515/9783110469509)

Grundsätzlich sind in Stata drei Arten der Gewichtung verfügbar:

+ `pweight` $\Rightarrow$ die Beobachtungen aus der Grundgesamtheit haben eine unterschiedliche Wahrscheinlichkeit, in der Stichprobe zu sein
+ `fweight` $\Rightarrow$ jede Beobachtung kommt mehrfach vor (Hochrechnungsfaktor)
+ `aweight` $\Rightarrow$ jede Beobachtung im Datensatz ist eigentlich ein Gruppenmittelwert, der auf mehreren Beobachtungen basiert

[*Weiterlesen*](https://www.parisschoolofeconomics.eu/docs/dupraz-yannick/using-weights-in-stata(1).pdf)


Wir können Gewichtungen in Stata auf zwei Weisen anwenden:

Zum einen können die Daten mit dem Befehl `svyset` als Surveydaten definiert werden. Hierbei können Variablen spezifiziert werden, die Informationen über das Survey-Design enthalten, wie die Stratifizierung und die anzuwendende Gewichtungsvariable. Anschließende Analyseverfahren werden mit dem Befehlspräfix `svy` durchgeführt. In diesem Beispiel:
```{stata weight1, eval = F}
tabulate S1 // ungewichtet
```
```{stata weight1b, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tabulate S1
```

```{stata weight1a, eval = F}
svyset _n [pweight=gew2018]
svy: tabulate S1 , col count
```

```{stata weight1ab, echo = F}
set linesize 100
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
svyset _n [pweight=gew2018]
svy: tabulate S1 , col count
```

Mit `format(%10.3f)` können wir die Darstellung als *scientific notation* (`1.1e+04`) umgehen: 
```{stata weight1a1, eval = F}
svy: tabulate S1 , col count format(%10.3f)
```

```{stata weight1ab1, echo = F}
set linesize 100
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui svyset _n [pweight=gew2018]
svy: tabulate S1 , col count format(%10.3f)
```

Allerdings steht das `svy`-Präfix  nicht für jeden Befehl zur Verfügung. 
Außerdem können kein weiteren Präfixe neben `svy` verwendet werden - beispielsweise [`by`](#by). 
Daher steht auch eine Alternative zur Verfügung, bei der wir die Gewichtung bei jedem Auswertungsschritt einzeln angeben. 
In unserem Beispiel also 
```{stata weight2, eval = F}
tabulate S1 [fweight=gew2018]
```
<span style="color:red">`may not use noninteger frequency weights`</span>  
<span style="color:blue">`r(401);`</span>

Allerdings akzeptiert Stata bei Häufigkeitsgewichten "frequency weights", keine Gewichtungswerte mit Nachkommastellen.
Leider führt ein einfaches Auf- oder Abrunden führt aber häufig zu falschen Ergebnissen: 
```{stata weight2b, eval = F}
tabulate S1 [fweight=round(gew2018)] // runden führt zu Abweichung - Vergleich mit svy: Ergebnis
```

```{stata weight2ab, echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tabulate S1 [fweight=round(gew2018)] // runden führt zu Abweichung - Vergleich mit svy: Ergebnis
```
Eine mögliche Lösung dieser Problematik ist es, zunächst die Gewichtungsvariable mit einer hohen Zahl (bspw.100 000) zu multiplizieren
und anschließend auf ganze Werte zu runden:

```{stata weight3, eval =F}
tabulate S1 [fweight=round(gew2018*100 000)]
```

Das führt zu korrekten Verteilungen. 
Allerdings ist für die Ergebnisse jedoch zu beachten, dass die zu Grunde liegende Fallzahl anschließend wieder korrigiert werden muss (also in diesem Beispiel durch 100 000 geteilt werden muss).

Der eigentliche Zweck von `fweight` ist aber, die Auszählung "hochzugewichten".
In der Erwerbstätigenbefragung können wir das mit `gew2018_hr17`, dem Hochrechnungsfaktor auf dem Mikrozensus 2017 erreichen:
```{stata fweight1a1, eval = F}
tabulate S1 [fweight = round(gew2018_hr17)]
```

```{stata fweight1ab1, echo = F}
set linesize 100
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
tabulate S1 [fweight = round(gew2018_hr17)]
```

Die Fallzahl ist also hier dann deutlich größer als die im Datensatz vorhandenen Fälle $\Rightarrow$ jeder Fall im Datensatz steht für eine Vielzahl an Fällen.

## Übungen 7 

Laden Sie den BIBB/BAuA Erwerbstätigenbefragung 2018 (`BIBBBAuA_2018_suf1.dta`). 

##### Übung 7.1 {#tvsmu}

(@) Berechnen Sie das Konfidenzintervall für die durchschnittliche Arbeitszeit (`az`)

(@) Testen Sie die Hypothese, dass die eigentliche durchschnittliche Arbeitszeit 38.5 Stunden beträgt. Interpretieren Sie die Ergebnisse im Sinne einen rechts-, links- und beidseitigen Tests!


##### Übung 7.2 {#tgroup}

(@) Testen Sie die Hypothese, dass ein signifikanter Unterschied in der Arbeitszeit zwischen Männern und Frauen besteht (`S1`)

##### Übung 7.3 {#weight}

(@) Erstellen Sie eine Häufigkeitsauszählung der Ausbildungsabschlüsse (`m1202`) - zunächst ohne und dann mit Gewichtung. Verwenden Sie sowohl die `pweights`  als auch die `fweights` an.


## Anhang

### Student-t vs. Normalverteilung {#distr}

```{r, out.height="60%", out.width="60%", fig.align='center', echo = F}
data1 <- data.frame(z = seq(-4,4,.01)) # dataframe erstellen mit Zahlenfolge zwischen -4 & 4
data1$nv.var <- dnorm(x=data1$z,mean = 0 ,sd =  1) # Dichtefunktion der Std-NV
data1$t.var <- dt(x=data1$z,df =  2) # Dichtefunktion der t-Verteilung mit df=2
data1$t.var10 <- dt(x=data1$z,df =  10) # Dichtefunktion der t-Verteilung mit df=10
data2 <- data1 %>% pivot_longer(cols = contains("var"),values_to = "nv",names_to = "Verteilung") %>% 
  mutate(Verteilung = case_when(grepl("nv",Verteilung)~ "Standard-NV",
                                grepl("t\\.var$",Verteilung)~ "Student-t mit df = 2",
                                grepl("t\\.var10$",Verteilung)~ "Student-t mit df = 10")) 

ggplot(data = data2, aes(x=z, color = Verteilung)) +   
  geom_line(aes(y= nv), size = .75) +
  scale_color_manual(values =c("#3B64A1","#3B414F","#B3875C"), name = "") +
  theme_minimal() +
  labs(y = "Häufigkeitsdichte", x = "") +
  theme(legend.position = "top",
        panel.grid.major = element_line(size = .1),panel.grid.minor = element_blank()) +
  guides(colour = guide_legend(override.aes = list(shape = 15 ,size = 6) ,
                               label.position ="right" , ncol = 3,reverse = T) )
```

$$\text{Normalverteilung:}\;\mathcal{N}(\mu,\sigma)$$

$$\text{Student-t-Verteilung:}\;\;\; t_n=\frac{\mathcal{N}(0,1)}{\sqrt{\frac{\chi^{2}_{n}}{n}}}$$

Der wesentlich Punkt ist, dass die Normalverteilung voraussetzt, dass wir $\sigma^2$ kennen - die Varianz (Streuung) der Altersangaben in der Grundgesamtheit. 
Aber wir kennen $\sigma^2$ genauso wenig wie $\mu$, sondern nur die Stichprobenmittelwerte $\bar{x}$ und -varianz $s^2$.
Daher halten wir uns an die Student-t-Verteilung, die quasi eine etwas lockerere Normalverteilung ist bei unbekannter Populationsvarianz.


Die Bedeutung der Normalverteilung ergibt sich daraus, dass wichtige statistische Maßzahlen, die man aus Stichproben berechnen kann, unter bestimmten Bedingungen normalverteilt sind.

Literaturtipps:  

+ S.133 *ff.*, Kapitel 6 "Zufallsstichproben und Schätzen" in [Diaz-Bone, R. (2019). Statistik für Soziologen (4. Auflage)](https://www.utb-studi-e-book.de/9783838550718)

+ S.79 *ff.* Kapitel 6 "Stichprobe und Grundgesamtheit" in [Bortz, J., & Schuster, C. (2010). Statistik für Human- und Sozialwissenschaftler (7. Auflage)](https://link.springer.com/book/10.1007/978-3-642-12770-0) (ausführliche formale Beschreibung)

<!--chapter:end:07_gewichtung.Rmd-->

# Zusammenhangsmaße {#zshg} 

```{r setup8, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
options(width = 200)
df <- data.frame(var1 = c(1,2,7,8),
                 var2 = c(2,4,7,6))

# path <- "D:/oCloud/RFS/"
# ak <- readr::read_delim(paste0(path,"allbus_kumuliert_1980-2018.csv"), delim = ";", col_types = cols(.default = col_double())) %>%  
#   mutate_all(~ifelse(.<0,NA,.))
```

Zusammenhänge sind das Herz (fast) aller statistischer Analysen. Im Folgenden lernen wir die Berechnung einer Reihe von Kennzahlen kennen, welche den Zusammenhang zwischen zwei Variablen ausdrücken. Diese variieren je nachdem ob sie nur die Stäke oder auch die Richtung eines Zusammenhangs ausdrücken. Der Abstand zur Null dient dabei als Indikator für die Stärke des Zusammenhangs - je größer der Abstand zur Null, desto stärker der Zusammenhang:
  
|  |  |
|:----------|:---------------------------|
| Wert der Kennzahl ($K$) |  Grad des Zusammenhangs |
| $K$ = 0 | Keiner |
| 0,00 ≤ $K$ < 0,05 | Praktisch keiner |
| 0,05 ≤ $K$ < 0,25 | Geringer |
| 0,25 ≤ $K$ < 0,50 | Mittlerer |
| 0,50 ≤ $K$ < 1,00 | Starker |
| $K$ = 1,00 | Perfekter | 
  
  
Zusammenhangsmaße für metrische und ordinale Variablen geben mittels der Vorzeichen auch die *Zusammenhangsrichtung* an, sie variieren zwischen -1 und +1.

## Metrisch skalierte Variablen {#zshg_met}

Wir sehen uns den (möglichen) Zusammenhang zwischen dem Alter (`zpalter`) und der Stundenzahl im Home Office (`F321`) an.

### Korrelationskoeffizient {#cor}
Zur Bestimmung eines Zusammenhangs zwischen zwei metrischen Variablen empfiehlt sich der Korrelationskoeffizient nach Pearson. 
Dieser ist definiert als die Kovarianz dividiert durch die jeweiligen Standardabweichungen der beiden Variablen und liegt im Intervall [-1,1]:

$$r = \frac{\frac{1}{n}\Sigma_{i}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\frac{1}{n}\Sigma_{i}^{n}(x_i-\bar{x})^2} \times \sqrt{\frac{1}{n}\Sigma_{i}^{n} (y_i-\bar{y})^2}} = \frac{cov_{xy}}{s_x \times s_y}$$

Ein positiver Korrelationskoeffizient deutet auf einen positiven Zusammenhang hin ("je größer X, desto *größer* Y").
Ein negativer Korrelationskoeffizient deutet auf einen negativen Zusammenhang hin ("je größer X, desto *kleiner* Y").
Die Standardabweichung hatten wir in Session 4 kennengelernt, die Kovarianz erfasst die Lage der Datenpunkte relativ zu den Mittelwerten der beiden interessierenden Variablen (liegen Punkte > $\bar{x}$ auch $>\bar{y}$?):
  
```{r s07_15b, message=F, warning=F, out.height="80%", out.width="100%", echo=F,fig.align='center'}
eq1 <-  substitute(italic(bar(x))) # formel erstellen
eq2 <-  substitute(italic(bar(y))) # formel erstellen

baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F,
                                select.cols = c("F231","zpalter")) %>% 
  mutate(across(matches("F231|zpalter"), ~ ifelse(.x>100,NA,.x)))
set.seed(26131)
baua2 <- baua %>% select(matches("F231|zpalter")) %>%  na.omit() %>%  sample_n(150)

ggplot(baua2, aes(x = zpalter, y = F231)) + 
  geom_point(color ="#29304E", fill = "grey98", shape = 21) + 
  geom_vline(aes(xintercept = mean(zpalter,na.rm = T)), color = "#19D4EE", size = .75) +
  geom_hline(aes(yintercept = mean(F231,na.rm = T)),color = "#2473D5", size = .75) +
  geom_text(data = data.frame(zpalter = 50, F231 = 51), aes(label = as.character(as.expression(eq1)) ),
            label.size = .01, hjust = 0.5, color = prismatic::clr_darken("#19D4EE"), parse = T, size = 4.5) +
  geom_text(data = data.frame(zpalter = 97, F231 = 11), aes(label = as.character(as.expression(eq2)) ),
            label.size = .01, hjust = 0.5, 
            color = prismatic::clr_darken("#2473D5"), parse = T, size = 4.5) +
  ggthemes::theme_stata() +
  labs(title = "Alter & Stunden im Home Office (F231)",
       caption = "Quelle: 150 zufällig ausgewählte Beobachtungen der ETB 2018",
       y = "Stunden im Home Office (F231)",
       x = "Alter (zpalter)")+
  theme(aspect.ratio = 1)

# cor(baua2$F231,baua2$zpalter,use = "pairwise.complete.obs")
```
In der Grafik sind der Übersichtlichkeit halber nur 150 Beobachtungen dargestellt, für diese Auswahl ist liegt der Korrelationskoeffizient bei `r round(cor(baua2$F231,baua2$zpalter,use = "pairwise.complete.obs"),3)`.


In Stata können wir den Korrelationskoeffizienten mit `pwcorr` berechnen. Mit der Option `,sig` werden :
```{stata corr1,eval=F}
pwcorr zpalter F231, sig
```
```{stata corr1b, echo =F}
set linesize 90
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui mvdecode zpalter, mv(9999)
qui replace F231 =  . if F231 > 99
pwcorr zpalter F231, sig
```

Es handelt sich mit `0.0548` also um einen geringen Zusammenhang. Der p-Wert gibt uns auch hier wieder Auskunft über die stat. Signifikanz: mit `r sprintf("%.4f",cor.test(baua$F231,baua$zpalter,use = "pairwise.complete.obs")$p.value)` liegt der p-Wert deutlich unter 0,05 $\Rightarrow$ wir würden hier die Nullhypothese verwerfen, dass die Korrelation in der Grundpopulation gleich Null ist. 


## Ordinal skalierte Variablen {#zshg_ord}

Ein klassisches ordinales Merkmal ist die Schulbildung, die wir aus `S3` zusammenfassen können (Details im DoFile):
            

Wir sehen uns den (möglichen) Zusammenhang zwischen der Schulbildung und `F600_12` an:

 

```{r ord_vars, echo =F}
tribble(~"v",~"l",
        "educ", "höchster Schulabschluss",
          "1" , "max. Hauptschulabschluss",
          "2" , "max. mittlere Reife",
          "3" , "(Fach-)Abitur",
          "F600_12", "Häufigkeit: unter Lärm arbeiten",
          "1" , "häufig",
          "2" , "manchmal",
          "3" , "selten",
          "4" , "nie"
        ) %>% 
    kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 10) %>% 
  column_spec(1,monospace = TRUE) %>% 
  row_spec(c(1,5), bold = T, background = "#F2F2F2FF") %>% 
    row_spec(0, color = "white")
```

            
So sieht die Verteilung zunächst in einer Kreuztabelle aus:
```{stata tabF, eval = F}
tab F600_12 educ
```
          
```{stata tabT, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F600_12, mv(9)
	lab var educ "educ"
}
tab F600_12 educ // finale Tabelle
```
          
Auch hier fragen wir uns jetzt: sind die Werte von `F600_12` tendenziell höher oder niedriger bei höheren Werten von `educ`? Allerdings ist hier der Korrelationskoeffizient nicht adäquat, die hier die Abstände zwischen den Kategorien nicht gleichmäßig sind (es handelt sich ja um ordinale Merkmale). Daher müssen hier spezifische Zusammenhangsmaße für ordinale Variablen verwendet werden.
          
### Spearman- Rangkorrelation {#spear}

Eine Möglichkeit, zur Bestimmung eines Zusammenhangs zwischen zwei *ordinal* skalierten Variablen ist der Spearman-Rangkorrelationskoeffizient ($\rho$). Für den Rangkorrelationskoeffizienten werden die Werte der Variablen in Ränge überführt und dann mit diesen Rängen den Korrelationskoeffizient berechnet. Wir können den Rangkorrelationskoeffizienten mit `spearman` berechnen:
```{stata spearmF, eval = F}
spearman educ F600_12
```

```{stata spearmT, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F600_12, mv(9)
}
spearman  educ F600_12 
```
          
Es zeigt sich also mit einem Korrelationskoeffizienten von 0.17 ein schwacher, positiver Zusammenhang. 
Das positive Vorzeichen des Zusammenhangs deutet darauf hin, dass mit einer höheren Ausprägung von `educ` tendenziell höhere Werte für  `F600_12` einher gehen: eine höhere Schulbildung geht mit einem höheren Wert in `F600_12` einher. 
Da höhere Werte in `F600_12` seltenere Belastung angeben sind also Befragte mit höherer Schulbildung seltener von Lärm belastet.
          
### Konkordanzmaße {#tau}
          
          
```{stata tabKTF, eval = F}
tab educ mi02
```

```{stata tabKTT, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F600_12, mv(9)
	lab var educ "educ"
}
tab educ F600_12
```

Ein weiteres Zusammenhangsmaß für ordinale Variablen sind Konkordanzmaße wie Kendall's $\tau$. 
Hierfür werden die Werteverhältnisse bzw. Paarvergleiche gezählt.
Die Idee ist, dass aus den Tabellenwerten Paarvergleiche gebildet werden können:
```{r paarvgl, echo = F,out.width = "90%",fig.height= 3, fig.align="center"}
knitr::include_graphics("./pics/08_paarvgl.png")
```

+ $\tau_a$: Differenz der konkordaten (C) und diskordanten (D) Paarvergleiche als Anteil an allen möglichen Paarvergleichen $\frac{n\times(n-1)}{2}$: 

$$\tau_a=\frac{C-D}{\frac{n\times(n-1)}{2}}$$
  Nachteil: Bei Bindungen in X u. Y Maximalwerte (-1; +1) nicht zu erreichen


+ $\tau_b$: Differenz der konkordaten (C) und diskordanten (D) Paarvergleiche als Anteil an allen möglichen Paarvergleichen $\frac{n\times(n-1)}{2}$ *unter Ausschluss von Bindungen in X und Y* 

$$\tau_{b}=\frac{C-D}{\sqrt{(C+D+T_x)\times(C+D+T_y)}}$$
  Nachteil: Bei Bindungen in X o. Y Maximalwerte (-1; +1) nicht zu erreichen

+ Goodman & Kruskal's $\gamma$ ignoriert die Bindungen vollständig:
            
$$\gamma=\frac{C-D}{C+D}$$
            
Zur Berechnung in Stata können wir `ktau` verwenden:
```{stata tauF, eval = F}
ktau  educ F600_12
```
```{stata tauT, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F600_12, mv(9)
	lab var educ "educ"
}
ktau  educ F600_12
```
Auch hier zeigt sich eine Zusammenhangsstärke in der gleichen Größenordnung wie beim Rangkorrelationskoeffizienten ($\tau_a$ = `-0.2151` und $\tau_b$ = `-0.3350`). Zudem ist auch hier das Vorzeichen negativ: `educ` korreliert negativ mit  `mi02`. Der Wert von Kendall's $\tau_a$ ist deutlich niedriger als von Kendall's $\tau_b$, da hier der Nenner durch die Berücksichtigung *aller* möglichen Paarvergleiche größer wird, der Zähler aber für beide Varianten von Kendall's $\tau$ gleich definiert ist. 

Ein weiteres Maß ist Goodman & Kruskal's $\gamma$, dieses bekommen wir mit der Option `,gamma` in `tab`:
```{stata tauG, eval = F}
tab educ F600_12, gamma
```
```{stata tauG2, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F600_12, mv(9)
	lab var educ "educ"
}
tab educ F600_12, gamma
```

Auch Goodman & Kruskal's $\gamma$ deutet auf einen negativen Zusammenhang hin, hier ist die Stärke mit (`-0.5065`) aber deutlich höher. Dies ist auf die Berücksichtigung der Bindungen zurückzuführen: hier werden alle Bindungen ausgeschlossen, also auch Paarvergleiche mit Bindungen nur auf  einer Variable. Es reduziert sich also der Nenner, somit ergibt sich im Ergebnis ein höherer Koeffizient für Goodman & Kruskal's $\gamma$ als für Kendall's $\tau_b$. 


Insgesamt ist also von einem mittleren Zusammenhang zwischen `educ` und `mi02` auszugehen. 

***

**[Übung 1](#met_ord)**

***


## Nominal skalierte Variablen {#zshg_nom}

Unser Beispiel für nominal skalierte Variablen dreht sich um die Frage: *Gibt es Geschlechterunterschiede bei der Abgeltung der Überstunden?*
Dazu betrachten wir die Variablen `F204` und `S1`:
  
```{r nom_vars, echo =F}
tribble(~"v",~"l",
        "F204",  "Wie wird Ihre Mehrarbeit bzw. wie werden Ihre Überstunden abgegolten?",
        "1"  ,"durch Auszahlung",
        "2"  ,"durch Freizeitausgleich",
        "3"  ,"durch beides",
        "4"  ,"keine Abgeltung",
        "S1"  ,"Geschlecht",
        "1"  ,"männlich",
        "2"  ,"weiblich"
        ) %>% 
    kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 10) %>% 
  column_spec(1,monospace = TRUE) %>% 
  row_spec(c(1,6), bold = T, background = "#F2F2F2FF") %>% 
    row_spec(0, color = "white")
```


Ausgangspunkt der Zusammenhangsmaße für nominale Merkmale ist die Kontingenztabelle der beiden Variablen (Vorbereitungen im DoFile):
```{stata, crstabF, eval =F}
tab S1 F204
```

```{stata, crstabT, echo =F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F204, mv(9)
}
tab F204 S1 
```

### Chi²-basierte Maße {#chi2}

$\chi^2$  basiert auf dem Vergleich der beobachteten Häufigkeit mit einer (theoretischen) Verteilung, welche statistische Unabhängigkeit abbildet (Indifferenztabelle - [mehr dazu](#indiff)). Wir bleiben bei `aq03` und `dh01`. Den $\chi^2$-Wert für diese Häufigkeitstabelle bekommen wir mit `, chi2`:
  
```{r, echo=F, warning=F}
baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F,
                                select.cols = c("S1","F204")) 
tab1 <- xtabs(~ F204 + S1, data = baua %>% select(S1,F204) %>% filter(F204<9))
tx <- chisq.test(tab1)
```

```{stata chitabF, eval =F}
tab F204 S1, chi 
```
```{stata chitabT, echo =F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F204, mv(9)
}
tab F204 S1, chi wrap
```
### Cramér's $\upsilon$

Auf Basis dieses $\chi^2$-Werts von `r round(as.numeric(tx$statistic),3)` können wir Cramér's $\upsilon$ berechnen. Dieses ist definiert als der Quotient aus dem $\chi^2$-Wert und der Fallzahl multipliziert mit dem Minimum der Zeilen- und Spaltenzahl. `n`, erkennen wir aus dem `Total` rechts unten in der Tabelle. Außerdem hat unsere Tabelle 2 Zeilen und 4 Spalten, dementsprechend entspricht das Minimum hier 2:

$$ Cramer's\,\,\upsilon = \sqrt{\frac{\chi^2}{n \times min(k-1,m-1)}}=\sqrt{\frac{225.4461}{9359\times(2-1)}} = 0.1552$$
            
Das geht auch einfacher mit der Option `, V`:
```{stata cramersv, eval =F}
tab F204 S1, V 
```
```{stata cramersvT, echo =F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F204, mv(9)
}
tab F204 S1, V
```

Dieser Wert für Cramér's $\upsilon$ legt einen praktisch keinen Zusammenhang nahe.

Cramér's $\upsilon$ für 2x2-Tabellen wird auch als $\phi$ ("phi") bezeichnet. Dies wäre das passende Maß für die zusammengefasste Variable aller Haustierbesitzer\*innen `aq03b` von oben und `dh01`:

$$\phi= \sqrt{\frac{\chi^2}{n}}$$ 
$$\text{bei k=2 und m=2:}\qquad Cramer's\,\,\upsilon = \sqrt{\frac{\chi^2}{n\times min(k-1,m-1)}}=\sqrt{\frac{\chi^2}{n\times\,1}}$$


***
          
**[Übung 8-2](#auf_nom)**
          
***
          
## Welches Maß richtig?
                
Wir haben jetzt eine ganze Reihe an Zusammenhangsmaßen kennengelernt, die folgende Liste fasst nochmal alle Varianten zusammen. Es gibt noch eine ganze Reihe weiterer Zusammenhangsmaße und diese Liste deckt lediglich die Maße ab, die wir kennengelernt haben:  
                
+ nominal skalierte Variablen 
+ [Odds Ratio](#OR): basierend auf Kreuztabelle `tab x y`
+ [$\chi^2$-basierte Maße](#chi2) `tab x y, chi`, danach Division von $\chi^2$ durch n und Zahl der Spalten/Zeilen
                  
+ ordinal skalierte Variablen
+ [Spearman-Rangkorrelationskoeffizient](#spear) `spearman x y` 
  + [Konkordanzmaße](#tau) 
  + Kendall's $\tau_a$ & Kendall's $\tau_b$:  `ktau x y` 
  + Goodman & Kruskal's $\gamma$: `tab x y, gamma`      
      
        
+ metrische skalierte Variablen 
  + Zusammenhangsstärke: [Pearson-Korrelationskoeffizient](#corr) `corr x y`
  + [Regression](#regression) zur Vorhersage von Werten auf Basis einer Variable: `reg x y`

Ausschlaggebend ist dabei die Variable mit dem niedrigeren Skalenniveau! Ggf. können metrische Variablen durch Kategorisierung ([Kapitel 6](#egen)) in ordinale Variablen überführt werden.  



## Übungen 8

(@) Laden Sie die Erwerbstätigenbefragung in Stata. 

### Übung 8-1 {#met_ord}

(@) Untersuchen Sie den Zusammenhang zwischen der Wochenarbeitszeit (`az`) und dem Einkommen (`F518_SUF`) der Befragten. Welches Maß ist das richtige?
  + Denken Sie daran, die Missings in `F518_SUF` auszuschließen: `mvdecode F518_SUF, mv(99998/99999)`, `az` hat keine Missings.

(@) Untersuchen Sie den Zusammenhang zwischen der Häufigkeit von starkem Termin- oder Leistungsdruck `F411_01` und der dreistufigen Schulbildungsvariable `educ`.
  + So können Sie `educ` erstellen: `recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "mittlere Reife") (7/9 = 3 "(Fach-)Abi") (else = .), into(educ)` 
  + Denken Sie daran, die Missings in `F411_01` zu überschreiben: `mvdecode F411_01, mv(9)`
  + Berechnen Sie ein oder mehrere geeignete Zusammenhangsmaße für diese beiden Variablen.

### Übung 8-2 {#auf_nom}

(@) Untersuchen Sie den Zusammenhang zwischen der Frage, ob der\*die Befragte teiweise von zu Hause aus arbeitet (`F228`) und ob die Befragten gerne eine andere als ihre aktuelle Tätigkeit (`F103`) ausüben würden. 
  + In beiden Variablen ist `9` als fehlender Wert auszuschließen


## Anhang

### Indifferenztabelle {#indiff}

[$\chi^2$](#chi2) ergibt sich aus der Differenz zwischen der Indifferenztabelle und den beobachteten Häufigkeiten. Die Indifferenztabelle können wir mit `,expected` aufrufen (mit `nofreq` blenden wir die tatsächlichen Häufigkeiten aus):
```{stata indtabF, eval =F}
tab F204 S1, expected nofreq
```
```{stata indtabT, echo =F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F204, mv(9)
	lab var F204 "F204"
}
tab F204 S1, expected nofreq
```

Ausgangspunkt für diese Indifferenztabelle sind die relativen Häufigkeiten der tatsächlich beoachteten Werte:
```{stata indtab2F, eval =F}
tab F204 S1, cell nofreq
```
```{stata indtab2T, echo =F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F204, mv(9)
	lab var F204 "F204"
}
tab F204 S1, cell nofreq
```

```{r, echo = F,message = F,warning=F}
baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F,
                                select.cols = c("S1","F204")) %>% 
  mutate(across(matches("S1|F204"), ~ ifelse(.x>8,NA,.x))) %>% select(S1,F204) %>% filter(F204<9)
tab1 <- xtabs(~ F204 + S1, data = baua )
tx <- chisq.test(tab1)
rel_tab1 <- prop.table(tab1)
abs_tab1 <- addmargins(tab1)
rand_tab1 <- addmargins(rel_tab1)
```
<!-- `r round(rand_tab1[2,2],2)*100` \% der Befragten sind Frauen (`S1` = 2) und bekommen ihre Überstunden durch Freizeitausgleich ausgeglichen (`F204`=2).     -->
Uns interessieren hier nur die Randverteilungen aus `Sum`: `r round(rand_tab1[2,"Sum"],4)*100` \% aller Befragten bekommen ihre Überstunden (ausschließlich) durch Freizeitausgleich ausgeglichen (`F204`=2).  `r round(rand_tab1["Sum",2],4)*100` \% aller Befragten sind Frauen (`S1` = 2).

```{r, echo=F, warning=F, message=F}
baua_lab <- haven::read_dta("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",n_max = 1)

rand_tab2 <- addmargins(rel_tab1)
rand_tab2[1:4,1:2] <- LETTERS[1:length(unlist(rand_tab2[1:4,1:2]))]
rand_tab2["Sum",] <- rand_tab2["Sum",] %>% as.numeric(.) %>% round(.,4)
rand_tab2[,"Sum"] <- rand_tab2[,"Sum"] %>% as.numeric(.) %>% round(.,4)


rownames(rand_tab2) <- c(names(attributes(baua_lab$F204)$labels)[1:4],"Total")
colnames(rand_tab2) <- c("männlich","weiblich","Total")

kable(rand_tab2) %>% kable_styling(bootstrap_options = "condensed", full_width = F)
```
Wären beide Merkmale unabhängig voneinander, würden wir erwarten, dass die Wahrscheinlichkeit für das Auftreten einer Merkmalskombination dem Produkt der Einzelwahrscheinlichkeiten entspricht (das ist die "Indifferenz"): $P(A\cup B) = P(A) \times P(B)$. Bspw. ergibt sich der erwartete Wert für die Zellen dann aus der relativen Randverteilung multipliziert mit der Gesamtfallzahl:

+ für Zelle  `B`: `0.4522` $\times$ `0.5383` $\times$ `r sum(tab1)` = `r round(addmargins(rel_tab1)[5]*addmargins(rel_tab1)[12]*sum(tab1),4)`.

+ für Zelle  `H`: `0.1987` $\times$ `0.4617` $\times$ `r sum(tab1)` = `r round(addmargins(rel_tab1)[10]*addmargins(rel_tab1)[14]*sum(tab1),4)`.

Dies sind auch die Werte, die Stata uns oben mit `tab  F204 S1, expected nofreq` ausgegeben hatte.

$\chi^2$ ist dann die summierte Differenz zwischen dieser Indifferenztabelle (also der erwarteten Verteilung bei Unabhängigkeit beider Merkmale) und den beobachteten Häufigkeiten: je größer die Differenz, desto unwahrscheinlicher ist es, dass beide Merkmale unabhängig sind.  



<!--chapter:end:08_zshg.Rmd-->

# Regressionsmodelle {#regression} 

```{r setup9, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
library(scico)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
options(width = 200)
df <- data.frame(var1 = c(1,2,7,8),
                 var2 = c(2,4,7,6))

# path <- "D:/oCloud/RFS/"
# ak <- readr::read_delim(paste0(path,"allbus_kumuliert_1980-2018.csv"), delim = ";", col_types = cols(.default = col_double())) %>%  
#   mutate_all(~ifelse(.<0,NA,.))
```



## Regressionsmodelle - Grundlagen {#reg_intro}

Regressionsmodelle zeigen uns den (linearen) Trend zwischen zwei Variablen. Hier geht es darum, um wie sich im Durchschnitt `var2` verändert, wenn sich `var1` um eine Einheit erhöht. `var2` ist also unsere abhängige, `var1` unsere unabhängige Variable - wir möchten `var2` mit `var1` vorhersagen. Im Folgenden sehen wir, dass Regressionsmodelle mit `reg` erstellt werden können und wie wir den Output interpretieren können. Falls Sie nochmal mehr zu den Grundlagen erfahren wollten, findet sich [hier](#bg_reg) eine ausführlichere Erklärung. 
  
  Wir betrachten einen kleinen Beispieldatensatz mit lediglich 4 Fällen und zwei metrischen Variablen `var1` und `var2`:
```{stata use_data, eval = F}
use "https://github.com/filius23/Stata_Skript/raw/master/regression_bsp.dta", clear
```
  
```{r regp1,out.width = "80%",fig.height= 3.5, echo=T, fig.align="center" , echo = F,warning=F,message=F}
m1 <- lm(var2~ var1, data = df)  
df$pred_vorher <- m1$fitted.values
ggplot(df, aes(x = var1, y = var2)) +
  geom_point(size = 3) + 
  geom_smooth(method = "lm", color = "darkblue" , se = FALSE) +
  ggthemes::theme_stata() +
    expand_limits(y=c(0,8),x=c(0,8)) +
    theme(aspect.ratio = 1)
```
  
Mit Regressionsmodellen können wir lineare Zusammenhänge zwischen zwei metrischen Merkmalen untersuchen. In Stata können wir eine Regression mit dem `reg` Befehl berechnen:
    
```{stata reg_bsp, eval=F}
reg var2 var1
```

```{stata reg_bsp2, collectcode=F, echo = F}
use "regression_bsp.dta", clear
egen mean_var2 = mean(var2)
gen m_abw = var2 - mean_var2
qui reg var2 var1
predict pred_vorher, xb
gen res = var2 - pred_vorher 
gen res2 = res^2
set linesize 90
reg var2 var1
```

Hier steht jetzt eine ganze Menge an Informationen, die wir uns im Folgenden genauer ansehen werden. 

Allgemein sieht unsere Regressionformel wie folgt aus:

$$\widehat{var2}=\texttt{_cons} + \beta1 \times \texttt{var1} = 2.1351 + 0.5810 \times \texttt{var1}$$  

***
Ein positiver Wert unter `Coef.` in der Zeile von `var1` bedeutet, dass unsere Gerade von links nach rechts ansteigt und ein negativer eine fallende Linie bedeuten würde. Der Wert unter `var1` gibt an, um *wieviel sich die Gerade pro "Schritt nach rechts" nach oben/unten verändert*. 
  
> Die Gerade steigt also pro Einheit von `var1` um `r m1$coefficients[2]`.
  
Der Wert neben `_cons` gibt uns Auskunft darüber, wie hoch der vorhergesagte Wert für `var2` wäre, wenn `var1` = 0.
  
> Für `var1` = 0 würden wir `var2` = 2.135135 vorhersagen.
  
Außerdem erkennen wir unter `R-squared` die [Modellgüte](#r2) unseres Regressionsmodells. $R^2$ gibt die prozentuale Verbesserung der Vorhersgen durch die Gerade aus `reg` im Vergleich zum arithmetische Mittel an. $R^{2}$ bezieht sich auf die Verringerung der [Residuen](#resid2) durch das `reg`-Modell im Vergleich zur Mittelwertregel. 
    
> Unser Regressionsmodell kann also 84,7\% der Streuung um den Mittelwert erklären. 
    
Außerdem sehen wir oben links in der Spalte `SS` die ["Sum of Squares"](#quad_res): unter `Total` ist die Summe der quadrierten Abweichungen der beobachteten Werte vom arith. Mittel angegeben (die Abstände zwischen den orangen und schwarzen Punkten: `14.75`). Residual gibt die Summe der Abweichungsquadrate zwischen den beobachteten Werten und den vorhergesagten Werten der Regression (die Abstände zwischen den schwarzen und den blauen Punkten:`2.256..`). 
      
      
In der Spalte `P>|t|` und `t` sehen wir die Ergebnisse eines t-Tests für die jeweiligen Koeffizienten. Hier wird der Koeffizient (sprich: die Steigung der Geraden) "gegen Null" getestet. Ist der Zusammenhang zwischen dem Alter der Befragten und ihrem Fernsehkonsum in der Population nicht eigentlich 0?
        
$$H_0:\, \mu_{Koeff(var1)} = 0  \qquad H_A: \mu_{Koeff(var1)} \neq 0$$


Was hier also gemacht wird, ist ein t-Test für `0.5810811` vs. $\mu=0$. Unter `P>|t|` erkennen wir hier aber, dass die Irrtumswahrscheinlichkeit für die Ablehnung der $H_0$ größer als 0,05 ist. Dementsprechend verwerfen wir die Nullhypothese nicht (wonach $\beta1$ = 0 ist) und können hier nicht einen statistisch signifikanten Zusammenhang sprechen.
      
***
      
```{r reg_graph,out.width = "80%",fig.height= 3.5, echo=T, fig.align="center" , echo = F,warning=F,message=F}
df$mean_var2 <- mean(df$var2)
ggplot(df, aes(x = var1, y = var2)) + geom_point(size = 3) + ggthemes::theme_stata() +
      geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
      geom_point(aes(x = var1, y = mean_var2), col = "darkorange", size = 3) +
      geom_segment(aes(x = var1, xend = var1, y = var2, yend = mean_var2), color = "red", size = .65, linetype = "dotted")  +
      geom_smooth(method = "lm", color = "darkblue" , se = FALSE) +
      geom_point(aes(x = var1, y = pred_vorher), color = "dodgerblue3", size = 3) +
      geom_segment(aes(x = var1, xend = var1, y = var2, yend = pred_vorher), color = "dodgerblue3", size = .65, linetype = 1) +
      expand_limits(y=c(0,8),x=c(0,8)) +
      theme(aspect.ratio = 1)
```
      
      
### vorhergesagte Werte

Die vorhergesagten Werte aus `reg var2 var1` entsprechen einfach der Summe aus dem Wert neben `_cons` und dem Koeffizienten neben `var1` multipliziert mit dem jeweiligen Wert für `var1`. Wir starten also sozusagen bei `var2`=0  und gehen dann eben x Schritte entlang der Geraden.
      
```{stata vorhw1, eval=F}
reg var2 var1, noheader /// noheader macht den Output übersichtlicher
```


```{stata vorhw2, echo=F, collectcode = F}
use "regression_bsp.dta", clear
qui egen mean_var2 = mean(var2)
qui gen m_abw = var2 - mean_var2
qui reg var2 var1
qui predict pred_vorher, xb
qui gen res = var2 - pred_vorher 
qui gen res2 = res^2
qui set linesize 90
reg var2 var1, noheader
```
      
Vorhergesagte Werte für `var2` werden mit $\widehat{var2}$  bezeichnet - das $\widehat{}$ steht dabei für "geschätzt":
        
$$\widehat{var2}=\texttt{_cons} + \beta1 \times \texttt{var1} = 2.1351 + 0.5810 \times \texttt{var1}$$  
Für die erste Zeile ergibt sich also folgender vorhergesagter Wert: $2.1351+0.5811\times1$= `r 2.1351+0.5811*1`
Wir könnten also auch einen vorhergesagten Wert für einen beliebigen Wert von `var1` berechnen, für 5: $2.1351+0.5811\times5$= `r 2.1351+0.5811*5`
 
Wir können dafür `display 2.1351+0.5811*5` nutzen.
Alternativ können wir uns die Tipparbeit auch sparen, indem wir erst `reg` laufen lassen und dann mit `_b` auf die Ergebnisse zugreifen:
```{stata pre_dis2, eval = F}
display _b[_cons] + _b[var1] * 5
```
```{stata pre_dis2T, echo=F, collectcode = F}
use "regression_bsp.dta", clear
qui reg var2 var1
display _b[_cons] + _b[var1] * 5
```
      
      
Mit `predict` können wir jeweils die vorhergesagten Werte für die Ausprägungen von `var1` im Datensatz berechnen und in einer neuen Variable `pred_vorher` ablegen:
```{stata pred1, eval = F}
predict pred_vorher, xb
```
Hier wird also gerechnet:
        
+ `2.1351 + 0.5810811 * 1 = 2.716216`  
+ `2.1351 + 0.5810811 * 2 = 3.297297`  
+ `2.1351 + 0.5810811 * 7 = 6.202703`  
+ `2.1351 + 0.5810811 * 8 = 6.783784`  
        
```{r predx, echo=F}
m1 <- lm(var2~ var1, data = df)  
df$manual_vorhers = 2.1351 + 0.5811 * df$var1
df$pred_vorher <- m1$fitted.values
df %>% select(matches("(var|pred_)"))
```

Die Grafik oben zeigt wie Vorhersagen auf Basis des Regressionsmodells aussehen: Sie entsprechen den Werten auf der blauen Geraden (der sog. Regressionsgeraden) an den jeweiligen Stellen für `var1`. 
      
### Korrelation und Regression
```{r reg_korr,out.width = "100%", fig.align="center" , echo = F,warning=F,message=F, dpi = 900}
set.seed(26131)
df <- 
  tibble(x = seq(-20,20,.25)) %>% 
        mutate(e = runif(nrow(.),-20,20),
               y1 = 2.1225 + 1.12*x + e,
               yhat = predict(lm(y1~x),newdata = data.frame(x)),
               yres = y1 - yhat,
               y2 = yhat + 3* yres,
               y3 = 16.2 - 1.12*x + e*-1,
               yhat3 = predict(lm(y3~x),newdata = data.frame(x)),
               yres3 = y3 - yhat3,
               y4 = yhat3 + 3*yres3) 

df2 <- 
  df %>% 
  pivot_longer(cols = matches("y\\d"),names_to = "set", values_to = "y") %>% 
  group_by(set) %>% 
  mutate(cor =  sprintf("%.3f",cor(x,y)),
         my= mean(y),
         mx= mean(x))  %>% 
  group_by(set) %>% nest() %>% 
  mutate(m = map(data,~lm(y~x,data=.x) %>% broom::tidy(.)),
         r2 = map(data,~summary(lm(y~x,data=.x))$r.squared ) %>% unlist(.) ) %>% 
  unnest(data) %>% 
  unnest(m) %>% 
  mutate(term = ifelse(term=="x","b1","b0")) %>% 
  select(-matches("statistic|std\\.|p\\.va")) %>% 
  pivot_wider(names_from = term,values_from = estimate) %>% 
  mutate(reg_fm = paste0("hat(italic(y))==~",format(b0, digits = 2),"~+~",format(b1, digits = 2),"%*%~x")) 
  # mutate(reg_fm = paste0("italic(y) == ", format(b0, digits = 2), " + ",format(b1, digits = 2)," %*%")) 


ggplot(df2,aes(x,y)) + 
  geom_hline(aes(yintercept = my),size = .75, color = "orange") +
  geom_vline(aes(xintercept = mx),size = .75, color = "navy") +
  geom_point(shape = 21, color = "#020C3D", fill = "grey95") + 
  geom_smooth(method = "lm",se=F, color = "#129FB3",size = .75) +
  geom_label(data = distinct(df2,cor,set,b0,b1,reg_fm), 
            aes(x = 0,y = 114,
                label =  paste0("Pearson's r = ",cor) ),
            fill = alpha("grey99",.97), color = "grey25",
            family = "serif",
            hjust= .5, label.size = 0, label.r = unit(.2,"lines")) +
  geom_label(data = distinct(df2,cor,set,b0,b1,reg_fm), 
             aes(x = 0,y = 95,
                 label =  reg_fm),
             fill = alpha("grey99",.97), color = "grey25",
             family = "serif",
             parse = T,
             hjust= .5, label.size = 0, label.r = unit(.2,"lines")) +
  facet_wrap(~set,nrow = 2) +
  coord_cartesian(ylim = c(-72,115.5)) +
  theme_minimal() +
  theme(
    panel.background = element_rect(fill = "grey98", color = NA),
    strip.text = element_blank()) 

```

Außerdem entspricht der Korrelationskoeffizient der Wurzel von $R^2$ oder umgekfehrt: $(R^2)^2$ entspricht dem quadrierten Korrelationskoeffizienten:


```{stata corr_r2reg2, eval=F}
reg var2 var1, notable // notable um regtabelle auszublenden
```
```{stata corr_r2reg, echo=F}
qui use "regression_bsp.dta", clear
reg var2 var1, notable
```

```{stata corr_r2corv, eval=F}
pwcorr var2 var1
```
```{stata corr_r2corc, echo=F}
qui use "regression_bsp.dta", clear
pwcorr var2 var1
```


```{stata r2corr, collectcode = F}
dis sqrt(0.8470) // R² zu corr
```

```{stata r2corr2, collectcode = F}
dis 0.9203^2 // corr zu R²
```




***

**[Übung 1](#regue1)**

***



## kategoriale unabhängige Variablen

Wir können aber auch kategoriale unabhängige Variablen in Regressionsmodellen verwenden.

### Dummyvariablen

Ein klassisches Beispiel für ene Dummyvariable aus unab. Variable in einem Regressionsmodell ist der Zusammenhang zwischen dem Geschlecht und dem Einkommen (Stichwort Gender Pay Gap). In der ETB 2018 ist das Geschlecht unter `S1` abgelegt: `1` steht dabei für Männer, `2` steht für Frauen.

Wie können wir `S1` nun in das Regressionsmodell aufnehmen? Bisher hatten wir ja nur Modelle für metrische unabhängige und abhängige Variablen erstellt. Das Geschlecht ist aber kein metrisches Merkmal ($\Rightarrow$ sondern?). Die Zahlencodes sollten hier nicht als Werte verwendet werden. Hier gibt es keine Punktewolke, durch die sinnvoll eine Gerade durch Residuenminimierung gezogen werden kann:
```{r dummyplot, out.width="70%", out.height="70%", fig.align='center', echo = F, warning=F}
baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F,
                                select.cols = c("S1","F518_SUF","S3")) %>% 
  mutate(across(matches("S1|F"), ~ ifelse(.x>99998,NA,.x)),
         S3 = ifelse(S3 %in% 2:9,S3,NA),
         educ = case_when(S3 %in% 2:4 ~ "1",
                          S3 %in% 5:6 ~ "2",
                          S3 %in% 7:9 ~ "3")) %>% 
  select(S1,F518_SUF,S3, educ) %>% filter(F518_SUF<99998)

baua <- baua %>% group_by(S1) %>% mutate(mean_inc = mean(F518_SUF,na.rm = T), n1 = 1:n() ) %>% ungroup(.)
baua$sex1  <- factor(baua$S1)


mark_color <- "grey25"
color1x =  "#00519E" # uol farbe
colorhex <- "#FCFCFC" #"#FCF9F0FF"7

baua %>% 
  ggplot(.,aes(x = sex1, y = F518_SUF, color = sex1)) +
  geom_point(size = .5) +
  scale_x_discrete(breaks = c("1","2"), labels = c("Männer", "Frauen")) +
  scale_color_manual(values = scico::scico(3,palette = "davos")[1:2],guide = F) +
  labs(color = "", 
       y = "Einkommen (€)",
       x = "") +
  theme_minimal(base_family = "Nunito",base_size = 10) +
  theme(
    text = element_text(family = "Nunito"),
    plot.background = element_rect(fill = colorhex, linetype = 1, colour = NA),
    rect = element_rect(fill = colorhex, linetype = 1, colour = NA),
    axis.text =  element_text(color = mark_color,face = "plain", size = rel(1.05), angle = 0), 
    axis.title = element_text(color = mark_color,face = "plain", size = rel(1), angle = 0), 
    axis.title.y = element_text(color = mark_color,face = "plain", angle = 90,vjust = .5), 
    axis.ticks = element_blank(),
    axis.line = element_line(size = .1), 
    legend.text = element_text(family = "Nunito"),
    legend.title = element_text(family = "Nunito"),
    panel.grid = element_line(colour = "grey81", linetype = 1, size = .15), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.minor.x = element_blank(), 
    plot.subtitle = element_text(hjust=.5,family = "Nunito"),
    plot.caption = element_text(hjust=1, size = rel(1.2), color = mark_color),
    plot.margin = unit(c(1, 1, 1, 1), "lines"))

```

In einem Regressionsmodell müssen wir Stata mit dem Präfix `i.` mitteilen, dass es sich bei der Variable um eine kategoriale Variable handelt:

```{stata reg1, eval = F}
reg F518_SUF i.S1
```


```{stata reg1a, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode F518_SUF, mv(99998/99999)
}
reg F518_SUF i.S1
```
```{r, echo=F}
baua$sex <- factor(baua$S1)
m2 <- lm(F518_SUF ~ sex,baua[baua$F518_SUF<99998,])
```

Der Koeffizient für `S1` entspricht also der Differenz zwischen dem mittleren Einkommen für Männer und Frauen. Durch den Zusatz `weiblich` erhalten wir zusätzlich die Information über die *Referenzkategorie*: 

Befragte mit der Ausprägung `S1 = 2`, also Frauen,  haben im Vergleich zu Befragten mit der anderen Ausprägung ($\rightarrow$ Männer) ein um `r sprintf("%2.3f", abs(m2$coefficients[2]))` EUR niedrigeres Einkommen. Der Zusammenhang zwischen Einkommen und dem Geschlecht der Befragten ist am 0,001-Niveau signifikant. 

Formal sieht unser Modell also so aus:

$$\widehat{\text{F518_SUF}} =  \hat{\beta0} + \hat{\beta1} \times \texttt{S1=2}$$


Für `S1=2` setzen wir dann entweder 0 oder 1 ein - je nachdem ob wir einen vorhergesagten Wert für Frauen oder für Männer berechnen wollen. 0 setzen wir immer dann ein, wenn wir einen Wert für die *Referenzkategorie* berechnen wollen:


`r sprintf("%2.3f", abs(m2$coefficients[1]))` + `r sprintf("%2.3f", m2$coefficients[2])` $\times 0$ = `r sprintf("%2.3f", abs(m2$coefficients[1]))`

Für Frauen sähe die Berechnung wie folgt aus - hier setzen wir die 1 ein, weil sozusagen `sex2` zutrifft:

`r sprintf("%2.3f", abs(m2$coefficients[1]))` + `r sprintf("%2.3f", m2$coefficients[2])` $\times 1$ = `r sprintf("%2.3f", m2$coefficients[1] + m2$coefficients[2])`

Die Logik folgt hier einem Gruppenvergleich zwischen den Gruppen. Wir vergleichen also die Mittelwerte zwischen verschiedenen Gruppen, in diesem Fall zwischen Männern und Frauen:[^1]

[^1]: Um zu verdeutlichen wieviele Befragte jeweils das (ungefähr) gleiche Einkommen haben, wurden die Punkte hier nach links und rechts verschoben. Im Unterschied zu den bisherigen Streudiagrammen hat die relative Position innerhalb der Kategorien Männer und Frauen jedoch keine Bedeutung.

```{r educplot,out.width="70%", fig.align='center', echo = F, warning=F}
baua$sex <- factor(baua$S1)
m2 <- lm(F518_SUF ~ sex,baua[baua$F518_SUF<99998,])

theme_x <- 
  theme_minimal(base_family = "Nunito",base_size = 10) +
  theme(
    text = element_text(family = "Nunito"),
    plot.background = element_rect(fill = colorhex, linetype = 1, colour = NA),
    rect = element_rect(fill = colorhex, linetype = 1, colour = NA),
    axis.text =  element_text(color = mark_color,face = "plain", size = rel(1.05), angle = 0), 
    axis.title = element_text(color = mark_color,face = "plain", size = rel(1), angle = 0), 
    axis.title.y = element_text(color = mark_color,face = "plain", angle = 90,vjust = .5), 
    axis.ticks = element_blank(),
    axis.line = element_line(size = .1), 
    legend.text = element_text(family = "Nunito"),
    legend.title = element_text(family = "Nunito"),
    panel.grid = element_line(colour = "grey81", linetype = 1, size = .15), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.minor.x = element_blank(), 
    plot.subtitle = element_text(hjust=.5,family = "Nunito"),
    plot.caption = element_text(hjust=1, size = rel(1.2), color = mark_color),
    plot.margin = unit(c(1, 1, 1, 1), "lines"))


lab_df <- data.frame(sex1 = c("1","2"), 
                     F518_SUF  = c(mean(baua$F518_SUF[baua$sex1=="1"], na.rm = T),mean(baua$F518_SUF[baua$sex1=="2"], na.rm = T)), 
                     S1  = c(-.75,1), 
                     lab1 = c(paste0("bar(F518_SUF_[m])==", sprintf("%2.1f",mean(baua$F518_SUF[baua$sex1=="1"], na.rm = T))),
                              paste0("bar(F518_SUF_[f])==", sprintf("%2.1f",mean(baua$F518_SUF[baua$sex1=="2"], na.rm = T)))) )
lab_df2 <- data.frame(sex1 = "1", 
                     F518_SUF  = mean(baua$F518_SUF[baua$sex1=="1"], na.rm = T) + mean(baua$F518_SUF[baua$sex1=="2"], na.rm = T)/2 , 
                     S1  = 0, 
                     lab1 = paste0("Delta==", 
                                   sprintf("%2.1f",mean(baua$F518_SUF[baua$sex1=="1"], na.rm = T) - mean(baua$F518_SUF[baua$sex1=="2"], na.rm = T)) ))

baua %>% 
  ggplot(.,aes(x = S1, y = F518_SUF, color = sex1)) +
  geom_point(position = position_jitter(height = 0, width = .25), size = .75) +
  geom_tile(data = filter(baua, n1 ==1), aes(x = S1, y = mean_inc, width = 1, height = 50), color = scico(7,palette = "davos")[6:5],
            fill = scico(7,palette = "davos")[6:5]) +
  geom_segment(data=data.frame(sex1 = "1", S1= 1.5, ymin = min(baua$mean_inc), ymax = max(baua$mean_inc)), aes(xend = S1,y=ymax,yend=ymin),
               arrow = arrow(length = unit(.35,units = "lines"),type = "closed"), color = "orange",size = .75) +
  # geom_label(data =lab_df,aes(label = lab1),family = "Nunito",
  #            size = 3, hjust = 0, parse = T, nudge_x = 1.45,label.size = .01) + # Beschriftung der Mittelwerte
  geom_curve(data=data.frame(sex1 = "1", 
                             S1= 1.5, 
                             y1 = lab_df2$F518_SUF, 
                             y2 = mean(baua$F518_SUF[baua$se1x=="1"], na.rm = T) - 
                               (mean(baua$F518_SUF[baua$sex1=="1"], na.rm = T) - mean(baua$F518_SUF[baua$sex1=="2"], na.rm = T)) /2  ), 
             aes(xend = S1,y=y1,yend=y2),curvature = -.5,
             color = "orange",size = .5) +  # delta beschriftung
  geom_label(data =lab_df2,
           aes(label = lab1), hjust = .5,
           family = "Nunito", fill = "orange",
           size = 3, hjust = 0, parse = T, nudge_x = 1.45,label.size = .01) + #delta 
  coord_cartesian(ylim = c(0,20000)) +
  scale_x_continuous(breaks = 1:2, labels = c("Männer", "Frauen")) +
  scale_color_manual(values = scico(3,palette = "davos")[1:2],guide = F) +
  labs(color = "", 
       caption = "Werte über 20,000EUR ausgeblendet",
       y = "Einkommen (€)",
       x = "") +
  theme_x 
```

Wir können die Referenzkategorie natürlich auch ändern auf `S1 = 2`. 
Dazu geben wir `ib2.` an:

```{stata reg2, eval = F}
reg F518_SUF ib2.S1
```


```{stata reg2a, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode F518_SUF, mv(99998/99999)
}
reg F518_SUF ib2.S1
```


Wie hier ist dann also die Interpretation "umgekehrt": 
```{r, echo =F}
bauasex <- relevel(baua$sex, ref = "2")
m2b <- lm(F518_SUF ~ sex,baua)
```


Befragte mit der Ausprägung `S1 = 1`, also Männer, haben im Vergleich zu Befragten mit der anderen Ausprägung ($\rightarrow$ Frauen) ein um `r sprintf("%2.2f", abs(m2b$coefficients[2]))` EUR höheres Einkommen. Der Zusammenhang zwischen Einkommen und dem Geschlecht der Befragten ist am 0,001-Niveau signifikant. 

### Vergleich mit `ttest`


```{stata ttestrega, eval = F}
reg F518_SUF i.S1
ttest F518_SUF,by(S1)
```

```{stata ttestregb, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode F518_SUF, mv(99998/99999)
}
reg F518_SUF i.S1
ttest F518_SUF,by(S1)
```

### Mehrere Ausprägungen 

Diese Logik des Gruppenvergleichs lässt sich auch auf kategoriale Variablen mit mehreren Ausprägungen erweitern, bspw. der Schulbildung der Befragten (`educ` - siehe DoFile für Vorbereitung):
```{r ord_vars2, echo =F}
tribble(~"v",~"l",
        "educ", "höchster Schulabschluss",
          "1" , "max. Hauptschulabschluss",
          "2" , "max. mittlere Reife",
          "3" , "(Fach-)Abitur"
        ) %>% 
    kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 10) %>% 
  column_spec(1,monospace = TRUE) %>% 
  row_spec(c(1), bold = T, background = "#F2F2F2FF") %>% 
    row_spec(0, color = "white")
```

Wir müssen Stata wieder mit `i.` mitteilen, dass es sich um Gruppencodes (eine kategoriale Variable) handelt:
```{stata kat2reg2, eval = F}
reg F518_SUF i.educ
```
          
```{stata kat2reg1, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F518_SUF, mv(99998/99999)
	lab var educ "educ"
}
reg F518_SUF i.educ
```

```{r educreg_R, echo = F}
m4 <- lm(F518_SUF ~educ,baua)
```


Auch diese Koeffizienten sind als Gruppenvergleiche zu interpretieren - und zwar immer relativ zu der Ausprägung von `educ`, die *nicht* in der Regressionstabelle aufgeführt ist. (Hier also max. Hauptschule) 

Befragte mit mittlerer Reife haben ein um `r sprintf("%3.2f",m4$coefficients[2])` Euro höheres Bruttoeinkommen als Befragte, die maximal einen Hauptschulabschluss haben. Der Zusammenhang ist am 0,01-Niveau signifikant. \   

Befragte mit (Fach-)Abitur verdienen `r sprintf("%3.2f",m4$coefficients[3])` Euro höheres Bruttoeinkommen als Befragte, die maximal Hauptschulabschluss haben. Der Zusammenhang ist am 0,001-Niveau signifikant. \   


```{r,  echo=F, out.height="80%", out.width="80%", fig.align="center", message=F, warning=F}
baua_e <- baua %>% filter(!is.na(educ)) %>% group_by(educ) %>% mutate(mean_inc = mean(F518_SUF,na.rm = T), n1 = 1:n() ) %>% ungroup(.)

baua_e %>% 
  ggplot(aes(x = educ, y = F518_SUF, color = educ), alpha = .5) + 
  theme_x +
  geom_point(position = position_jitter(height = 0, width = .25), size = .75) +
  geom_tile(data = filter(baua_e, n1 ==1,!is.na(educ)), aes(x = educ, y = mean_inc, width = 1, height = 4), color = "black") +
  scale_color_manual(values = scico(6,palette = "tokyo")[2:5],guide = F) +
  scale_x_discrete(breaks = 1:3, labels = c("Hauptschulabschluss","mittlere Reife","(Fach-)Abi")) +
  labs(y = "Einkommen", x = "",
       caption = "Werte über 12 500 EUR ausgeblendet") +
  coord_cartesian(ylim = c(0,12500))
```  

Formal sieht unser Modell also so aus:
$$\widehat{\texttt{F518_SUF}} = \hat{\beta0} + \hat{\beta1}\times\texttt{educ=2} + \hat{\beta2} \times \texttt{educ=3}$$

Für `educ=2` wird nur dann eine `1` eingesetzt, wenn der/die Befragte mittlere Reife hat - alle anderen Variablen sind dann notwendigerweise auf 0 gesetzt. Für eine(n) Befragte*n mit max Hauptschulabschluss (also eine Person aus der Referenzkategorie) wird für alle Variablen die Null eingesetzt. 

Wir können die Referenzkategorie natürlich auch ändern, bspw. auf `educ = 3` - dazu geben wir `ib3.` an:
```{stata reg8, eval = F}
reg F518_SUF ib3.educ
```

```{stata reg8a, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode F518_SUF, mv(99998/99999)
	qui recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "mittlere Reife") (7/9 = 3 "(Fach-)Abi") (else = .), into(educ)
}
reg F518_SUF ib2.educ
```

***

**[Übung 2](#regue2)**

***


## mehrere unabhängige Variablen

Natürlich können wir auch mehrere unabhängige Variablen in unser Modell aufnehmen. Beispielsweise können wir so für die Wochenarbeitszeiten kontrollieren:
$$\hat{\texttt{F518_SUF}} = \beta0 + \beta1\times\texttt{educ2} + \beta2\times\texttt{educ3} + \beta3\times\texttt{F200} + \beta4\times\texttt{educ2}\times\texttt{F200}$$


```{stata mreg1, eval = F}
reg F518_SUF i.educ F200
```

```{stata mreg2, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F518_SUF, mv(99998/99999)
	lab var educ "educ"
	mvdecode F200, mv(97/99)
}
reg F518_SUF i.educ F200
```

Hier ändert sich dann die Interpretation: unter Konstanthaltung der Wochenarbeitszeit geben Befagte mit (Fach-)Abitur ein um 1632.40EUR höheres Einkommen als Befragte mit maximal Hauptschulabschluss an.

Ein äußerst hilfreicher Befehl in diesem Zusammenhang ist `margins` bzw. `marginsplot`:
```{stata mar1, eval =F}
margins, at( F200 = (15(5)45) educ = (1(1)3))
marginsplot
```
```{r marplot1, echo = F,out.width = "90%", fig.align="center"}
knitr::include_graphics("./pics/09_mplot1.png")
```


### Interaktionsterme

Wenn wir uns aber fragen, ob sich der Zusammenhang zwischen der Schulbildung und dem Einkommen mit der Wochenarbeitszeit verändert, benötigen wir einen Interaktionsterm:

$$\hat{\texttt{F518_SUF}} = \beta0 + \beta1\times\texttt{educ2} + \beta2\times\texttt{educ3} + \beta3\times\texttt{F200} + \beta4\times\texttt{educ2}\times\texttt{F200} + \beta5\times\texttt{educ3}\times\texttt{F200}$$

```{stata mreg13, eval = F}
reg F518_SUF i.educ##c.F200
```

```{stata mreg4, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F518_SUF, mv(99998/99999)
	lab var educ "educ"
	mvdecode F200, mv(97/99)
}
reg F518_SUF i.educ##c.F200
```

Bei Interaktionstermen empfiehlt sich ganz besonders, einen `marginsplot` zu erstellen:
```{stata marginIA, eval = F}
margins, at( F200 = (15(5)45) educ = (1(1)3))
marginsplot
```

```{r marplot2, echo = F,out.width = "90%", fig.align="center"}
knitr::include_graphics("./pics/09_mplot2.png")
```

### Quadratische Terme

Wenn wir uns aber fragen, ob sich der Zusammenhang zwischen der Wochenarbeitszeit und dem Einkommen nicht-linear verläuft, also sich abflacht oder eine zusätzliche Stunde Arbeitszeit mit höheren zusätzlichen Einkommen einhergeht (keine kausale Aussage!):

$$\hat{\texttt{F518_SUF}} = \beta0 + \beta1\times\texttt{educ2} + \beta2\times\texttt{educ3} + \beta3\times\texttt{F200} + \beta4\times\texttt{F200}\times\texttt{F200}$$

$$\hat{\texttt{F518_SUF}} = \beta0 + \beta1\times\texttt{educ2} + \beta2\times\texttt{educ3} + \beta3\times\texttt{F200} + \beta4\times\texttt{F200}^2$$

```{stata mreg13Q, eval = F}
reg F518_SUF i.educ c.F200##c.F200
```

```{stata mreg4Q, echo = F}
quietly{
set linesize 120
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
	mvdecode S3, mv(1 10/12 99)
	cap drop educ
	recode S3 (2/4 = 1 "Haupt")(5/6 = 2 "m. Reife") (7/9 = 3 "(Fach-)Abi"), into(educ)
	mvdecode F518_SUF, mv(99998/99999)
	lab var educ "educ"
	mvdecode F200, mv(97/99)
}
reg F518_SUF i.educ c.F200##c.F200
```

Bei quadratischen Termen empfiehlt sich ganz besonders, einen `marginsplot` zu erstellen:
```{stata marginQ, eval = F}
margins, at( F200 = (15(5)45) educ = (1(1)3))
marginsplot
```

```{r marplot3, echo = F,out.width = "90%", fig.align="center"}
knitr::include_graphics("./pics/09_mplot3.png")
```


## Übungen 9

Laden Sie die Erwerbstätigenbefragung. 

### Übung 1 {#regue1}

Berechnen Sie ein Regressionsmodell mit der Arbeitszeit (`az`) aus der Hauptbeschäftigung als abh. Variable und dem Alter der Befragten (`zpalter`) als unabh. Variable.
  + Denken Sie daran, die Missings in `zupalter` zu überschreiben: `mvdecode zpalter, mv(9999)`
  + Weclhe Richtung hat der Zusammenhang? Arbeiten ältere Befragte eher mehr oder eher weniger? 
  + Wie ist der Koeffizient zu interpretieren? Was können Sie über die Varianzaufklärung sagen?

### Übung 2 {#regue2}

3. Berechnen Sie ein Regressionsmodell mit der Arbeitszeit (`az`) aus der Hauptbeschäftigung als abh. Variable und der Ausbildung der Befragten (`m1202`) als unabh. Variable: wie sind die Koeffizienten zu interpretieren? 
`mvdecode m1202,mv(-1)`


### Übung 3 {#regue3}
4. Erstellen Sie ein multivariates Modell mit `az` als abhängiger Variable hat und der Ausbildung (`m1202`)  und dem Alter der Befragten (`zpalter`) als unabhängiger Variable.

5. Verändern Sie das multivariate Modell aus 4. um eine Interaktion zwischen der Ausbildung (`m1202`)  und dem Alter der Befragten (`zpalter`)


## Anhang

### `marginsplot` anpassen

```{stata marp2_cost, eval = F}
marginsplot, /// 
		graphregion(fcolor(white)) /// Hintergundfarbe (außerhalb des eigentlichen Plots)
    plot1(color("57 65 101")  msize(small)) /// Farbe & Größe für erste Gruppe
		ci1opts(color("57 65 101")) /// Farbe für Konfidenzintervalle der ersten Gruppe
		plot2(color("177 147 74") msize(small)) ///
		ci2opts(color("177 147 74")) /// 
		plot3(color("16 15 20")   msize(small)) /// 
		ci3opts(color("16 15 20")) /// 
    legend(cols(3) region(color(white) fcolor(white))  )   /// Spaltenzahl, Rahmen- & Hintergrundfarbe für Legende
		xtitle("vertragl. vereinb. Arbeitsstunden") /// Achsentitel
		ylabel(,angle(0)) /// Achen
    ytitle("Einkommen (F518_SUF)") /// 
		title("Titel") ///
    subtitle("Untertitel") ///
    caption("{it:Quelle: Erwerbstätigenbefragung 2018}", size(8pt) position(5) ring(5) )
```

```{r marplot2c, echo = F,out.width = "90%", fig.align="center"}
knitr::include_graphics("./pics/09_mplot4.png")
```

**[weiterführende Informationen](https://www.stata.com/meeting/germany18/slides/germany18_Jann.pdf)**

<!--chapter:end:09_mreg.Rmd-->

# Regression Hintergründe {#bg_reg}
```{r setup21, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
library(scico)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
options(width = 200)
df <- data.frame(var1 = c(1,2,7,8),
                 var2 = c(2,4,7,6))

# path <- "D:/oCloud/RFS/"
# ak <- readr::read_delim(paste0(path,"allbus_kumuliert_1980-2018.csv"), delim = ";", col_types = cols(.default = col_double())) %>%  
#   mutate_all(~ifelse(.<0,NA,.))
```


Zum Einstieg betrachten wir zunächst einen (fiktiven) Datensatz mit lediglich 4 Fällen und lediglich zwei Variablen: `var1` und `var2`: 
```{stata, eval = F}
use "https://github.com/filius23/Stata_Skript/raw/master/regression_bsp.dta", clear
list
```

```{r start, echo = F}
df <- data.frame(var1 = c(1,2,7,8),
                 var2 = c(2,4,7,6))
df
```

```{r regplot1, fig.align="center",out.width = "80%",fig.height= 3.5, eval=T, echo=F}
library(ggplot2)
ggplot(df, aes(x = var1, y = var2)) + 
  geom_point(size = 2) + 
  ggthemes::theme_stata() +
  scale_y_continuous(breaks = seq(0,8,2)) +
  scale_x_continuous(breaks = seq(0,8,2)) +
  theme(aspect.ratio = 1)
```


Ziel der Regression ist es, den Zusammenhang zwischen diesen beiden Variablen zu bestimmen. Gibt es einen Trend, in dem Sinn, dass ein höherer Wert von `var1` mit einem höheren oder niedrigeren Wert von `var2` einhergeht?

Etwas anders gesagt könnte man auch fragen, welchen Wert für `var2` wir vorhersagen würden, wenn wir `var1` kennen. Ein Ausgangspunkt ist das arithmetische Mittel. Dieses können wir mit `mean` zB für `var2` berechnen.[^055] Diesen Wert fügen wir als neue Spalte `mean_var2` in den Datensatz ein:

[^055]: Mehr zu `egen` in Kapitel \@ref(egen)

```{stata, eval=F}
egen mean_var2 = mean(var2)
```
  
```{r mittelw,echo=F}
df$mean_var2 <- mean(df$var2)
df
```

```{r plot_mittelwe,out.width = "80%",fig.height= 3.5, echo=F, fig.align="center"}
ggplot(df, aes(x = var1, y = var2)) + 
  geom_point(size = 3) + 
  ggthemes::theme_stata() +
  scale_y_continuous(breaks = seq(0,8,2)) +
  scale_x_continuous(breaks = seq(0,8,2)) +
  geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  geom_label(aes(y = 4, x = 5), label = "mean = 4.75", color = "grey30")+
  theme(aspect.ratio = 1)
```
Müssten wir eine Prognose für die Werte von `var2` abgeben, wäre das arith. Mittel eine gute Wahl. Die **vorhergesagten Werte** werden jeweils auf der Linie für das arith. Mittel liegen.   

```{r mittelw2,out.width = "80%",fig.height= 3.5, echo=F, fig.align="center"}
ggplot(df, aes(x = var1, y = var2)) + 
  geom_point(size = 3) + 
  ggthemes::theme_stata() +
  scale_y_continuous(breaks = seq(0,8,2)) +
  scale_x_continuous(breaks = seq(0,8,2)) +
  geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  geom_point(aes(x = var1, y = mean_var2), color = "darkorange", size = 3) +
  theme(aspect.ratio = 1)
```

### Residuen {#resid}

Allerdings liegen wir mit dem arith. Mittel dann immer auch Stück daneben. Diese Abweichung zwischen dem tatsächlichen und dem vorhergesagten Wert wird als **Residuum** bezeichnet, in unserem Beispiel ist das jeweils die Differenz zwischen `var2` und `mean`:
$$Residuum = beobachteter\, Wert \; - \; vorhergesagter\,Wert$$
Als Formel wird das in der Regel wie folgt dargestellt:
$$\epsilon_{\text{i}} = \text{y}_{i} - \hat{\text{y}}_{i}$$

Wir können also die Residuen als Differenz zwischen `var2` und `mean` berechnen und in `df` ablegen:
```{stata, eval = F}
gen m_abw = var2 - mean_var2
```


```{r, echo = F}
df$m_abw <- df$var2 - df$mean
df
```

```{r res_pl1,out.width = "80%",fig.height= 3.5, echo=F, fig.align="center" }
ggplot(df, aes(x = var1, y = var2)) + 
  geom_point(size = 3) + 
  ggthemes::theme_stata() +
  scale_y_continuous(breaks = seq(0,8,2)) +
  scale_x_continuous(breaks = seq(0,8,2)) +
  geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  geom_point(aes(x = var1, y = mean_var2), color = "darkorange", size = 3) +
  geom_segment(aes(x = var1, xend = var1, y = var2, yend = mean_var2), 
               color = "red", size = .65, linetype = "dotted") +
  theme(aspect.ratio = 1)
```

$\rightarrow$ Was bedeutet also ein negativer oder ein positiver Wert für das Residuum?[^056]

[^056]: Ein positiver Wert des Residuums bedeutet, dass der beobachtete Wert größer als der vorhergesagte Wert ist. Die Regressionsgerade *unterschätzt* also den Wert. Ein negatives Residuum bedeutet dementsprechend, dass die Regressionsgerade den Wert überschätzt - der wahre Wert ist niedriger als der vorhergesagte Wert. Siehe [Formeln](#resid)

Die horizontale Linie für das arithm. Mittel ist aber sehr deutlich nicht die beste Methode, um die Werte für `var2` vorherzusagen. In der Graphik können wir deutlich sehen, dass die Werte "weiter links", also mit geringeren Werten für `var1`, auch geringere Werte für `var2` aufweisen. Wir könnten also unseren Vorhersagefehler bzw. das *Residuum* minimieren indem wir die Linie drehen. Die Idee der Regressionsanalyse ist es dabei, die Residuuen zu minimieren. Was würde aber passieren wenn wir die Residuen aus der Mittelwertsvorhersage aufsummieren, um Sie dann zu minimieren?

### Quadrierte Residuen {#quad_res}

```{r, echo =F}
df
```
**Die Summe der Resiuden auf Basis des arith. Mittels ist immer Null!**    
Anders formuliert: die gestrichelten Linien nach oben sind in Summe genauso lang wie gestrichelten Linien nach unten.
Die Lösung ist die Residuen zu quadrieren. So ergibt sich eine Kennzahl, die wir minimieren können:
```{stata, eval=F}
gen m_abw2 = m_abw^2 
tabstat m_abw2, s(sum)
```

```{r abw2, echo = F}
df$m_abw2 <- df$m_abw^2 
df
```

```{r, echo = F}
sum(df$m_abw2)
m1 <- lm(var2~ var1, data = df)  
```



### Interpretation der Regression
  
Die Minimierung erledigt `reg` für uns. Hier geben wir zuerst das Merkmal an, das auf der y-Achse liegt (die *abhängige* Variable) und dann das Merkmal für die x-Achse (*unabhängige* Variable) an. Ein positiver Wert unter `Coef.` in der Zeile von `var1` bedeutet, dass unsere Gerade von links nach rechts ansteigt und ein negativer eine fallende Linie bedeuten würde. Der Wert unter `var1` gibt an, um *wieviel sich die Gerade pro "Schritt nach rechts" nach oben/unten verändert*. Die Gerade steigt also pro Einheit von `var1` um `r m1$coefficients[2]`:

```{stata, eval=F}
reg var2 var1
```


```{stata, echo=F}
use "regression_bsp.dta", clear
qui egen mean_var2 = mean(var2)
qui gen m_abw = var2 - mean_var2
qui reg var2 var1
qui predict pred_vorher, xb
qui gen res = var2 - pred_vorher 
qui gen res2 = res^2
qui set linesize 90
reg var2 var1
```


In unserer Grafik sieht diese Gerade so aus:
```{r,out.width = "80%",fig.height= 3.5, echo=T, fig.align="center" , echo = F,warning=F,message=F}
ggplot(df, aes(x = var1, y = var2)) + 
  geom_point(size = 2) + 
  ggthemes::theme_stata() +
  # geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  scale_y_continuous(breaks = seq(0,8,2)) +
  scale_x_continuous(breaks = seq(0,8,2)) +
  # geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  # geom_point(aes(x = var1, y = mean_var2), color = "darkorange", size = 3) +
  # geom_segment(aes(x = var1, xend = var1, y = var2, yend = mean_var2), 
               # color = "red", size = .65, linetype = "dotted") +
  geom_smooth(method = "lm", color = "darkblue" , se = FALSE) +
  theme(aspect.ratio = 1)
```


### Vorhergesagte Werte {#reg_pred}

Wie hoch ist nun der vorhergesagte Wert auf Basis der blauen Gerade? 
Die vohergesagten Werte aus `reg var2 var1` entsprechen einfach der Summe aus dem Wert neben `_cons` und dem Koeffizienten neben `var1` multipliziert mit dem jeweiligen Wert für `var1`.[^057]

```{stata, eval=F}
reg var2 var1, noheader
```


```{stata, echo=F}
use "regression_bsp.dta", clear
qui egen mean_var2 = mean(var2)
qui gen m_abw = var2 - mean_var2
qui reg var2 var1
qui predict pred_vorher, xb
qui gen res = var2 - pred_vorher 
qui gen res2 = res^2
qui set linesize 90
reg var2 var1, noheader
```

Vorhergesagte Werte werden mit $\widehat{var2}$  bezeichnet - der ^ steht dabei für "vorhergesagt":

$$\widehat{var2}=\texttt{Intercept} + 0.5811 \times \texttt{var1}$$  
Für die erste Zeile ergibt sich also folgender vorhergesagter Wert: 2.1351+0.5811\*1= `r 2.1351+0.5811*1`

[^057]: Die Option `noheader` macht den Output von `reg` etwas übersichtlicher.

Die vorhergesagten Werte können wir auch mit `predict` berechnen und in einer neuen Variable `pred_vorher` ablegen:
```{stata, eval = F}
predict pred_vorher, xb
```

```{r, echo=F}
df$manual_vorhers = 2.1351 + 0.5811 * df$var1
df$pred_vorher <- m1$fitted.values
df
```
Die Grafik zeigt wie Vorhersagen auf Basis des Regressionsmodells aussehen: Sie entsprechen den Werten auf der blauen Geraden (der sog. Regressionsgeraden) an den jeweiligen Stellen für `var1`. 
```{r,out.width = "80%",fig.height= 3.5, echo=T, fig.align="center" , echo = F,warning=F,message=F}
ggplot(df, aes(x = var1, y = var2)) + geom_point(size = 3) + ggthemes::theme_stata() +
  scale_y_continuous(breaks = seq(0,8,2)) +
  scale_x_continuous(breaks = seq(0,8,2)) +
  geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  geom_point(aes(x = var1, y = mean_var2), col = "darkorange", size = 3) +
  geom_segment(aes(x = var1, xend = var1, y = var2, yend = mean_var2), color = "red", size = .65, linetype = "dotted")  +
  geom_smooth(method = "lm", color = "darkblue" , se = FALSE) +
  geom_point(aes(x = var1, y = pred_vorher), color = "dodgerblue3", size = 3) +
  theme(aspect.ratio = 1)
```
Wir können erkennen, dass die hellblauen Punkte (also die Vorhersagen des Regressionsmodells) deutlich näher an den tatsächlichen Punkten liegen als die orangen Vorhersagen auf Basis des `mean`. 

### Residuen Teil 2 {#resid2}

Trotzdem sind auch die hellblauen Punkte nicht deckungsgleich mit den tatsächlichen Werten. Es gibt also auch hier wieder [Residuen](#resid), also Abweichungen des beobachteten vom vorhergesagten Wert. Wir können diese per Hand berechnen als Differenz zwischen dem tatsächlichen und dem vorhergesagten Wert:
```{stata, eval = F}
gen res = var2 - pred_vorher 
```
Oder wir können Sie mit `predict neue_variable , residuals` erstellen:
```{stata, eval = F}
reg var2 var1 // zunächst nochmal die regression laufen lassen
predict p_res , residuals
list
```
```{r, echo=F}
df$res <- m1$residuals
df$p_res <- m1$residuals
df
df$p_res <- NULL
```

Hier sind die Residuen für `p_res` als hellblaue Linien eingezeichnet:
```{r,out.width = "80%",fig.height= 3.5, echo=F, fig.align="center" , eval = T, message=F}
ggplot(df, aes(x = var1, y = var2)) + geom_point(size = 3) + ggthemes::theme_stata() +
  geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  geom_point(aes(x = var1, y = mean_var2), col = "darkorange", size = 3) +
  geom_segment(aes(x = var1, xend = var1, y = var2, yend = mean_var2), color = "red", size = .65, linetype = "dotted")  +
  geom_smooth(method = "lm", color = "darkblue" , se = FALSE) +
  geom_point(aes(x = var1, y = pred_vorher), color = "dodgerblue3", size = 3) +
  geom_segment(aes(x = var1, xend = var1, y = var2, yend = pred_vorher), color = "dodgerblue3", size = .65, linetype = 1) +
  theme(aspect.ratio = 1)
```
$\rightarrow$ *Wie groß ist die (einfache) Summe der Residuen für `pred_vorher`?*[^058]

[^058]: Die (einfache, unquadrierte) Summe der Resiuden beträgt auch hier Null.

### Modellgüte {#r2}

Um zu beurteilen, um wieviel besser unsere Gerade aus `reg` die Werte vorhersagen kann als der `mean` können wir die Summe der quadrierten Residuen vergleichen. Dazu quadrieren wir also die Residuen:
```{stata, eval = F}
gen res2 = res^2
list
```

```{r, echo = F}
df$res2 <- df$res^2
```

```{r, echo = F}
df
```

Dann können wir die Summen der quadierten Abweichungen aus der Mittelwertregel und dem Regressionsmodell vergleichen:

```{stata, collectcode = F, collapse = T}
dis 7.5625+0.5625+5.0625+1.5625 // abw2 aus Mittelwertsregel
```

```{stata, collectcode = F, collapse = T}
dis 0.5129657+0.4937911+0.6356830+0.6143170 // res2 aus regressionsmodell
```
Zum Beispiel können wir uns fragen, um wieviel sich die Summe der quadrierten Residuen verringert wenn wir statt des `mean` unser `reg`-Modell verwenden.
Wenn wir diese Veränderung ins Verhältnis mit dem "Ausgangswert", also den Residuen aus der Mittelwertregel setzen, dann erhalten wir das $R^{2}$ für unser `reg`-Modell. 
Dieses gibt die prozentuale Verringerung der Residuen durch das `reg`-Modell im Vergleich zur Mittelwertregel an: 
```{stata}
dis (14.75 -  2.256757) / 14.75
```
Unser Regressionsmodell kann also 84,7\% der Streuung um den Mittelwert erklären. Dieser Wert wird auch als $R^2$ bezeichnet. Im Regressionsoutput können wir das $R^2$ oben rechts neben `R-squared` ablesen. 
```{stata, collectcode = F, echo = F}
qui use "https://github.com/filius23/Stata_Skript/raw/master/regression_bsp.dta", clear
reg var2 var1
```
Außerdem sehen wir oben links in der Spalte `SS` die "Sum of Squares". Hier finden wir auch die Werte von oben wieder: unter Total ist die Summe der quadrierten Abweichungen der beobachteten Werte vom arith. Mittel angegeben (`14.75`, sozusagen die Summe der `m_abw2` von oben). Residual gibt die Summe der Abweichungsquadrate zwischen den beobachteten Werten und den vorhergesagten Werten der Regression (`2.256..`, Die Summe von `res2`). 


<!--chapter:end:21_bgregression.Rmd-->

# ANOVA {#anova}

```{r setup22, include=F}
.libPaths("D:/R-library4") 
library(tidyverse)
library(ggplot2)
library(LaCroixColoR)
library(patchwork)
library(Statamarkdown)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
knitr::opts_chunk$set(collapse = F)
knitr::opts_chunk$set(collectcode = F)

# ak <- readr::read_delim("D:/oCloud/RFS/allbus_kumuliert.csv", delim = ";", col_types = cols(.default = col_double())) 
# a14gr <- filter(ak, year == 2014, hs16>0 )
```

ANOVA steht für **an**alysis **o**f **v**ariance und wird auch als univariate Varianzanalyse bezeichnet. 

ANOVA wird verwendet, um Mittelwertunterschiede zwischen 2 oder mehr Gruppen zu vergleichen. Dies geschieht, indem die Varianz in den Daten betrachtet wird (daher der Name). Insbesondere vergleicht ANOVA das Ausmaß der Variation zwischen den Gruppen (*between variance*) mit dem Ausmaß der Variation innerhalb der Gruppen (*within variance*).  Wir hatten diese Logik der Varianzzerlegung schon bei Regressionsmodellen kennengelernt:

```{r anova_plt1,out.width = "80%",fig.height= 3.5, echo=F, fig.align="center" , eval = T, message=F}
df <- data.frame(var1 = c(1,2,7,8),
                 var2 = c(2,4,7,6)) %>% mutate(mean_var2 = mean(var2))
m1 <- lm(var2~ var1, data = df)  
df$pred_vorher <- m1$fitted.values

ggplot(df, aes(x = var1, y = var2)) + geom_point(size = 3) + ggthemes::theme_stata() +
  geom_hline(aes(yintercept = mean_var2), color = "grey50", size = .75, linetype = "dashed") +
  geom_point(aes(x = var1, y = mean_var2), col = "darkorange", size = 3) +
  geom_segment(aes(x = var1, xend = var1, y = var2, yend = mean_var2), color = "red", size = .65, linetype = "dotted")  +
  geom_smooth(method = "lm", color = "darkblue" , se = FALSE) +
  geom_point(aes(x = var1, y = pred_vorher), color = "dodgerblue3", size = 3) +
  geom_segment(aes(x = var1, xend = var1, y = var2, yend = pred_vorher), color = "dodgerblue3", size = .65, linetype = 1) +
  theme(aspect.ratio = 1)
```

Hier hatten wir die Gesamtvarianz in erklärte und unerklärte Varianz zerlegt. Diese Sum of Squares bezeichnet Stata `Model` und `Residual`:
```{stata regx_bsp, eval=F}
use "https://github.com/filius23/Stata_Skript/raw/master/regression_bsp.dta", clear
reg var2 var1
```

```{stata regx_bsp1, collectcode=F, echo = F}
set linesize 200
qui use "D:\oCloud\Home-Cloud\Lehre\Methodenseminar\Stata_Skript\regression_bsp.dta", clear
reg var2 var1
```

Diese Logik überträgt ANOVA auf kategoriale Variablen, indem hier die Varianz in eine Streuung zwischen (`between`) und innerhalb der (`within`) der Gruppen aufgeteilt wird:

```{r anova_plot1,out.width = "100%", fig.height= 8, dpi = 1000, echo=F, fig.align="center" , eval = T, message=F, warning =F}
eq <-  substitute(italic(bar(inc))) # formel erstellen
eq2 <-  substitute(italic(bar(inc[m]))) # formel erstellen
eq3 <-  substitute(italic(bar(inc[f]))) # formel erstellen



df2 <- tibble(sex = rep(1:2,15)) %>% mutate(inc = runif(30,1500,3000) + ifelse(sex==1, 1806.5515,0))

tot_plt <- 
    df2 %>% 
      mutate(mean = mean(inc,na.rm = T)) %>% 
      group_by(sex) %>% 
      mutate(mean_s = mean(inc,na.rm = T)) %>%
      ungroup() %>%
      sample_n(30) %>% 
      mutate(id = 1:n() ) %>%  # %>% str_pad(.,width = 2,side = "left",pad = 0) %>%  paste0(sex,. ) %>% parse_number(.) ) %>% 
      ggplot(.,aes(x = id, y = inc)) +
      geom_segment(aes(yend= mean, xend = id , color = factor(sex)),
                   size = .45, linetype = 2 ) +
      geom_line(aes(x=id,y=mean), size = .75, color = "#B8A369") +
      geom_point(size = 1.95,aes(color = factor(sex)) ) +
      geom_label(aes(x = 20, y = mean, label = as.character(as.expression(eq))), 
                 label.size = .01, hjust = 0.5, color = "#B8A369", fill = "white", parse = T, size = 4.5) +
      scale_color_manual( values = c("#172869","#5E70B5"), name = "", breaks = 1:2, labels = c("Männer","Frauen")) +
      guides(color= guide_legend(override.aes = list(shape = 15,size = 6) ,
                                 label.position ="right" , ncol = 2,reverse = F)  ) + 
      labs(y = "Einkommen (inc)", x = "", caption = "Fiktive Daten", title = "Total variation") +
      ggthemes::theme_stata(base_size = 13) +
      expand_limits(y = c(1000,4800)) +
      theme(axis.text.x = element_blank(), 
            plot.title = element_text(hjust=.5),
            plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot",
            legend.background = element_rect(fill="grey95", size=.05),
            legend.title = element_blank(),
            legend.justification=c(0.5,0), legend.position=c(.5,0))
wit_plt <-
          df2 %>% 
            mutate(mean = mean(inc,na.rm = T)) %>% 
            group_by(sex) %>% 
            mutate(mean_s = mean(inc,na.rm = T)) %>%
            sample_n(15) %>%
            ungroup() %>%
            mutate(id = 1:n() %>% ifelse(sex==2,.+7,.)) %>%  
            ggplot(.,aes(x = id, y = inc)) +
            geom_segment(aes(yend= mean_s, xend = id , color = factor(sex)),
                         size = .45, linetype = 2 ) +
            geom_line(aes(x=id,y=mean_s,color = factor(sex), group =  factor(sex)), size = .75) +
            geom_point(size = 1.95,aes(color = factor(sex)) )+ 
            geom_label(data = data.frame(x1 = 16, mean_s1 = mean(df2$inc[df2$sex==1]), sex = 1),
                       aes(x = x1, y = mean_s1, label = as.character(as.expression(eq2)) , color = factor(sex)), 
                       label.size = .01, hjust = 0, fill = "white", parse = T, size = 4.5, show.legend = F) +
            geom_label(data = data.frame(x1 = 22, mean_s1 = mean(df2$inc[df2$sex==2]), sex = 2),
                       aes(x = x1, y = mean_s1, label = as.character(as.expression(eq3)) , color = factor(sex)), 
                       label.size = .01, hjust = 1, fill = "white", parse = T, size = 4.5, show.legend = F) +
            scale_color_manual( values = c("#172869","#5E70B5"), name = "", breaks = 1:2, labels = c("Männer","Frauen")) +
            guides(color= guide_legend(override.aes = list(shape = 15,size = 6) ,
                                       label.position ="right" , ncol = 1,reverse = F)  ) + 
            labs(y = "Einkommen (inc)", x = "", caption = "Fiktive Daten", title = "Within-group-variation") +
            ggthemes::theme_stata(base_size = 9) +
            expand_limits(y = c(1000,4800)) +
            theme(axis.text.x = element_blank(), 
                  plot.title = element_text(hjust=.5),
                  plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot",
                  legend.background = element_rect(fill="grey95", size=.05),
                  legend.title = element_blank(),
                  legend.justification=c(1,1), legend.position=c(1,1))


bet_plt <-
           df2 %>% 
              mutate(mean = mean(inc,na.rm = T)) %>% 
              group_by(sex) %>% 
              mutate(mean_s = mean(inc,na.rm = T)) %>%
              sample_n(15) %>%
              ungroup() %>%
              mutate(id = 1:n() %>% ifelse(sex==2,.+7,.)) %>%  
              ggplot(.,aes(x = id, y = mean)) +
              geom_point(size = 1.95,aes(y = inc,color = factor(sex)), alpha = .3 )+ 
              geom_segment(aes(yend= mean_s, xend = id, color = factor(sex) ),
                           linetype = "dotted", size = .65, show.legend = F) +
              geom_line(aes(x=id,y=mean_s,color = factor(sex), group =  factor(sex)), size = .75) +
              geom_hline(aes(yintercept = mean), color = "#B8A369", size = .75)  +
              geom_label(data = data.frame(x1 = 16, mean_s1 = mean(df2$inc[df2$sex==1]), sex = 1),
                         aes(x = x1, y = mean_s1, label = as.character(as.expression(eq2)) , color = factor(sex)), 
                         label.size = .01, hjust = 0, fill = "white", parse = T, size = 4.5, show.legend = F) +
              geom_label(data = data.frame(x1 = 22, mean_s1 = mean(df2$inc[df2$sex==2]), sex = 2),
                         aes(x = x1, y = mean_s1, label = as.character(as.expression(eq3)) , color = factor(sex)), 
                         label.size = .01, hjust = 1, fill = "white", parse = T, size = 4.5, show.legend = F) +
              geom_label(aes(x = 20, y = mean, label = as.character(as.expression(eq))), label.size = .01, hjust = 0.5, color = "#B8A369", fill = "white", parse = T, size = 4.5) +
              scale_color_manual( values = c("#172869","#5E70B5"), name = "", breaks = 1:2, labels = c("Männer","Frauen")) +
              scale_linetype_manual(values = c(2,NA)) +
              guides(color= guide_legend(override.aes = list(shape = 15,size = 6) ,
                                         label.position ="right" , ncol = 1,reverse = F)  ) + 
              labs(y = "Einkommen (inc)", x = "", caption = "Fiktive Daten", title = "between-group-variation") +
              ggthemes::theme_stata(base_size = 9) +
              expand_limits(y = c(1000,4800)) +
              theme(axis.text.x = element_blank(), 
                    plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot",
                    legend.background = element_rect(fill="grey95", size=.05),
                    legend.title = element_blank(),
                    legend.justification=c(1,1), legend.position=c(1,1))

tot_plt / (bet_plt | wit_plt )
```

Aus diesen beiden Streuungen berechnen wir wieder `Sum of squares`, welche dann ins Verhältnis gesetzt werden, um den sog. F-Wertb zu berechnen:

$$F=\frac{\textbf{between}\;\texttt{Sum of Squares}}{\textbf{within}\;\texttt{Sum of Squares}}$$

Wenn der durchschnittliche Unterschied zwischen den Gruppen ähnlich ist wie innerhalb der Gruppen, beträgt das F-Verhältnis etwa 1. Wenn der durchschnittliche Unterschied zwischen den Gruppen größer wird als der innerhalb der Gruppen, wird das F-Verhältnis größer als 1. Um einen P-Wert zu erhalten, kann er gegen die F-Verteilung einer Zufallsvariablen mit den mit dem Zähler und Nenner des Verhältnisses verbundenen Freiheitsgraden getestet werden (ähnlich wie beim F-Test oben). Der P-Wert ist die Wahrscheinlichkeit, dieses oder ein größeres F-Verhältnis zu erhalten. Größere F-Verhältnisse ergeben kleinere P-Werte.

Mit `oneway F518_SUF S1, tabulate` bekommen wir bspw. die Varianzzerlegung der Einkommensangaben (`F518_SUF`) nach Geschlechtern (`S1`):
```{stata anova1, eval =F }
oneway F518_SUF S1, tabulate
```

```{stata anova2, echo =F ,  collectcode=F}
set linesize 200
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0_clean.dta",clear
oneway F518_SUF S1, tabulate
```
Die deskriptive Zusammenfassung oben liefert einige Deskriptionen: das arith. Mittel (`Mean`), die Standardabweichung (`Std. Dev.`) und die Stichprobengrößen (`Freq.`) für die abhängige Variable (Einkommen in unserem Beispiel) für jede Gruppe der unabhängigen Variable `S1` (also Frauen und Männer) sowie wenn alle Gruppen kombiniert werden (`Total`). 

Die Stata-Ausgabe der einseitige ANOVA findet sich in der unteren Tabelle und zeigt an, ob wir einen statistisch signifikanten Unterschied zwischen unseren beiden Gruppenmittelwerten haben. Das Verhältnis von `between` und `within` wird unter `F` angegeben. Wir können sehen, dass das Signifikanzniveau `Prob > F` deutlich unter 0,05 liegt. Das legt einen statistisch signifikanten Unterschied im mittleren Einkommen den beiden Gruppen nahe. 

Außerdem werden uns die Sum of Squares für die Unterschiede innerhalb und zwischen den Gruppen angezeigt. Wir sehen hier, dass die Varianz innerhalb der Gruppen die Gruppendifferenz deutlich übersteigt: die Sum of Squares zwischen den Gruppen sind mit `1.9886e+11` deutlich geringer als die Within-group SS `8.5219e+09`. 
(`8.5219e+09` steht für `8,521,900,000`, also `4.883*10^9` ",`1.9886e+11` entspricht `198,860,000,000`).
Wir können aus den Zahlen für die Sum of Squares auch die Varianzaufklärung durch die Variable `S1` berechnen (`between`/`Total`):

```{stata dipl1r2, collectcode=F}
dis  (8.5219e+09/ 2.0738e+11)
```
Durch Kenntnis der Varibale `S1` können also 4.11% der gesamten Varianz ("Unterschiede") des Einkommens erklärt werden.


### ANOVA vs. t-Tests

Der t-Test wird beim Vergleich zweier Gruppen verwendet, während die ANOVA für den Vergleich von mehr als 2 Gruppen verwendet wird. Wenn wir den p-Wert unter Verwendung der ANOVA für 2 Gruppen berechnen, erhalten wir die gleichen Ergebnisse wie beim t-Test - hier also einen signifikanten Gruppenunterschied:
```{stata ttest_anova1, eval =F }
ttest F518_SUF, by(S1) unequ
```

```{stata  ttest_anova2, echo =F }
set linesize 200
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0_clean.dta",clear
ttest F518_SUF, by(S1) unequ
```

### ANOVA vs. lineare Regression

Die lineare Regression wird zur Analyse kontinuierlicher Beziehungen verwendet; die Regression ist jedoch im Wesentlichen die gleiche wie die ANOVA. Bei der ANOVA berechnen wir Mittelwerte und Abweichungen unserer Daten von den Mittelwerten. Bei der linearen Regression berechnen wir die beste Linie durch die Daten und berechnen die Abweichungen der Daten von dieser Linie. Stata gibt uns das F-Verhältnis bei Regressionsmodellen direkt mit aus. Zu beachten ist aber hier, dass wir für eine kategoriale unabhängige Variable `i.` voranstellen müssen:

```{stata reg_anova, eval =F }
reg F518_SUF i.S1
```

```{stata  reg_anova2, echo =F,collectcode = F }
set linesize 200
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0_clean.dta",clear
reg F518_SUF i.S1
```

+ die obere Tabelle entspricht dem Output von `oneway`: 
  + die Sum of Squares innerhalb von `S1` betragen `1.9886e+11`, zwischen `S1` ist die Sum of Squares (`8.5219e+09`)
  + Das Verhältnis der within und between Streuung beträgt `F(  1, 16633) =  712.78` (siehe Spalte `F` in `oneway`)
  + die Variable `S1` kann 4,11% der Streuung der Einkommen erklären (`R-squared =  0.0411`), siehe unsere Berechnung oben

- die untere Tabelle entspricht dem `ttest`:
  + Frauen verdienen im Mittel `1431.809` weniger als Männer und dieser Unterschied ist statistisch signifikant  (vgl. `diff` bei `ttest`) 

***

[Übungen 3](#anova1)

***


## ANOVA mehrere Gruppen

Der Vorteil von ANOVA ist aber, dass sich auch Gruppenunterschiede für Variablen mit mehr als zwei Ausprägungen untersuchen lassen, bspw. Schulbildungsniveaus:

```{r anova_plot2,out.width = "100%", fig.height= 8, dpi = 1000, echo=F, fig.align="center" , eval = T, message=F, warning =F}
set.seed(023123)
df3 <- tibble(educ = rep(1:5,8)) %>% mutate(inc = runif(40,1500,3000),
                                                            inc = case_when(educ == 2 ~ inc + 347.0904,
                                                                            educ == 3 ~ inc + 544.888 ,
                                                                            educ == 4 ~ inc + 1074.884,
                                                                            educ == 5 ~ inc + 1270.937,
                                                                            TRUE ~ inc))

 tot_plt3 <- 
    df3 %>% 
  mutate(mean = mean(inc,na.rm = T)) %>% 
  group_by(educ) %>% 
  mutate(mean_s = mean(inc,na.rm = T)) %>%
  ungroup() %>%
  mutate(id = 1:n() ) %>%  
  ggplot(.,aes(x = id, y = inc)) +
  geom_hline(aes(yintercept = mean), color = "#AF6125", size = .75)  +
  geom_segment(aes(yend= mean, xend = id , color = factor(educ)),
                 size = .45, linetype = 2 ) +
  geom_point(size = 1.95,aes(color = factor(educ)) ) +
  scale_color_viridis_d(name = "", breaks = 1:5, labels = c("k.Abs.","Hauptschule","Mittlere Reife","Fachabi","Abi"), end = .75) +
  scale_linetype_manual(values = c(2,NA)) +
  guides(color= guide_legend(override.aes = list(shape = 15,size = 6) ,
                             label.position ="right" , ncol = 3,reverse = F)  ) +  
    labs(y = "Einkommen (inc)", x = "", caption = "Fiktive Daten", title = "Total variation") +
    ggthemes::theme_stata(base_size = 13) +
    expand_limits(y = c(1000,4800)) +
      theme(axis.text.x = element_blank(), 
            plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot",
            legend.background = element_rect(fill=NA, color=NA),
            legend.title = element_blank(),
            legend.justification=c(1,0), legend.position=c(1,0))


wit_plt3 <-
  df3 %>% 
  mutate(mean = mean(inc,na.rm = T)) %>% 
  group_by(educ) %>% 
  mutate(mean_s = mean(inc,na.rm = T)) %>%
  ungroup() %>%
  arrange(educ) %>% 
  mutate(id = 1:n() ) %>%  
  ggplot(.,aes(x = id, y = inc)) +
  geom_segment(aes(yend= mean_s, xend = id , color = factor(educ)),
               size = .45, linetype = 2 ) +
  geom_line(aes(x=id,y=mean_s,color = factor(educ), group =  factor(educ)), size = .75) +
  geom_point(size = 1.95,aes(color = factor(educ)) )+ 
  scale_color_viridis_d(name = "", breaks = 1:5, labels = c("k.Abs.","Hauptschule","Mittlere Reife","Fachabi","Abi"), end = .75) +
  guides(color= F) +
  labs(y = "Einkommen (inc)", x = "", caption = "Fiktive Daten", title = "Within-group-variation") +
  ggthemes::theme_stata(base_size = 7) +
  expand_limits(y = c(1000,4800)) +
  theme(axis.text.x = element_blank(), 
        plot.title = element_text(hjust=.5),
        plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot",
        legend.background = element_rect(fill=NA, color=NA),
        legend.title = element_blank(),
        legend.justification=c(1,0), legend.position=c(1,0))

  
bet_plt3 <-
  df3 %>% 
  mutate(mean = mean(inc,na.rm = T)) %>% 
  group_by(educ) %>% 
  mutate(mean_s = mean(inc,na.rm = T)) %>%
  ungroup() %>%
    arrange(educ) %>% 
  mutate(id = 1:n() ) %>% 
  ggplot(.,aes(x = id, y = mean)) +
  geom_point(size = 1.95,aes(y = inc,color = factor(educ)), alpha = .3 )+ 
  geom_segment(aes(yend= mean_s, xend = id, color = factor(educ)), 
               linetype = "dotted", size = .45, show.legend = F) +
  geom_line(aes(x=id,y=mean_s,color = factor(educ), group =  factor(educ)), size = .5) +
  geom_hline(aes(yintercept = mean), color = "#AF6125", size = .75)  +
  scale_color_viridis_d(name = "", breaks = 1:5, labels = c("k.Abs.","Hauptschule","Mittlere Reife","Fachabi","Abi"), end = .75) +
  scale_linetype_manual(values = c(1,NA)) +
  guides(color= F  ) + 
  labs(y = "Einkommen (inc)", x = "", caption = "Fiktive Daten", title = "Between-group-variation") +
  ggthemes::theme_stata(base_size = 7) +
  expand_limits(y = c(1000,4800)) +
  theme(axis.text.x = element_blank(), 
        plot.caption = element_text(size = rel(.75)), plot.caption.position = "plot",
        legend.background = element_rect(fill=NA, color=NA),
        legend.title = element_blank(),
        legend.justification=c(1,0), legend.position=c(1,0))

tot_plt3 / (bet_plt3 | wit_plt3 )
```


```{stata anova3groupts, eval = F}
oneway F518_SUF m1202, tabulate
```

```{stata anova3groupts2, echo = F,collectcode = F }
set linesize 200
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0_clean.dta",clear
oneway F518_SUF m1202, tabulate
```

Wir erkennen:

+ dass signifikante Gruppenunterschiede bestehen: `Prob > F` ist deutlich < 0,05
+ Kenntnis von `m1202` kann 
+ dass Befragte mit einem Abschluss der Fachhochschule, Universität/ geh., höhere Beamte die höchsten Durchschnittseinkommen haben (`Mean` = `4643.9031`), Befatgte ohne Berufsabschluss die niedrigsten (`Mean` = `2150.7608`) usw.

Auch hier der Vergleich zu Regressionsmodellen einer kategorialen UV:

```{stata reg_anova3, eval =F }
reg F518_SUF i.m1202
```

```{stata  reg_anova4, echo =F , collectcode = T,warning = F}
clear all
set linesize 200
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0_clean.dta"
reg F518_SUF i.m1202
```


$\rightarrow$ hier sind die Koeffizienten jeweils auf den Vergleich zu `m1202=1` zu interpretieren:

+ Befragte mit dualer o. schulischer Berufsausbildung/einf.,mittl. Beamte verdienen im Schnitt  586.0038 EUR mehr als Befragte ohne Abschluss (`m1202 = 1`). Der Unterschied ist statistisch signifikant (`P>|t|` < 0,05).
+ Befragte mit  Aufstiegsfortbildung verdienen im Schnitt 1483.236 EUR mehr als Befragte ohne Abschluss (`m1202 = 1`). Der Unterschied ist statistisch signifikant (`P>|t|` < 0,05).
+ Befragte mit Abschluss einer Fachhochschule, Universität/ geh., höhere Beamte verdienen im Schnitt 2493.142 EUR mehr als Befragte ohne Abschluss (`m1202 = 1`). Der Unterschied ist statistisch signifikant (`P>|t|` < 0,05).




<!--chapter:end:22_anova.Rmd-->

# Literatur {#lit}

## Stata 

Kohler, U., & Kreuter, F. (2017). Datenanalyse mit Stata: Allgemeine Konzepte der Datenanalyse und ihre praktische Anwendung (5., aktualisierte Auflage.). De Gruyter Oldenbourg. [**Link**](https://doi.org/10.1515/9783110469509)  

**[The Interguide to Stata](https://wlm.userweb.mwn.de/Stata/)**  
**[Hilfeseiten der UCLA](https://stats.idre.ucla.edu/stata/)**  
**[StataList-Forum](https://statalist.org/forums/)**  
**[Germán Rodríguez](https://data.princeton.edu/stata)**s Seiten enthalten viele Beispiele, Tricks und Erklärungen  

## Statistik-Lehrbücher

Fokus auf Intuition und Beispiele:

Diaz-Bone, R. (2019). Statistik für Soziologen (4., überarbeitete Auflage.). [**Link**](https://www.utb-studi-e-book.de/9783838550718)

Kronthaler, F. (2014). Statistik angewandt. Springer Berlin Heidelberg. [**Link**](https://doi.org/10.1007/978-3-642-53740-0)

Urban, D., & Mayerl, J. (2018). Angewandte Regressionsanalyse: Theorie, Technik und Praxis. Springer Fachmedien Wiesbaden. [**Link**](https://doi.org/10.1007/978-3-658-01915-0)

etwas formaler und ausführlicher:

Backhaus, K., Erichson, B., Plinke, W., & Weiber, R. (2016). Multivariate Analysemethoden: Eine anwendungsorientierte Einführung (14., überarbeitete und aktualisierte Auflage). Springer Gabler. [**Link**](https://link.springer.com/book/10.1007/978-3-662-46076-4)

Bortz, J., & Schuster, C. (2010). Statistik für Human- und Sozialwissenschaftler (7th ed.). Springer. [**Link**](https://link.springer.com/book/10.1007/978-3-642-12770-0)

Wolf, C., & Best, H. (Eds.). (2010). Handbuch der sozialwissenschaftlichen Datenanalyse (1. Aufl). VS Verlag für Sozialwissenschaften.
[**Link**](https://doi.org/10.1007/978-3-531-92038-2)

<!--chapter:end:30_literatur.Rmd-->

# Anhang - fortgeschrittene Themen {#appendix} 

```{r setup31, echo = F, message=F, warning = F}
.libPaths("D:/R-library4")
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(dpi=800)
library(Statamarkdown)
library(tidyverse)
library(kableExtra)
# stataexe <- "C:/Program Files (x86)/Stata13/StataSE-64.exe"
stataexe <- "C:/Program Files/Stata16/StataSE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
# baua <- readstata13::read.dta13("D:/Datenspeicher/BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta",convert.factors = F)
```


## Import und Aufbereitung aus Excel {#xlsimport}


```{r gendata, include=F,eval = F}
set.seed(26131)
data.frame(id = 1:15,
           x1 = sample(c(1:2,-4), size = 15,replace = T),
           x2 = sample(c(1,0,-4,-9),     15,replace = T),
           x3 = sample(c(1,0,-4,-9),     15,replace = T),
           x4 = sample(1:5,              15,replace = T)) %>% 
  xlsx::write.xlsx("./Stata_Einführung_BIBB/Rohdaten.xlsx",row.names = F)
```


### Datenimport aus Excel-Datei

Unter `file -> import -> excel spreadsheet` können wir einen Excel-Import erstellen, den Befehl aus dem Ausgabefenster können wir dann kopieren:

```{stata xl1, eval =F }
import excel "D:\oCloud\Home-Cloud\Lehre\BIBB\StataBIBB1\Stata_Einführung_BIBB\Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
// oder:
cd "D:\oCloud\Home-Cloud\Lehre\BIBB\StataBIBB1\Stata_Einführung_BIBB\"
import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
* Überblick gewinnen
list
browse
```

```{stata xl1b, echo = F}
qui import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
list, clean
```

Hier eine kleine (Fantasie-)Doku zum Datensatz:


```{r xlsdoke,echo=F}
tribble(
    ~"Variablenname", ~"Beschreibung", ~"Details",
	 "id" ,		"Befragten-Identifikationsnummer", "",
	 "x1" ,		"Geschlecht", "1 = Frau, 2 = Mann, -4 keine Angabe",
	 "x2" ,		"Haustiere?", "0 = nein, 1 =  ja, -4  keine Angabe, -9 Datenfehler",
	 "x3" ,		"Frühaufsteher?", "0 = nein, 1 =  ja, -4  keine Angabe, -9 Datenfehler",
	 "x4"	,		"5er Likert-Skala: Stata macht Spaß", "1 = trifft überhaupt nicht zu",
	 ""	,		"", "5 = tifft voll zu"
) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed", full_width = F,font_size = 12) %>% 
  column_spec(1,monospace = TRUE) 

```
	 
				
### Variablen umbenennen: `rename`

Dann benennen wir die Variablen in sprechendere Namen um `rename alt neu`:
```{stata xl2, eval = F}
rename x1 gender
rename x2 pets
rename x3 earlybird
rename x4 stata
```

```{stata xl2T, echo = F}
qui import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
rename x1 gender
rename x2 pets
rename x3 earlybird
rename x4 stata
list, clean
```



### Variablenbeschreibung: `label variable`

```{stata xl3, eval = F}
label variable gender		  "Geschlecht"
label variable pets			  "Haustierbesitz?"
label variable earlybird	"Frühaufsteher"
label variable stata     	"Stata macht Spaß"
```
```{stata xl3T, echo = F}
qui import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
rename x1 gender
rename x2 pets
rename x3 earlybird
rename x4 stata
label variable gender		  "Geschlecht"
label variable pets			  "Haustierbesitz?"
label variable earlybird	"Frühaufsteher"
label variable stata     	"Stata macht Spaß"
d
```

### Werte labeln `label values`

Mit  `.buchstabe` können wir fehlende Werte unterscheidbar halten, so können wir sowohl "keine Angabe" als auch "Datenfehler als Missing überschreiben und trotzdem beide Varianten unterscheidbar halten:

```{stata xl4, eval = F}
tab gender
replace gender = .k if gender == -4
tab gender
tab gender, m

label define gend_lab 1 "Frau" 2 "Mann" .k "keine Angabe"
lab val gender gend_lab
tab gender
tab gender, m
```

```{stata xl4T, echo = F}
quietly{
import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
rename x1 gender
rename x2 pets
rename x3 earlybird
rename x4 stata
label variable gender		  "Geschlecht"
label variable pets			  "Haustierbesitz?"
label variable earlybird	"Frühaufsteher"
label variable stata     	"Stata macht Spaß"
  }
replace gender = .k if gender == -4
label define gend_lab 1 "Frau" 2 "Mann" .k "keine Angabe"
lab val gender gend_lab
tab gender, nol
tab gender, m
```

Für die weiteren Variablen können wir ähnlich vorgehen: 
```{stata xl5, eval = F}
lab def dummy_lab 0 "Nein" 1 "Ja" .k "keine Angabe" .d "Datenfehler"
lab val earlybird dummy_lab

tab earlybird
recode  earlybird (-9=.d) (-4=.k)
	* oder:
	replace earlybird = .k if earlybird == -4
	replace earlybird = .d if earlybird == -9
tab earlybird,m
```
```{stata xl5T, echo = F}
quietly{
import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
rename x1 gender
rename x2 pets
rename x3 earlybird
rename x4 stata
label variable gender		  "Geschlecht"
label variable pets			  "Haustierbesitz?"
label variable earlybird	"Frühaufsteher"
label variable stata     	"Stata macht Spaß"
  }
lab def dummy_lab 0 "Nein" 1 "Ja" .k "keine Angabe" .d "Datenfehler"
lab val earlybird dummy_lab
recode  earlybird (-9=.d) (-4=.k)
tab earlybird,m
```


und wir verwenden eine fortgeschrittene Programmierung: foreach-Schleife
```{stata xlloop, eval = F}
foreach v of varlist earlybird pets {
	replace `v' = .k if `v' == -4
	replace `v' = .d if `v' == -9	
	lab val `v' dummy_lab
}
// oder
foreach v of varlist earlybird pets {
	recode  `v' (-9=.d) (-4=.k)
	lab val `v' dummy_lab
}
```


```{stata xlloopT, echo = F}
quietly{
import excel "Rohdaten.xlsx", sheet("Sheet1") firstrow case(lower) clear
rename x1 gender
rename x2 pets
rename x3 earlybird
rename x4 stata
label variable gender		  "Geschlecht"
label variable pets			  "Haustierbesitz?"
label variable earlybird	"Frühaufsteher"
label variable stata     	"Stata macht Spaß"
}
foreach v of varlist earlybird pets {
	recode  `v' (-9=.d) (-4=.k)
	lab val `v' dummy_lab
}
```


### exportieren
```{stata xlexp, eval = F}
compress // variablen auf minimale speichergröße bringen
save "Datensatz_ready.dta", replace
```
Dieser Datensatz kann mit `use` geladen werden.

## `adopath`

Mit `adopath` können wir die Ordner anzeigen, wo die `ado`s liegen.
Mit `which` können wir den Speicherort eines `ado` ansehen:
```{stata ado, eval = F}
which mdesc
```
```{stata ado2, echo = F}
dis ""
which mdesc
```

Mit `adopath ++ PFAD` können wir `ado` aus zusätzlichen Ordner verwenden:
```{stata ado3, eval = F}
adopath ++ "D:\oCloud\Home-Cloud\Lehre\BIBB\StataBIBB1\Stata_Einführung_BIBB/u"
```



## log file

Mit log-Files können wir alles mitprotokollieren, was im Outputfenster passiert. Wir starten ein log-File mit `log using dateiname`. Ab dann wird alles in dieser Datei mitgeschrieben, was wir uns im Outputfenster anzeigen lassen:
```{stata s100, eval = F}
log using "C:\Pfad\zum\Ordner/log/logfile.txt", text replace // mit replace wird die Datei ggf überschrieben
* ------------------------------------------------ *
* Alter, Bildung, Geschlecht in der ETB 2018
* ------------------------------------------------ *
glo data "D:\Datenspeicher\BIBB_BAuA/"
use "${data}/BIBBBAuA_2018_suf1.0.dta", clear
mvdecode zpalter, mv(9999)	  // dieses log enthält auch kommentare
mvdecode m1202,mv(-1)   // noch einen 

* ------------------------ *
* Auswertung
* ------------------------ *
tab m1202 S1
su zpalter 

log close
```

```{r logfile, echo = F,out.width = "100%",out.height="100%", fig.align="center"}
knitr::include_graphics("./pics/10_logfile.png")
```


## Beobachtungen markieren mit `gen` & `if`

Die Kombination aus `gen` und `if` hilft uns, wenn wir eine Variable nur für einige Zeilen erstellen wollen. Das hilft insbesondere, wenn wir bestimmte Beobachtungen markieren wollen. Beispielsweise Frauen (`S1` = 1), die einer Nebentätigkeit (`nt` = 1) nachgehen:

```{stata genif, eval = F}
gen nt_frauen = 1 if S1 == 1 & nt == 1
```
```{stata genifb2,echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
gen nt_frauen = 1 if S1 == 1 & nt == 1
```

Nur wenn beide Bedingungen zutreffen, wird unter `nt_frauen` eine 1 abgelegt:
```{stata genif2, eval = F}
list S1 nt nt_frauen in 19671/19675, clean noobs abb(12) 
```
```{stata genifb,echo = F}
qui use "D:\Datenspeicher\BIBB_BAuA/BIBBBAuA_2018_suf1.0.dta", clear
qui gen nt_frauen = 1 if S1 == 1 & nt == 1
list S1 nt nt_frauen in 19671/19675, clean noobs abb(12) 
```

In der alltäglichen Arbeit hilft das manchmal, wenn man spezielle Fälle immer wieder aufrufen möchte und nicht immer die "ausgeschriebenen" `if`-Bedingungen verwenden möchte.


## Index bilden

In Kapitel 5 hatten wir mit [`recode`](#recode) eine Möglichkeit kennen gelernt, wie Codierungen verändert werden können.
Ein typischer Fall ist das "Umdrehen" von Skalen in einer Item-Batterie, sodass in beiden/allen Variablen höhere Werte auch inhaltlich das gleiche bedeuten.
In der BIBB/BAuA 2018 könnten wir bspw. einen Index für die Autonomie/Einbindung am Arbeitsplatz erstellen, indem wir die Antworten aus den folgenden beiden Fragen addieren:
 + `F700_02`: Wie häufig kommt es vor, dass Sie Ihre eigene Arbeit selbst planen und einteilen können?
 + `F700_08`: Wie oft kommt es vor, dass Sie nicht rechtzeitig über einschneidende Entscheidungen, Veränderungen oder Pläne für die Zukunft informiert werden?

Hier wäre es also so, dass der niedrigste Wert (1 = Häufig) jeweils etwas anderes bedeutet. Im Fall von `F700_08`
Variable betrachten, sehen wir
```{stata recode10, eval = F}
recode F700_02 (4=1) (3=2) (2=3) (1=4), into(F700_02_rev)
```


```{stata recode10a, eval = F}
gen index = (F700_02_rev + F700_10)/2
```

> <span style="color:#FFA500FF"><i>Inhaltlich ist das sicher nicht 100% überzeugend - hier geht's um die technische Umsetzung</i></span>

***

**Tipp**

Man kann in diesem speziellen Fall einer "umgedrehten" Skala können wir auch einfach "höchster Wert + 1 minus Variablenwert rechnen". Im Fall von `F700_02` ist der höchste Wert 4, also würden wir 4-`F700_02` rechnen:
```{stata revert, eval =F}
gen F700_02_rev2 = 5 - F700_02 
```

***

<!--chapter:end:31_appendix.Rmd-->

